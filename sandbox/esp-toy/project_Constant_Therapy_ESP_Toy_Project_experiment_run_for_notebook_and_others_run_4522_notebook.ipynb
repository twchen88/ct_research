{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7963fa52",
   "metadata": {},
   "source": [
    "# Project: Constant Therapy ESP Toy Project\n",
    "**Project Description**: Playing with ESP for Constant Therapy  \n",
    "**Experiment**: run for notebook and others  \n",
    "**Run ID**: 4522  \n",
    "**Notebook**: Neuro AI - Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa98a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line and execute the cell to install this notebook's dependencies.\n",
    "# You might need to restart the notebook's kernel.\n",
    "\n",
    "# %pip install -r project_Constant_Therapy_ESP_Toy_Project_experiment_run_for_notebook_and_others_run_4522_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68da1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numbers\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "import pandas as pd\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from io import StringIO\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Type\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from leaf_common.candidates.representation_types import RepresentationType\n",
    "from leaf_common.representation.rule_based.config.rule_set_config_helper import RuleSetConfigHelper\n",
    "from leaf_common.representation.rule_based.data.features import Features\n",
    "from leaf_common.representation.rule_based.data.rule_set import RuleSet\n",
    "from leaf_common.representation.rule_based.data.rule_set_binding import RuleSetBinding\n",
    "from leaf_common.representation.rule_based.evaluation.rule_set_binding_evaluator import RuleSetBindingEvaluator\n",
    "from leaf_common.representation.rule_based.persistence.rule_set_file_persistence import RuleSetFilePersistence\n",
    "from esp_sdk.esp_evaluator import EspEvaluator\n",
    "from esp_sdk.esp_service import EspService\n",
    "from unileaf_util.framework.data.profiling.dataframe_profiler import DataFrameProfiler\n",
    "from unileaf_util.framework.interfaces.data_frame_predictor import DataFramePredictor\n",
    "from unileaf_util.framework.metrics.metrics_manager import MetricsManager\n",
    "from unileaf_util.framework.transformers.data_encoder import DataEncoder\n",
    "from unileaf_util.framework.transformers.rules_data_encoder import RulesDataEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d18e",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "By default, load the dataset exported with the notebook, but you may plug your own dataset by changing the path for DATASET_CSV here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86c3822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  \\\n",
       "0           0         1.0         1.0         1.0         1.0         1.0   \n",
       "1           1         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   target_d_6  target_d_7  target_d_8  target_d_9  target_d_10  target_d_11  \\\n",
       "0         1.0         1.0         1.0         1.0          1.0          1.0   \n",
       "1         0.0         0.0         0.0         0.0          0.0          0.0   \n",
       "\n",
       "   target_d_12  target_d_13  target_d_14  d_1_score  d_1_ind  d_2_score  \\\n",
       "0          1.0          1.0          1.0        1.0      1.0        1.0   \n",
       "1          0.0          0.0          0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_2_ind  d_3_score  d_3_ind  d_4_score  d_4_ind  d_5_score  d_5_ind  \\\n",
       "0      1.0        1.0      1.0        1.0      1.0        1.0      1.0   \n",
       "1      0.0        0.0      0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_6_score  d_6_ind  d_7_score  d_7_ind  d_8_score  d_8_ind  d_9_score  \\\n",
       "0        1.0      1.0        1.0      1.0        1.0      1.0        1.0   \n",
       "1        0.0      0.0        0.0      0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_9_ind  d_10_score  d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  \\\n",
       "0      1.0         1.0       1.0         1.0       1.0         1.0       1.0   \n",
       "1      0.0         0.0       0.0         0.0       0.0         0.0       0.0   \n",
       "\n",
       "   d_13_score  d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  \\\n",
       "0         1.0       1.0         1.0       1.0       1.0       1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   d_4_next  d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  \\\n",
       "0       1.0       1.0       1.0       1.0       1.0       1.0        1.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "   d_11_next  d_12_next  d_13_next  d_14_next  \n",
       "0        1.0        1.0        1.0        1.0  \n",
       "1        0.0        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the dataset csv file\n",
    "DATASET_CSV = 'esp_toy.csv'\n",
    "with open(DATASET_CSV) as df_file:\n",
    "    data_source_df = pd.read_csv(df_file)\n",
    "data_source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d6e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  \\\n",
       "count    2.000000    2.000000    2.000000    2.000000    2.000000    2.000000   \n",
       "mean     0.500000    0.500000    0.500000    0.500000    0.500000    0.500000   \n",
       "std      0.707107    0.707107    0.707107    0.707107    0.707107    0.707107   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.250000    0.250000    0.250000    0.250000    0.250000    0.250000   \n",
       "50%      0.500000    0.500000    0.500000    0.500000    0.500000    0.500000   \n",
       "75%      0.750000    0.750000    0.750000    0.750000    0.750000    0.750000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       target_d_6  target_d_7  target_d_8  target_d_9  target_d_10  \\\n",
       "count    2.000000    2.000000    2.000000    2.000000     2.000000   \n",
       "mean     0.500000    0.500000    0.500000    0.500000     0.500000   \n",
       "std      0.707107    0.707107    0.707107    0.707107     0.707107   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.000000   \n",
       "25%      0.250000    0.250000    0.250000    0.250000     0.250000   \n",
       "50%      0.500000    0.500000    0.500000    0.500000     0.500000   \n",
       "75%      0.750000    0.750000    0.750000    0.750000     0.750000   \n",
       "max      1.000000    1.000000    1.000000    1.000000     1.000000   \n",
       "\n",
       "       target_d_11  target_d_12  target_d_13  target_d_14  d_1_score  \\\n",
       "count     2.000000     2.000000     2.000000     2.000000   2.000000   \n",
       "mean      0.500000     0.500000     0.500000     0.500000   0.500000   \n",
       "std       0.707107     0.707107     0.707107     0.707107   0.707107   \n",
       "min       0.000000     0.000000     0.000000     0.000000   0.000000   \n",
       "25%       0.250000     0.250000     0.250000     0.250000   0.250000   \n",
       "50%       0.500000     0.500000     0.500000     0.500000   0.500000   \n",
       "75%       0.750000     0.750000     0.750000     0.750000   0.750000   \n",
       "max       1.000000     1.000000     1.000000     1.000000   1.000000   \n",
       "\n",
       "        d_1_ind  d_2_score   d_2_ind  d_3_score   d_3_ind  d_4_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000   2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107   0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000   0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000   0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000   0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000   1.000000   \n",
       "\n",
       "        d_4_ind  d_5_score   d_5_ind  d_6_score   d_6_ind  d_7_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000   2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107   0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000   0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000   0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000   0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000   1.000000   \n",
       "\n",
       "        d_7_ind  d_8_score   d_8_ind  d_9_score   d_9_ind  d_10_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000    2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000    0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107    0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000    0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000    0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000    0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000    0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000    1.000000   \n",
       "\n",
       "       d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  d_13_score  \\\n",
       "count  2.000000    2.000000  2.000000    2.000000  2.000000    2.000000   \n",
       "mean   0.500000    0.500000  0.500000    0.500000  0.500000    0.500000   \n",
       "std    0.707107    0.707107  0.707107    0.707107  0.707107    0.707107   \n",
       "min    0.000000    0.000000  0.000000    0.000000  0.000000    0.000000   \n",
       "25%    0.250000    0.250000  0.250000    0.250000  0.250000    0.250000   \n",
       "50%    0.500000    0.500000  0.500000    0.500000  0.500000    0.500000   \n",
       "75%    0.750000    0.750000  0.750000    0.750000  0.750000    0.750000   \n",
       "max    1.000000    1.000000  1.000000    1.000000  1.000000    1.000000   \n",
       "\n",
       "       d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  d_4_next  \\\n",
       "count  2.000000    2.000000  2.000000  2.000000  2.000000  2.000000  2.000000   \n",
       "mean   0.500000    0.500000  0.500000  0.500000  0.500000  0.500000  0.500000   \n",
       "std    0.707107    0.707107  0.707107  0.707107  0.707107  0.707107  0.707107   \n",
       "min    0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25%    0.250000    0.250000  0.250000  0.250000  0.250000  0.250000  0.250000   \n",
       "50%    0.500000    0.500000  0.500000  0.500000  0.500000  0.500000  0.500000   \n",
       "75%    0.750000    0.750000  0.750000  0.750000  0.750000  0.750000  0.750000   \n",
       "max    1.000000    1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "\n",
       "       d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  d_11_next  \\\n",
       "count  2.000000  2.000000  2.000000  2.000000  2.000000   2.000000   2.000000   \n",
       "mean   0.500000  0.500000  0.500000  0.500000  0.500000   0.500000   0.500000   \n",
       "std    0.707107  0.707107  0.707107  0.707107  0.707107   0.707107   0.707107   \n",
       "min    0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "25%    0.250000  0.250000  0.250000  0.250000  0.250000   0.250000   0.250000   \n",
       "50%    0.500000  0.500000  0.500000  0.500000  0.500000   0.500000   0.500000   \n",
       "75%    0.750000  0.750000  0.750000  0.750000  0.750000   0.750000   0.750000   \n",
       "max    1.000000  1.000000  1.000000  1.000000  1.000000   1.000000   1.000000   \n",
       "\n",
       "       d_12_next  d_13_next  d_14_next  \n",
       "count   2.000000   2.000000   2.000000  \n",
       "mean    0.500000   0.500000   0.500000  \n",
       "std     0.707107   0.707107   0.707107  \n",
       "min     0.000000   0.000000   0.000000  \n",
       "25%     0.250000   0.250000   0.250000  \n",
       "50%     0.500000   0.500000   0.500000  \n",
       "75%     0.750000   0.750000   0.750000  \n",
       "max     1.000000   1.000000   1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ab931",
   "metadata": {},
   "source": [
    "## Encode the dataset\n",
    "Encode the dataset using the fields definition from the Experiment's data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6985dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = DataFrameProfiler()\n",
    "data_profile = profiler.profile_data_frame(data_source_df)\n",
    "\n",
    "# Get fields from the data profile\n",
    "fields = data_profile.get('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e18793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "d_10_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_10_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_10_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_1": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_10": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_11": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_12": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_13": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_14": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_2": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_3": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_4": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_5": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_6": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_7": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_8": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_9": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b55d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cao_mapping = {'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['d_1_next', 'd_2_next', 'd_3_next', 'd_4_next', 'd_5_next', 'd_6_next', 'd_7_next', 'd_8_next', 'd_9_next', 'd_10_next', 'd_11_next', 'd_12_next', 'd_13_next', 'd_14_next']}\n",
    "cao_fields =  set(cao_mapping['context'] + cao_mapping['actions'] + cao_mapping['outcomes'])\n",
    "\n",
    "# Validate if the fields match with cao_mapping\n",
    "missing_fields = set(fields.keys()) - cao_fields\n",
    "extra_fields =  cao_fields - set(fields.keys())\n",
    "if missing_fields != set():\n",
    "    print(f'The dataset contains fields that are NOT part of cao_mapping: {missing_fields}')\n",
    "    print('Please add them to the cao_mapping dictionary and make sure the rest of the notebook handles them correctly.')\n",
    "if extra_fields != set():\n",
    "    print(f'The cao_mapping contains fields that are NOT part of the dataset: {extra_fields}')\n",
    "    print('Please remove them from the cao_mapping dictionary and make sure they are not used in the rest of the notebook.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223ab569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "actions": [
        "target_d_1",
        "target_d_2",
        "target_d_3",
        "target_d_4",
        "target_d_5",
        "target_d_6",
        "target_d_7",
        "target_d_8",
        "target_d_9",
        "target_d_10",
        "target_d_11",
        "target_d_12",
        "target_d_13",
        "target_d_14"
       ],
       "context": [
        "d_1_ind",
        "d_2_ind",
        "d_3_ind",
        "d_4_ind",
        "d_5_ind",
        "d_6_ind",
        "d_7_ind",
        "d_8_ind",
        "d_9_ind",
        "d_10_ind",
        "d_11_ind",
        "d_12_ind",
        "d_13_ind",
        "d_14_ind",
        "d_1_score",
        "d_2_score",
        "d_3_score",
        "d_4_score",
        "d_5_score",
        "d_6_score",
        "d_7_score",
        "d_8_score",
        "d_9_score",
        "d_10_score",
        "d_11_score",
        "d_12_score",
        "d_13_score",
        "d_14_score"
       ],
       "outcomes": [
        "d_1_next",
        "d_2_next",
        "d_3_next",
        "d_4_next",
        "d_5_next",
        "d_6_next",
        "d_7_next",
        "d_8_next",
        "d_9_next",
        "d_10_next",
        "d_11_next",
        "d_12_next",
        "d_13_next",
        "d_14_next"
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(cao_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea5e2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  target_d_6  \\\n",
       "0         1.0         1.0         1.0         1.0         1.0         1.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   target_d_7  target_d_8  target_d_9  target_d_10  target_d_11  target_d_12  \\\n",
       "0         1.0         1.0         1.0          1.0          1.0          1.0   \n",
       "1         0.0         0.0         0.0          0.0          0.0          0.0   \n",
       "\n",
       "   target_d_13  target_d_14  d_1_score  d_1_ind  d_2_score  d_2_ind  \\\n",
       "0          1.0          1.0        1.0      1.0        1.0      1.0   \n",
       "1          0.0          0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_3_score  d_3_ind  d_4_score  d_4_ind  d_5_score  d_5_ind  d_6_score  \\\n",
       "0        1.0      1.0        1.0      1.0        1.0      1.0        1.0   \n",
       "1        0.0      0.0        0.0      0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_6_ind  d_7_score  d_7_ind  d_8_score  d_8_ind  d_9_score  d_9_ind  \\\n",
       "0      1.0        1.0      1.0        1.0      1.0        1.0      1.0   \n",
       "1      0.0        0.0      0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_10_score  d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  \\\n",
       "0         1.0       1.0         1.0       1.0         1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0         0.0       0.0   \n",
       "\n",
       "   d_13_score  d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  \\\n",
       "0         1.0       1.0         1.0       1.0       1.0       1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   d_4_next  d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  \\\n",
       "0       1.0       1.0       1.0       1.0       1.0       1.0        1.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "   d_11_next  d_12_next  d_13_next  d_14_next  \n",
       "0        1.0        1.0        1.0        1.0  \n",
       "1        0.0        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = DataEncoder(fields, cao_mapping)\n",
    "encoded_data_source_df = encoder.encode_as_df(data_source_df)\n",
    "encoded_data_source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcb394",
   "metadata": {},
   "source": [
    "## Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f466999",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSOR = 'regressor'\n",
    "CLASSIFIER = 'classifier'\n",
    "TYPES = [REGRESSOR, CLASSIFIER]\n",
    "predictors_by_id = {}\n",
    "model_metrics_by_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorType:\n",
    "    \"\"\"\n",
    "    This class defines the type of Predictor Possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predictor_type: str):\n",
    "        \"\"\"\n",
    "        The constructor confirms if the type of predictor is supported.\n",
    "        :param predictor_type: String describing a name for the type of the\n",
    "        predictor.\n",
    "        \"\"\"\n",
    "        assert predictor_type in TYPES, \"Invalid Predictor Type\"\n",
    "        self.type = predictor_type\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        This function overrides the string representation of the\n",
    "        class.\n",
    "        :return self.type: String\n",
    "        \"\"\"\n",
    "        return self.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c661dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(DataFramePredictor, ABC):\n",
    "    \"\"\"\n",
    "    This class contains the contract that any predictor\n",
    "    must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict[str, float],\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        Initializes a predictor, its params and the metadata.\n",
    "        :param data_df: DataFrame containing all processed data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes`\n",
    "        keys where each key returns a List of the selected column names as strings.\n",
    "        :param data_split: Dictionary containing the training splits indexed\n",
    "        by \"train_pct\" and \"val_pct\".\n",
    "        :param model_params: Parameters of the model\n",
    "        :param metadata: Dictionary describing any other information\n",
    "        that must be stored along with the model.\n",
    "        This might help in uniquely identifying the model\n",
    "        :returns nothing\n",
    "        \"\"\"\n",
    "        # Split the data between train, val and test sets\n",
    "        self.data_split = data_split\n",
    "\n",
    "        self.cao_mapping = cao_mapping\n",
    "        self.context_actions_columns = self.cao_mapping[\"context\"] + self.cao_mapping[\"actions\"]\n",
    "        # Check\n",
    "        if len(cao_mapping[\"outcomes\"]) > 1:\n",
    "            if not self.does_support_multiobjective():\n",
    "                raise ValueError(f\"{self.predictor_name} does NOT support multiple outputs\")\n",
    "\n",
    "        self.column_length = {}\n",
    "        if data_df is not None:\n",
    "            train_df, val_df, test_df = self.generate_data_split(data_df, self.data_split)\n",
    "\n",
    "            # Split the data between features (x) and labels(y)\n",
    "            self.train_x_df, self.train_y_df = self.get_data_xy_split(train_df, cao_mapping)\n",
    "            self.val_x_df, self.val_y_df = self.get_data_xy_split(val_df, cao_mapping)\n",
    "            self.test_x_df, self.test_y_df = self.get_data_xy_split(test_df, cao_mapping)\n",
    "\n",
    "            # Keep track of how many values are used to encode each outcome\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                first_value = self.train_y_df[column].head(1).values[0]\n",
    "                if isinstance(first_value, numbers.Number):\n",
    "                    # Value is a single scalar\n",
    "                    self.column_length[column] = 1\n",
    "                else:\n",
    "                    # value is a one-hot encoded vector, i.e. a list. Get its size.\n",
    "                    self.column_length[column] = len(self.train_y_df[column].head(1).values[0])\n",
    "        else:\n",
    "            # No data provided, assuming outcomes are numerical (not categorical)\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                self.column_length[column] = 1\n",
    "\n",
    "        if model_params is None:\n",
    "            model_params = {}\n",
    "        self.model_params = model_params\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Internal Parameters that are used to store the\n",
    "        # latest state of the model.\n",
    "        self._trained_model = None\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_type(self) -> PredictorType:\n",
    "        \"\"\"\n",
    "        :return the PredictorType of this Predictor: Regressor or Classifier\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def library(self) -> str:\n",
    "        \"\"\"\n",
    "        :return the underlying library that implements this predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_name(self) -> str:\n",
    "        \"\"\"\n",
    "        :return: the name of the Predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self, model_params: Dict):\n",
    "        \"\"\"\n",
    "        This function must be overridden to build the model using the model\n",
    "        parameters if desired and return a model.\n",
    "        :param model_params: Dictionary containing the model parameters\n",
    "        :return model: The built model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model(self, model,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray]) -> Type:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model\n",
    "        \"\"\"\n",
    "\n",
    "    def set_trained_model(self, trained_model) -> None:\n",
    "        \"\"\"\n",
    "        Sets the underlying trained model to the passed one.\n",
    "        :param trained_model: a trained model\n",
    "        :return Nothing:\n",
    "        \"\"\"\n",
    "        self._trained_model = trained_model\n",
    "\n",
    "    def get_trained_model(self):\n",
    "        \"\"\"\n",
    "        Returns the trained model if it has been set, None otherwise\n",
    "        :return self._trained_model:\n",
    "        \"\"\"\n",
    "        return self._trained_model\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_split(data_df: pd.DataFrame,\n",
    "                            data_split: Dict[str, Any]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the data between train, validation (optional) and test sets\n",
    "        :param data_df: the full dataset as a Pandas DataFrame\n",
    "        :param data_split: a dictionary with the\n",
    "        :return: a tuple of Pandas DataFrame: one for train, one for validation (or None), and one for test\n",
    "        \"\"\"\n",
    "\n",
    "        # First, split the data set in train and test sets.\n",
    "        # Use the provided random_state, if any\n",
    "        random_state = data_split.get(\"random_state\", None)\n",
    "        shuffle = data_split.get(\"shuffle\", True)\n",
    "        train_df, test_df = train_test_split(data_df,\n",
    "                                             test_size=data_split[\"test_pct\"],\n",
    "                                             random_state=random_state,\n",
    "                                             shuffle=shuffle)\n",
    "\n",
    "        # If we also need a validation set, split the train set into train and validation sets.\n",
    "        val_pct = data_split.get(\"val_pct\", 0)\n",
    "        if val_pct > 0:\n",
    "            train_df, val_df = train_test_split(train_df,\n",
    "                                                test_size=val_pct,\n",
    "                                                random_state=random_state,\n",
    "                                                shuffle=shuffle)\n",
    "        else:\n",
    "            val_df = None\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def predict(self, encoded_context_actions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method uses the trained model to make a prediction for the passed Pandas DataFrame\n",
    "        of context and actions. Returns the predicted outcomes in a Pandas DataFrame.\n",
    "        :param encoded_context_actions_df: a Pandas DataFrame containing encoded rows of context and actions for\n",
    "        which a prediction is requested. Categorical columns contain one-hot vectors, e.g. [1, 0, 0]. Which means\n",
    "        a row can be a list of arrays (1 per column), e.g.: [1, 0, 0], [1,0].\n",
    "        :return a Pandas DataFrame of the predicted outcomes for each context and actions row.\n",
    "        \"\"\"\n",
    "        # Default implementation\n",
    "        if self._trained_model:\n",
    "            # Predict using the model's input columns, in case encoded_context_actions_df contains more columns\n",
    "            # or is in a different order\n",
    "            context_action_df = encoded_context_actions_df[self.context_actions_columns]\n",
    "            # Convert one-hot vector columns into a single feature vector\n",
    "            features = DataEncoder.encoded_df_to_np(context_action_df)\n",
    "            # Check if model type is onnx runtime or not\n",
    "            if isinstance(self._trained_model, InferenceSession):\n",
    "                predictions = self._trained_model.run(None, {\"X\": features.astype(np.float32)})[0]\n",
    "            else:\n",
    "                predictions = self._trained_model.predict(features)\n",
    "            if isinstance(predictions, pd.DataFrame):\n",
    "                # Predictions are already in a DataFrame. Make sure they have the correct outcome names\n",
    "                predictions_df = predictions\n",
    "                predictions_df.columns = self.cao_mapping[\"outcomes\"]\n",
    "                # Convert predictions to float64 as it's JSON serializable, while float32 is not\n",
    "                predictions_df = predictions_df.astype(\"float64\")\n",
    "            else:\n",
    "                # Assuming predictions is a ndarray, convert it to a DataFrame with the output column names\n",
    "                predictions_df = DataEncoder.np_to_encoded_df(predictions,\n",
    "                                                              self.column_length)\n",
    "        else:\n",
    "            raise ValueError(\"Can't make predictions because the model has not been trained\")\n",
    "        return predictions_df\n",
    "\n",
    "    @staticmethod\n",
    "    def export_metrics(metrics_dict: Dict[str, Any], file_path: str):\n",
    "        \"\"\"\n",
    "        Save the model's training metrics to the specified location\n",
    "        :param metrics_dict: a dictionary containing metrics\n",
    "        :param file_path: the name and path of the file to persist the bytes to\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as my_file:\n",
    "            json.dump(metrics_dict, my_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_xy_split(data_df: Optional[pd.DataFrame],\n",
    "                          cao_mapping: Dict) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        This function takes a dataframe and a dictionary mapping indices to context,\n",
    "        action, or outcome. This then splits the dataframe into two dataframes based\n",
    "        on it's CAO tagging.\n",
    "\n",
    "        data_x: Context and Actions\n",
    "        data_y: Outcomes\n",
    "\n",
    "        :param data_df: a Pandas DataFrame with all the data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes` keys where each key returns a List\n",
    "         ofthe selected column names as strings.\n",
    "        :return: A tuple containing two dataframes: data_x with the features, and data_y with the labels (outcomes)\n",
    "        \"\"\"\n",
    "        if data_df is None:\n",
    "            return None, None\n",
    "\n",
    "        data_x_df = data_df[cao_mapping[\"context\"] + cao_mapping[\"actions\"]]\n",
    "        data_y_df = data_df[cao_mapping[\"outcomes\"]]\n",
    "\n",
    "        return data_x_df, data_y_df\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.predictor_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cabbb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Regressor Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(REGRESSOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43dc5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Classifier Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(CLASSIFIER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad072a9c",
   "metadata": {},
   "source": [
    "## Predictor 704abc67-fe1c-409b-87c1-8e59864b7fe4\n",
    "### CAO columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2357ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLUMNS = ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score']\n",
    "ACTION_COLUMNS = ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14']\n",
    "OUTCOME_COLUMNS = ['d_3_next']\n",
    "CONTEXT_ACTION_COLUMNS = CONTEXT_COLUMNS + ACTION_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450b53f",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10218d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {\"train_pct\": 0.8, \"test_pct\": 0.2, \"random_state\": 42}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0177020",
   "metadata": {},
   "source": [
    "### Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f5313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Predictor):\n",
    "    \"\"\"\n",
    "    This class implements a linear regression model from the SKLearn library.\n",
    "    \"\"\"\n",
    "    predictor_type = Regressor()\n",
    "    library = \"sklearn\"\n",
    "    predictor_name = name = f\"{library} Linear Regression\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict = None,\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        The constructor initializes the base params.\n",
    "        \"\"\"\n",
    "        super().__init__(data_df=data_df,\n",
    "                         cao_mapping=cao_mapping,\n",
    "                         data_split=data_split,\n",
    "                         model_params=model_params,\n",
    "                         metadata=metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "        multioutput = True\n",
    "        return multioutput\n",
    "\n",
    "    def build_model(self, model_params: Dict[str, Any]) -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function instantiates a Linear Regression model with the given params.\n",
    "        :return model: a LinearRegression instance\n",
    "        \"\"\"\n",
    "        model = linear_model.LinearRegression(**model_params)\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model: linear_model.LinearRegression,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray])\\\n",
    "            -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model and the desired metrics as a dictionary.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model: The linear regression model trained\n",
    "        \"\"\"\n",
    "        trained_model = model.fit(train_x, train_y)\n",
    "        return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afc98746",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_node_id = '704abc67-fe1c-409b-87c1-8e59864b7fe4'\n",
    "predictor = LinearRegression(encoded_data_source_df,\n",
    "    cao_mapping={'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['d_3_next']},\n",
    "data_split=data_split,\n",
    "model_params={},\n",
    "metadata={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84165bf9",
   "metadata": {},
   "source": [
    "### Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9802378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = DataEncoder.encoded_df_to_np(predictor.train_x_df)\n",
    "train_y = DataEncoder.encoded_df_to_np(predictor.train_y_df)\n",
    "if predictor.val_x_df is not None:\n",
    "    val_x = DataEncoder.encoded_df_to_np(predictor.val_x_df)\n",
    "    val_y = DataEncoder.encoded_df_to_np(predictor.val_y_df)\n",
    "else:\n",
    "    val_x = None\n",
    "    val_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11aad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train to True to train a new model, False to re-use a previously trained one\n",
    "train=True\n",
    "if train:\n",
    "    model = predictor.build_model(predictor.model_params)\n",
    "    trained_model = predictor.train_model(model,\n",
    "                                          train_x, train_y,\n",
    "                                          val_x, val_y)\n",
    "    joblib.dump(trained_model, 'predictor-704abc67-fe1c-409b-87c1-8e59864b7fe4.joblib')\n",
    "    predictor.set_trained_model(trained_model)\n",
    "else:\n",
    "    trained_model = joblib.load('predictor-704abc67-fe1c-409b-87c1-8e59864b7fe4.joblib')\n",
    "    predictor.set_trained_model(trained_model)\n",
    "predictors_by_id[predictor_node_id] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660239",
   "metadata": {},
   "source": [
    "### Predictor metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eab3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor trained. Metrics: {'train_Mean Squared Error_d_3_next': 0.0, 'test_Mean Squared Error_d_3_next': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model_metrics_by_id[predictor_node_id] = [MetricsManager.get_calculator('Mean Squared Error')]\n",
    "metrics = MetricsManager.compute_metrics(predictor,\n",
    "model_metrics_by_id[predictor_node_id],predictor.train_x_df, predictor.train_y_df,predictor.val_x_df, predictor.val_y_df,predictor.test_x_df, predictor.test_y_df,encoder)\n",
    "\n",
    "print(f'Predictor trained. Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a19b68",
   "metadata": {},
   "source": [
    "## Prescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1dc11",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026d8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnileafPrescriptor(EspEvaluator):\n",
    "    \"\"\"\n",
    "    An Unileaf Prescriptor makes prescriptions given an ESP candidate and a context DataFrame.\n",
    "    It is also an EspEvaluator implementation that returns metrics for ESP candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: Dict[str, Any],\n",
    "                 evaluation_df: pd.DataFrame,\n",
    "                 data_encoder: DataEncoder,\n",
    "                 predictors: List[Predictor]):\n",
    "        \"\"\"\n",
    "        Constructs a prescriptor evaluator\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :param evaluation_df: the Pandas DataFrame to use to evaluate the candidates\n",
    "        :param data_encoder: the DataEncoder used to encode the dataset\n",
    "        :param predictors: the predictors this prescriptor relies on\n",
    "        \"\"\"\n",
    "        # Instantiate EspEvaluator\n",
    "        # Note: sets self.config\n",
    "        super().__init__(config)\n",
    "\n",
    "        # CAO\n",
    "        self.cao_mapping = {\"context\": self.get_context_field_names(config),\n",
    "                            \"actions\": self.get_action_field_names(config),\n",
    "                            \"outcomes\": self.get_fitness_metrics(config)}\n",
    "        self.context_df = evaluation_df[self.cao_mapping[\"context\"]]\n",
    "        self.row_index = self.context_df.index\n",
    "\n",
    "        # Convert the context DataFrame to a format a NN can ingest\n",
    "        self.context_as_nn_input = self.convert_to_nn_input(self.context_df)\n",
    "\n",
    "        # Data encoder\n",
    "        self.data_encoder = data_encoder\n",
    "\n",
    "        # Predictors\n",
    "        self.predictors = predictors\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_nn_input(context_df: pd.DataFrame) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Converts a context DataFrame to a list of numpy arrays a neural network can ingest\n",
    "        :param context_df: a DataFrame containing inputs for a neural network. Number of inputs and size must match\n",
    "        :return: a list of numpy ndarray, on ndarray per neural network input\n",
    "        \"\"\"\n",
    "        # The NN expects a list of i inputs by s samples (e.g. 9 x 299).\n",
    "        # So convert the data frame to a numpy array (gives shape 299 x 9), transpose it (gives 9 x 299)\n",
    "        # and convert to list(list of 9 arrays of 299)\n",
    "        context_as_nn_input = list(context_df.to_numpy().transpose())\n",
    "        # Convert each column's list of 1D array to a 2D array\n",
    "        context_as_nn_input = [np.stack(context_as_nn_input[i], axis=0) for i in\n",
    "                               range(len(context_as_nn_input))]\n",
    "        return context_as_nn_input\n",
    "\n",
    "    def evaluate_candidate(self, candidate):\n",
    "        \"\"\"\n",
    "        Evaluates a single Prescriptor candidate and returns its metrics.\n",
    "        Implements the EspEvaluator interface\n",
    "        :param candidate: a Keras neural network or rule based Prescriptor candidate\n",
    "        :return metrics: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Prescribe actions\n",
    "        prescribed_actions_df = self.prescribe(candidate)\n",
    "\n",
    "        # Aggregate the context and actions dataframes.\n",
    "        context_actions_df = pd.concat([self.context_df,\n",
    "                                        prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "\n",
    "        # Compute the metrics\n",
    "        metrics = self._compute_metrics(context_actions_df)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_metrics(self, context_actions_df):\n",
    "        \"\"\"\n",
    "        Computes metrics from the passed context/actions DataFrame using the instance's trained predictors.\n",
    "        :param context_actions_df: a DataFrame of context / prescribed actions\n",
    "        :return: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Get the predicted outcomes from the predictors\n",
    "        metrics = {}\n",
    "        for predictor in self.predictors:\n",
    "            predicted_outcomes = predictor.predict(context_actions_df)\n",
    "\n",
    "            # UN-853: Decode predictions before computing numerical metrics, if a data_encoder is available\n",
    "            if self.data_encoder is not None:\n",
    "                decoded_predicted_outcomes = self.data_encoder.decode_as_df(predicted_outcomes)\n",
    "            else:\n",
    "                decoded_predicted_outcomes = predicted_outcomes\n",
    "\n",
    "            # Only add a metric for the outcomes the prescriptor is interested in\n",
    "            for outcome in self.cao_mapping[\"outcomes\"]:\n",
    "                # Add the metrics that have been produced by this predictor\n",
    "                if outcome in predictor.cao_mapping[\"outcomes\"]:\n",
    "                    # Check the type of metric: numerical or categorical?\n",
    "                    if decoded_predicted_outcomes[[outcome]].iloc[:, 0].dtype == object:\n",
    "                        # Categorical outcome. Use the *encoded* predicted outcome.\n",
    "                        preds = predicted_outcomes[outcome]\n",
    "                        # Classifiers return the category's index in the list of categories, so we can take the mean\n",
    "                        # of the encoded outcomes. Note: this works because Outcomes are encoded using LabelEncoder\n",
    "                        # AND the user defined order for each Outcome categories.\n",
    "                        metrics[outcome] = preds.mean()\n",
    "                    else:\n",
    "                        # UN-853: Numerical outcome. Use the *decoded*, i.e. scaled back, predicted outcome\n",
    "                        preds = decoded_predicted_outcomes[outcome]\n",
    "                        # Regressors produce floats: take the mean of the decoded outcome\n",
    "                        metrics[outcome] = preds.mean()\n",
    "        return metrics\n",
    "\n",
    "    def prescribe(self, candidate, context_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed candidate and context\n",
    "        :param candidate: an ESP candidate, either neural network or rules\n",
    "        :param context_df: a DataFrame containing the context to prescribe for,\n",
    "         or None to use the instance one\n",
    "        :return: a DataFrame containing actions prescribed for each context\n",
    "        \"\"\"\n",
    "        if context_df is None:\n",
    "            # No context is provided, use the instance's one\n",
    "            context_as_nn_input = self.context_as_nn_input\n",
    "            row_index = self.row_index\n",
    "        else:\n",
    "            # Convert the context DataFrame to something more suitable for neural networks\n",
    "            context_as_nn_input = self.convert_to_nn_input(context_df)\n",
    "            # Use the context's row index\n",
    "            row_index = context_df.index\n",
    "\n",
    "        is_rule_based = isinstance(candidate, RuleSet)\n",
    "        if is_rule_based:\n",
    "            actions = self._prescribe_from_rules(candidate, context_as_nn_input)\n",
    "        else:\n",
    "            actions = self._prescribe_from_nn(candidate, context_as_nn_input)\n",
    "\n",
    "        # Convert the prescribed actions to a DataFrame\n",
    "        prescribed_actions_df = pd.DataFrame(actions,\n",
    "                                             columns=self.cao_mapping[\"actions\"],\n",
    "                                             index=row_index)\n",
    "        ### DEBUG:\n",
    "        print(\"-------------DEBUG: Prescribed Actions (encoded):\")\n",
    "        print(\"shape: \", prescribed_actions_df.shape)\n",
    "        print(\"dtype: \", prescribed_actions_df.dtypes)\n",
    "        has_arrays = prescribed_actions_df.applymap(lambda x: isinstance(x, (list, np.ndarray))).any() # type: ignore\n",
    "        print(\"array-valued columns:\\n\", has_arrays[has_arrays].index.tolist())\n",
    "\n",
    "        # UN-2430 Decode the softmaxes, if any, back into categories\n",
    "        prescribed_actions_df = self.data_encoder.decode_as_df(prescribed_actions_df)\n",
    "        # UN0-240 Re-encode the actions into what the predictors expect (e.g. one-hots for categorical data)\n",
    "        prescribed_actions_df = self.data_encoder.encode_as_df(prescribed_actions_df)\n",
    "        return prescribed_actions_df\n",
    "\n",
    "    def _prescribe_from_rules(self, candidate, context_as_nn_input: List[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed rules model candidate and context\n",
    "        :param candidate: a rules model candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to list of action values\n",
    "        \"\"\"\n",
    "        cand_states = RuleSetConfigHelper.get_states(self.config)\n",
    "        cand_actions = RuleSetConfigHelper.get_actions(self.config)\n",
    "        candidate = RuleSetBinding(candidate, cand_states, cand_actions)\n",
    "        rules_encoder = RulesDataEncoder(candidate.actions)\n",
    "        evaluator = RuleSetBindingEvaluator()\n",
    "        rules_input = rules_encoder.encode_to_rules_data(context_as_nn_input)\n",
    "        rules_output = evaluator.evaluate(candidate, rules_input)\n",
    "        actions = rules_encoder.decode_from_rules_data(rules_output)\n",
    "        return actions\n",
    "\n",
    "    def _prescribe_from_nn(self, candidate, context_as_nn_input: List[np.ndarray]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed neural network candidate and context\n",
    "        :param candidate: a Keras neural network candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to action value or list of action values\n",
    "        \"\"\"\n",
    "        # Get the prescribed actions\n",
    "        prescribed_actions = candidate.predict(context_as_nn_input)\n",
    "        actions = {}\n",
    "\n",
    "        if self._is_single_action_prescriptor():\n",
    "            # Put the single action in an array to process it like multiple actions\n",
    "            prescribed_actions = [prescribed_actions]\n",
    "\n",
    "        for index, action_col in enumerate(self.cao_mapping[\"actions\"]):\n",
    "            if self._is_scalar(prescribed_actions[index]):\n",
    "                # We have a single row and this action is numerical. Convert it to a scalar.\n",
    "                actions[action_col] = prescribed_actions[index].item()\n",
    "            else:\n",
    "                actions[action_col] = prescribed_actions[index].tolist()\n",
    "        return actions\n",
    "\n",
    "    def _is_single_action_prescriptor(self):\n",
    "        \"\"\"\n",
    "        Checks how many Actions have been defined in the Context, Actions, Outcomes mapping.\n",
    "        :return: True if only 1 action is defined, False otherwise\n",
    "        \"\"\"\n",
    "        return len(self.cao_mapping[\"actions\"]) == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_scalar(prescribed_action):\n",
    "        \"\"\"\n",
    "        Checks if the prescribed action contains a single value, i.e. a scalar, or an array.\n",
    "        A prescribed action contains a single value if it has been prescribed for a single context sample\n",
    "        :param prescribed_action: a scalar or an array\n",
    "        :return: True if the prescribed action contains a scalar, False otherwise.\n",
    "        \"\"\"\n",
    "        return prescribed_action.shape[0] == 1 and prescribed_action.shape[1] == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_context_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Context column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Context column names\n",
    "        \"\"\"\n",
    "        nn_inputs = config[\"network\"][\"inputs\"]\n",
    "        contexts = [nn_input[\"name\"] for nn_input in nn_inputs]\n",
    "        return contexts\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Action column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Action column names\n",
    "        \"\"\"\n",
    "        nn_outputs = config[\"network\"][\"outputs\"]\n",
    "        actions = [nn_output[\"name\"] for nn_output in nn_outputs]\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fitness_metrics(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of fitness metric names (Outcomes) to optimize.\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of fitness metric names\n",
    "        \"\"\"\n",
    "        metrics = config[\"evolution\"][\"fitness\"]\n",
    "        fitness_metrics = [metric[\"metric_name\"] for metric in metrics]\n",
    "        return fitness_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e500f",
   "metadata": {},
   "source": [
    "### Prescriptor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a68c60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'evolution': {'fitness': [{'maximize': True, 'metric_name': 'd_3_next'}], 'nb_elites': 5, 'mutation_type': 'gaussian_noise_percentage', 'nb_generations': 40, 'mutation_factor': 0.1, 'population_size': 10, 'parent_selection': 'tournament', 'initialization_range': 1, 'mutation_probability': 0.1, 'remove_population_pct': 0.8, 'initialization_distribution': 'orthogonal'}, 'network': {'inputs': [{'name': 'd_1_ind', 'size': 1, 'values': ['float']}, {'name': 'd_2_ind', 'size': 1, 'values': ['float']}, {'name': 'd_3_ind', 'size': 1, 'values': ['float']}, {'name': 'd_4_ind', 'size': 1, 'values': ['float']}, {'name': 'd_5_ind', 'size': 1, 'values': ['float']}, {'name': 'd_6_ind', 'size': 1, 'values': ['float']}, {'name': 'd_7_ind', 'size': 1, 'values': ['float']}, {'name': 'd_8_ind', 'size': 1, 'values': ['float']}, {'name': 'd_9_ind', 'size': 1, 'values': ['float']}, {'name': 'd_10_ind', 'size': 1, 'values': ['float']}, {'name': 'd_11_ind', 'size': 1, 'values': ['float']}, {'name': 'd_12_ind', 'size': 1, 'values': ['float']}, {'name': 'd_13_ind', 'size': 1, 'values': ['float']}, {'name': 'd_14_ind', 'size': 1, 'values': ['float']}, {'name': 'd_1_score', 'size': 1, 'values': ['float']}, {'name': 'd_2_score', 'size': 1, 'values': ['float']}, {'name': 'd_3_score', 'size': 1, 'values': ['float']}, {'name': 'd_4_score', 'size': 1, 'values': ['float']}, {'name': 'd_5_score', 'size': 1, 'values': ['float']}, {'name': 'd_6_score', 'size': 1, 'values': ['float']}, {'name': 'd_7_score', 'size': 1, 'values': ['float']}, {'name': 'd_8_score', 'size': 1, 'values': ['float']}, {'name': 'd_9_score', 'size': 1, 'values': ['float']}, {'name': 'd_10_score', 'size': 1, 'values': ['float']}, {'name': 'd_11_score', 'size': 1, 'values': ['float']}, {'name': 'd_12_score', 'size': 1, 'values': ['float']}, {'name': 'd_13_score', 'size': 1, 'values': ['float']}, {'name': 'd_14_score', 'size': 1, 'values': ['float']}], 'outputs': [{'name': 'target_d_1', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_2', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_3', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_4', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_5', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_6', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_7', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_8', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_9', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_10', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_11', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_12', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_13', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_14', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}], 'hidden_layers': [{'layer_name': 'hidden_1', 'layer_type': 'Dense', 'layer_params': {'units': 16, 'use_bias': True, 'activation': 'tanh'}}]}, 'LEAF': {'representation': 'NNWeights', 'experiment_id': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'version': '1.0.0', 'persistence_dir': 'trained_prescriptors/', 'candidates_to_persist': 'best'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b12f0ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "LEAF": {
        "candidates_to_persist": "best",
        "experiment_id": "UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726",
        "persistence_dir": "trained_prescriptors/",
        "representation": "NNWeights",
        "version": "1.0.0"
       },
       "evolution": {
        "fitness": [
         {
          "maximize": true,
          "metric_name": "d_3_next"
         }
        ],
        "initialization_distribution": "orthogonal",
        "initialization_range": 1,
        "mutation_factor": 0.1,
        "mutation_probability": 0.1,
        "mutation_type": "gaussian_noise_percentage",
        "nb_elites": 5,
        "nb_generations": 40,
        "parent_selection": "tournament",
        "population_size": 10,
        "remove_population_pct": 0.8
       },
       "network": {
        "hidden_layers": [
         {
          "layer_name": "hidden_1",
          "layer_params": {
           "activation": "tanh",
           "units": 16,
           "use_bias": true
          },
          "layer_type": "Dense"
         }
        ],
        "inputs": [
         {
          "name": "d_1_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_2_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_3_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_4_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_5_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_6_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_7_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_8_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_9_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_10_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_11_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_12_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_13_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_14_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_1_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_2_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_3_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_4_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_5_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_6_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_7_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_8_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_9_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_10_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_11_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_12_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_13_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_14_score",
          "size": 1,
          "values": [
           "float"
          ]
         }
        ],
        "outputs": [
         {
          "activation": "softmax",
          "name": "target_d_1",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_2",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_3",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_4",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_5",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_6",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_7",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_8",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_9",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_10",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_11",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_12",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_13",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_14",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         }
        ]
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7928d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_predictor_ids = ['704abc67-fe1c-409b-87c1-8e59864b7fe4']\n",
    "all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d3be7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "  experiment_id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726\n",
      "  checkpoint_id: None\n",
      "  timestamp: 20251024-164626\n",
      "Asking ESP for a seed generation...\n",
      "Seed generation received.\n",
      "Evaluating PopulationResponse for generation 1...:\n",
      "PopulationResponse:\n",
      "  Generation: 1\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1/20251024-224626\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "-------------DEBUG: Prescribed Actions (encoded):\n",
      "shape:  (2, 14)\n",
      "dtype:  target_d_1     object\n",
      "target_d_2     object\n",
      "target_d_3     object\n",
      "target_d_4     object\n",
      "target_d_5     object\n",
      "target_d_6     object\n",
      "target_d_7     object\n",
      "target_d_8     object\n",
      "target_d_9     object\n",
      "target_d_10    object\n",
      "target_d_11    object\n",
      "target_d_12    object\n",
      "target_d_13    object\n",
      "target_d_14    object\n",
      "dtype: object\n",
      "array-valued columns:\n",
      " ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/wczx8z_50yn5szt1q9b333fr0000gp/T/ipykernel_19589/1568703674.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  has_arrays = prescribed_actions_df.applymap(lambda x: isinstance(x, (list, np.ndarray))).any() # type: ignore\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (4) does not match length of index (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m esp_service = EspService(config)\n\u001b[32m      3\u001b[39m esp_evaluator = UnileafPrescriptor(config,\n\u001b[32m      4\u001b[39m                                    encoded_data_source_df,\n\u001b[32m      5\u001b[39m                                    encoder,\n\u001b[32m      6\u001b[39m                                    all_predictors)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m experiment_results_dir = \u001b[43mesp_service\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mesp_evaluator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/esp_service.py:139\u001b[39m, in \u001b[36mEspService.train\u001b[39m\u001b[34m(self, evaluator, checkpoint_id, early_stopper, plotter)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03mTrains and persists Prescriptors according to the experiment parameters.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m:param evaluator: an EspEvaluator to evaluate the candidate Prescriptors\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \u001b[33;03m:return: the name of the folder to which the Prescriptors have been persisted at the end of each generation\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     persistence_directory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_training_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_with_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m                                                                     \u001b[49m\u001b[43mcheckpoint_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m                                                                     \u001b[49m\u001b[43mearly_stopper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m                                                                     \u001b[49m\u001b[43mplotter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplotter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFailed to connect to ESP service. Can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/training/esp_training_loop.py:135\u001b[39m, in \u001b[36mEspTrainingLoop.train_with_evaluator\u001b[39m\u001b[34m(self, evaluator, checkpoint_id, early_stopper, plotter)\u001b[39m\n\u001b[32m    132\u001b[39m logging.debug(\u001b[33m\"\u001b[39m\u001b[33mPersistence directory: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(persistence_dir))\n\u001b[32m    134\u001b[39m eval_pop_start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_population_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m eval_pop_end_time = time.time()\n\u001b[32m    137\u001b[39m eval_pop_times.append(eval_pop_end_time - eval_pop_start_time)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/esp_evaluator.py:77\u001b[39m, in \u001b[36mEspEvaluator.evaluate_population\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     75\u001b[39m population_evaluator = DefaultPopulationEvaluator(\u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m     76\u001b[39m evaluation_data = \u001b[38;5;28;01mNone\u001b[39;00m      \u001b[38;5;66;03m# None passed in with this interface\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mpopulation_evaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/evaluation/default_population_evaluator.py:100\u001b[39m, in \u001b[36mDefaultPopulationEvaluator.evaluate\u001b[39m\u001b[34m(self, component, evaluation_data)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEvaluating candidates synchronously because max_workers == 0\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m     population_evaluator = SynchronousPopulationEvaluator(\u001b[38;5;28mself\u001b[39m._config,\n\u001b[32m     98\u001b[39m                                                           \u001b[38;5;28mself\u001b[39m._candidate_evaluator,\n\u001b[32m     99\u001b[39m                                                           candidates_to_evaluate)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mpopulation_evaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/evaluation/synchronous_population_evaluator.py:79\u001b[39m, in \u001b[36mSynchronousPopulationEvaluator.evaluate\u001b[39m\u001b[34m(self, component, evaluation_data)\u001b[39m\n\u001b[32m     75\u001b[39m model = ModelUtil.model_from_bytes(\u001b[38;5;28mself\u001b[39m._config, candidate.interpretation)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Actually evaluate a single candidate\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Simply pass along the evaluation_data, whatever it is\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_candidate_evaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Update the metrics of the candidate in place in the population list\u001b[39;00m\n\u001b[32m     82\u001b[39m candidate.metrics = MetricsSerializer.encode(metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/esp_sdk/esp_evaluator.py:90\u001b[39m, in \u001b[36mEspEvaluator.evaluate\u001b[39m\u001b[34m(self, component, evaluation_data)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03mEvaluates a single kind of model object on the given evaluation_data\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mreturning a metrics dictionary giving clues as to how the evaluation\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03mpassed in.\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m _ = evaluation_data\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mUnileafPrescriptor.evaluate_candidate\u001b[39m\u001b[34m(self, candidate)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mEvaluates a single Prescriptor candidate and returns its metrics.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03mImplements the EspEvaluator interface\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m:param candidate: a Keras neural network or rule based Prescriptor candidate\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m:return metrics: A dictionary of {'metric_name': metric_value}\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Prescribe actions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m prescribed_actions_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Aggregate the context and actions dataframes.\u001b[39;00m\n\u001b[32m     66\u001b[39m context_actions_df = pd.concat([\u001b[38;5;28mself\u001b[39m.context_df,\n\u001b[32m     67\u001b[39m                                 prescribed_actions_df],\n\u001b[32m     68\u001b[39m                                axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mUnileafPrescriptor.prescribe\u001b[39m\u001b[34m(self, candidate, context_df)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33marray-valued columns:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, has_arrays[has_arrays].index.tolist())\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# UN-2430 Decode the softmaxes, if any, back into categories\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m prescribed_actions_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_as_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprescribed_actions_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# UN0-240 Re-encode the actions into what the predictors expect (e.g. one-hots for categorical data)\u001b[39;00m\n\u001b[32m    148\u001b[39m prescribed_actions_df = \u001b[38;5;28mself\u001b[39m.data_encoder.encode_as_df(prescribed_actions_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/unileaf_util/framework/transformers/data_encoder.py:214\u001b[39m, in \u001b[36mDataEncoder.decode_as_df\u001b[39m\u001b[34m(self, data_df)\u001b[39m\n\u001b[32m    211\u001b[39m             column_values = np.rint(column_values)\n\u001b[32m    213\u001b[39m     values_by_column[column] = np.array(column_values.ravel(order=\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m), dtype=field_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m decoded_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_by_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m                                       \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m                                       \u001b[49m\u001b[43m)\u001b[49m[values_by_column.keys()]\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/pandas/core/frame.py:2542\u001b[39m, in \u001b[36mDataFrame.from_records\u001b[39m\u001b[34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[39m\n\u001b[32m   2539\u001b[39m     columns = columns.drop(exclude)\n\u001b[32m   2541\u001b[39m manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m mgr = \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_mgr(mgr, axes=mgr.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/pandas/core/internals/construction.py:119\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     arrays, refs = \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/pandas/core/internals/construction.py:630\u001b[39m, in \u001b[36m_homogenize\u001b[39m\u001b[34m(data, index, dtype)\u001b[39m\n\u001b[32m    627\u001b[39m         val = lib.fast_multiget(val, oindex._values, default=np.nan)\n\u001b[32m    629\u001b[39m     val = sanitize_array(val, index, dtype=dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m     refs.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    633\u001b[39m homogenized.append(val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (4) does not match length of index (2)"
     ]
    }
   ],
   "source": [
    "# Instantiate the EspService\n",
    "esp_service = EspService(config)\n",
    "esp_evaluator = UnileafPrescriptor(config,\n",
    "                                   encoded_data_source_df,\n",
    "                                   encoder,\n",
    "                                   all_predictors)\n",
    "experiment_results_dir = esp_service.train(esp_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a3f30",
   "metadata": {},
   "source": [
    "## ESP Summary Stats\n",
    "esp_service.train(...) returned the directory in which the experiment results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = os.path.join(experiment_results_dir, 'experiment_stats.csv')\n",
    "with open(stats_file) as csv_file:\n",
    "    stats_df = pd.read_csv(csv_file, sep=',')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637bf3",
   "metadata": {},
   "source": [
    "## ESP Summary Plot\n",
    "esp_service.train(...) generated a plot summarizing the experiment's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1753f8a",
   "metadata": {},
   "source": [
    "### d_3_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = os.path.join(experiment_results_dir, 'd_3_next_plot.png')\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71375ad",
   "metadata": {},
   "source": [
    "# Models usage\n",
    "## Load the Prescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last row of the stats DataFrame, i.e. the last generation, to find the best model\n",
    "last_gen = stats_df['generation'].iloc[-1]\n",
    "best_score = stats_df['max_d_3_next'].iloc[-1]\n",
    "cid_best_score = stats_df['cid_max_d_3_next'].iloc[-1]\n",
    "prescriptor_model_filename = os.path.join(experiment_results_dir,\n",
    "                                          str(last_gen),\n",
    "                                          cid_best_score + '.h5')\n",
    "print(f'Best max_d_3_next average is {best_score:.3f} for candidate id {cid_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(f'Loading prescriptor model: {prescriptor_model_filename}')\n",
    "prescriptor_model = load_model(prescriptor_model_filename, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77623b3",
   "metadata": {},
   "source": [
    "## Get a sample context\n",
    "Get the context from one of the rows in the dataset, and make a prescription for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82356a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_source_df.sample(1)\n",
    "sample_context_df = sample_df[CONTEXT_COLUMNS]\n",
    "sample_context_action_df = sample_df[CONTEXT_ACTION_COLUMNS]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6202e",
   "metadata": {},
   "source": [
    "### Prescribe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c30780",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_context_df = encoder.encode_as_df(sample_context_df)\n",
    "encoded_prescribed_actions_df = esp_evaluator.prescribe(prescriptor_model, encoded_sample_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effa5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the context and actions dataframes.\n",
    "encoded_context_actions_df = pd.concat([encoded_sample_context_df,\n",
    "                                        encoded_prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "sample_context_prescribed_action_df = encoder.decode_as_df(encoded_context_actions_df)\n",
    "sample_context_prescribed_action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f37e8",
   "metadata": {},
   "source": [
    "### Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(predictors, context_action_df, encoder):\n",
    "    pred_array = []\n",
    "    for predictor in predictors:\n",
    "        pred = predictor.predict(encoder.encode_as_df(context_action_df))\n",
    "        pred_array.append(encoder.decode_as_df(pred))\n",
    "    preds_df = pd.concat(pred_array, axis=1)\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccba3d",
   "metadata": {},
   "source": [
    "#### With original actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_actions_preds = get_predictions(all_predictors, sample_context_action_df, encoder)\n",
    "original_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416793",
   "metadata": {},
   "source": [
    "#### With prescribed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescribed_actions_preds = get_predictions(all_predictors, sample_context_prescribed_action_df, encoder)\n",
    "prescribed_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957136",
   "metadata": {},
   "source": [
    "#### With custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_custom_action_df = sample_context_prescribed_action_df.copy()\n",
    " # TODO: Uncomment and replace by the name of the actions(s) to customize\n",
    "# sample_context_custom_action_df['SOME_ACTION_TO_CUSTOMIZE'] = 42\n",
    "sample_context_custom_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_actions_preds = get_predictions(all_predictors, sample_context_custom_action_df, encoder)\n",
    "custom_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70319159",
   "metadata": {},
   "source": [
    "### Compare\n",
    "Compare 3 rows in a single table:\n",
    "- the original sample\n",
    "- the sample with the prescribed actions\n",
    "- the sample with some custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME_COLUMNS = list(original_actions_preds.columns)\n",
    "OUTCOME_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed OUTCOMES for the sample\n",
    "pd.DataFrame(sample_df[OUTCOME_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context and actions for 3 rows:\n",
    "# - the original sample\n",
    "# - the sample with the prescribed actions\n",
    "# - the sample with some custom actions\n",
    "comp_df = pd.concat([sample_context_action_df,\n",
    "                     sample_context_prescribed_action_df,\n",
    "                     sample_context_custom_action_df], axis=0)\n",
    "\n",
    "# Compute the outcomes\n",
    "outcomes_dict = {}\n",
    "for outcome in OUTCOME_COLUMNS:\n",
    "    # Observed outcome from the sample in the dataset\n",
    "    outcomes_dict[outcome] = [sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0]]\n",
    "    # Predicted outcome\n",
    "    outcomes_dict[f'{outcome}_predicted'] = [original_actions_preds[outcome].iloc[0],\n",
    "                                             prescribed_actions_preds[outcome].iloc[0],\n",
    "                                             custom_actions_preds[outcome].iloc[0]]\n",
    "    # For numerical outcomes, compute the diff between predicted and observed\n",
    "    if is_numeric_dtype(outcomes_dict[outcome][0]):\n",
    "        diff = [a - b for a, b in zip(outcomes_dict[f'{outcome}_predicted'],\n",
    "                                      outcomes_dict[outcome])]\n",
    "        outcomes_dict[f'{outcome}_diff'] = diff\n",
    "    \n",
    "outcomes_df = pd.DataFrame(outcomes_dict)\n",
    "comp_df[list(outcomes_dict.keys())] = outcomes_df[list(outcomes_dict.keys())].values\n",
    "comp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f111",
   "metadata": {},
   "source": [
    "## Initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = sample_df[CONTEXT_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = sample_df[ACTION_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = sample_df[OUTCOME_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(outcomes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

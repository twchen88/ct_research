{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7963fa52",
   "metadata": {},
   "source": [
    "# Project: Constant Therapy ESP Toy Project\n",
    "**Project Description**: Playing with ESP for Constant Therapy  \n",
    "**Experiment**: run for notebook and others  \n",
    "**Run ID**: 4522  \n",
    "**Notebook**: Neuro AI - Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa98a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line and execute the cell to install this notebook's dependencies.\n",
    "# You might need to restart the notebook's kernel.\n",
    "\n",
    "# %pip install -r project_Constant_Therapy_ESP_Toy_Project_experiment_run_for_notebook_and_others_run_4522_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68da1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numbers\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "import pandas as pd\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from io import StringIO\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Type\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from leaf_common.candidates.representation_types import RepresentationType\n",
    "from leaf_common.representation.rule_based.config.rule_set_config_helper import RuleSetConfigHelper\n",
    "from leaf_common.representation.rule_based.data.features import Features\n",
    "from leaf_common.representation.rule_based.data.rule_set import RuleSet\n",
    "from leaf_common.representation.rule_based.data.rule_set_binding import RuleSetBinding\n",
    "from leaf_common.representation.rule_based.evaluation.rule_set_binding_evaluator import RuleSetBindingEvaluator\n",
    "from leaf_common.representation.rule_based.persistence.rule_set_file_persistence import RuleSetFilePersistence\n",
    "from esp_sdk.esp_evaluator import EspEvaluator\n",
    "from esp_sdk.esp_service import EspService\n",
    "from unileaf_util.framework.data.profiling.dataframe_profiler import DataFrameProfiler\n",
    "from unileaf_util.framework.interfaces.data_frame_predictor import DataFramePredictor\n",
    "from unileaf_util.framework.metrics.metrics_manager import MetricsManager\n",
    "from unileaf_util.framework.transformers.data_encoder import DataEncoder\n",
    "from unileaf_util.framework.transformers.rules_data_encoder import RulesDataEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d18e",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "By default, load the dataset exported with the notebook, but you may plug your own dataset by changing the path for DATASET_CSV here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86c3822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  \\\n",
       "0           0         1.0         1.0         1.0         1.0         1.0   \n",
       "1           1         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   target_d_6  target_d_7  target_d_8  target_d_9  target_d_10  target_d_11  \\\n",
       "0         1.0         1.0         1.0         1.0          1.0          1.0   \n",
       "1         0.0         0.0         0.0         0.0          0.0          0.0   \n",
       "\n",
       "   target_d_12  target_d_13  target_d_14  d_1_score  d_1_ind  d_2_score  \\\n",
       "0          1.0          1.0          1.0        1.0      1.0        1.0   \n",
       "1          0.0          0.0          0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_2_ind  d_3_score  d_3_ind  d_4_score  d_4_ind  d_5_score  d_5_ind  \\\n",
       "0      1.0        1.0      1.0        1.0      1.0        1.0      1.0   \n",
       "1      0.0        0.0      0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_6_score  d_6_ind  d_7_score  d_7_ind  d_8_score  d_8_ind  d_9_score  \\\n",
       "0        1.0      1.0        1.0      1.0        1.0      1.0        1.0   \n",
       "1        0.0      0.0        0.0      0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_9_ind  d_10_score  d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  \\\n",
       "0      1.0         1.0       1.0         1.0       1.0         1.0       1.0   \n",
       "1      0.0         0.0       0.0         0.0       0.0         0.0       0.0   \n",
       "\n",
       "   d_13_score  d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  \\\n",
       "0         1.0       1.0         1.0       1.0       1.0       1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   d_4_next  d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  \\\n",
       "0       1.0       1.0       1.0       1.0       1.0       1.0        1.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "   d_11_next  d_12_next  d_13_next  d_14_next  \n",
       "0        1.0        1.0        1.0        1.0  \n",
       "1        0.0        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the dataset csv file\n",
    "DATASET_CSV = 'esp_toy.csv' # loading the toy dataset to understand notebook flow\n",
    "with open(DATASET_CSV) as df_file:\n",
    "    data_source_df = pd.read_csv(df_file)\n",
    "data_source_df.head() # as expected (be careful with how unnamed columns are handled later down the pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d6e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  \\\n",
       "count    2.000000    2.000000    2.000000    2.000000    2.000000    2.000000   \n",
       "mean     0.500000    0.500000    0.500000    0.500000    0.500000    0.500000   \n",
       "std      0.707107    0.707107    0.707107    0.707107    0.707107    0.707107   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.250000    0.250000    0.250000    0.250000    0.250000    0.250000   \n",
       "50%      0.500000    0.500000    0.500000    0.500000    0.500000    0.500000   \n",
       "75%      0.750000    0.750000    0.750000    0.750000    0.750000    0.750000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       target_d_6  target_d_7  target_d_8  target_d_9  target_d_10  \\\n",
       "count    2.000000    2.000000    2.000000    2.000000     2.000000   \n",
       "mean     0.500000    0.500000    0.500000    0.500000     0.500000   \n",
       "std      0.707107    0.707107    0.707107    0.707107     0.707107   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.000000   \n",
       "25%      0.250000    0.250000    0.250000    0.250000     0.250000   \n",
       "50%      0.500000    0.500000    0.500000    0.500000     0.500000   \n",
       "75%      0.750000    0.750000    0.750000    0.750000     0.750000   \n",
       "max      1.000000    1.000000    1.000000    1.000000     1.000000   \n",
       "\n",
       "       target_d_11  target_d_12  target_d_13  target_d_14  d_1_score  \\\n",
       "count     2.000000     2.000000     2.000000     2.000000   2.000000   \n",
       "mean      0.500000     0.500000     0.500000     0.500000   0.500000   \n",
       "std       0.707107     0.707107     0.707107     0.707107   0.707107   \n",
       "min       0.000000     0.000000     0.000000     0.000000   0.000000   \n",
       "25%       0.250000     0.250000     0.250000     0.250000   0.250000   \n",
       "50%       0.500000     0.500000     0.500000     0.500000   0.500000   \n",
       "75%       0.750000     0.750000     0.750000     0.750000   0.750000   \n",
       "max       1.000000     1.000000     1.000000     1.000000   1.000000   \n",
       "\n",
       "        d_1_ind  d_2_score   d_2_ind  d_3_score   d_3_ind  d_4_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000   2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107   0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000   0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000   0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000   0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000   1.000000   \n",
       "\n",
       "        d_4_ind  d_5_score   d_5_ind  d_6_score   d_6_ind  d_7_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000   2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107   0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000   0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000   0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000   0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000   0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000   1.000000   \n",
       "\n",
       "        d_7_ind  d_8_score   d_8_ind  d_9_score   d_9_ind  d_10_score  \\\n",
       "count  2.000000   2.000000  2.000000   2.000000  2.000000    2.000000   \n",
       "mean   0.500000   0.500000  0.500000   0.500000  0.500000    0.500000   \n",
       "std    0.707107   0.707107  0.707107   0.707107  0.707107    0.707107   \n",
       "min    0.000000   0.000000  0.000000   0.000000  0.000000    0.000000   \n",
       "25%    0.250000   0.250000  0.250000   0.250000  0.250000    0.250000   \n",
       "50%    0.500000   0.500000  0.500000   0.500000  0.500000    0.500000   \n",
       "75%    0.750000   0.750000  0.750000   0.750000  0.750000    0.750000   \n",
       "max    1.000000   1.000000  1.000000   1.000000  1.000000    1.000000   \n",
       "\n",
       "       d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  d_13_score  \\\n",
       "count  2.000000    2.000000  2.000000    2.000000  2.000000    2.000000   \n",
       "mean   0.500000    0.500000  0.500000    0.500000  0.500000    0.500000   \n",
       "std    0.707107    0.707107  0.707107    0.707107  0.707107    0.707107   \n",
       "min    0.000000    0.000000  0.000000    0.000000  0.000000    0.000000   \n",
       "25%    0.250000    0.250000  0.250000    0.250000  0.250000    0.250000   \n",
       "50%    0.500000    0.500000  0.500000    0.500000  0.500000    0.500000   \n",
       "75%    0.750000    0.750000  0.750000    0.750000  0.750000    0.750000   \n",
       "max    1.000000    1.000000  1.000000    1.000000  1.000000    1.000000   \n",
       "\n",
       "       d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  d_4_next  \\\n",
       "count  2.000000    2.000000  2.000000  2.000000  2.000000  2.000000  2.000000   \n",
       "mean   0.500000    0.500000  0.500000  0.500000  0.500000  0.500000  0.500000   \n",
       "std    0.707107    0.707107  0.707107  0.707107  0.707107  0.707107  0.707107   \n",
       "min    0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25%    0.250000    0.250000  0.250000  0.250000  0.250000  0.250000  0.250000   \n",
       "50%    0.500000    0.500000  0.500000  0.500000  0.500000  0.500000  0.500000   \n",
       "75%    0.750000    0.750000  0.750000  0.750000  0.750000  0.750000  0.750000   \n",
       "max    1.000000    1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "\n",
       "       d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  d_11_next  \\\n",
       "count  2.000000  2.000000  2.000000  2.000000  2.000000   2.000000   2.000000   \n",
       "mean   0.500000  0.500000  0.500000  0.500000  0.500000   0.500000   0.500000   \n",
       "std    0.707107  0.707107  0.707107  0.707107  0.707107   0.707107   0.707107   \n",
       "min    0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "25%    0.250000  0.250000  0.250000  0.250000  0.250000   0.250000   0.250000   \n",
       "50%    0.500000  0.500000  0.500000  0.500000  0.500000   0.500000   0.500000   \n",
       "75%    0.750000  0.750000  0.750000  0.750000  0.750000   0.750000   0.750000   \n",
       "max    1.000000  1.000000  1.000000  1.000000  1.000000   1.000000   1.000000   \n",
       "\n",
       "       d_12_next  d_13_next  d_14_next  \n",
       "count   2.000000   2.000000   2.000000  \n",
       "mean    0.500000   0.500000   0.500000  \n",
       "std     0.707107   0.707107   0.707107  \n",
       "min     0.000000   0.000000   0.000000  \n",
       "25%     0.250000   0.250000   0.250000  \n",
       "50%     0.500000   0.500000   0.500000  \n",
       "75%     0.750000   0.750000   0.750000  \n",
       "max     1.000000   1.000000   1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ab931",
   "metadata": {},
   "source": [
    "## Encode the dataset\n",
    "Encode the dataset using the fields definition from the Experiment's data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6985dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = DataFrameProfiler()\n",
    "data_profile = profiler.profile_data_frame(data_source_df)\n",
    "\n",
    "# Get fields from the data profile\n",
    "fields = data_profile.get('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e18793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "d_10_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_10_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_10_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_11_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_12_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_13_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_14_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_1_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_2_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_3_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_4_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_5_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_6_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_7_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_8_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_ind": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_next": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "d_9_score": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_1": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_10": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_11": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_12": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_13": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_14": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_2": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_3": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_4": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_5": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_6": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_7": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_8": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       },
       "target_d_9": {
        "data_type": "FLOAT",
        "discrete_categorical_values": [
         "0.0",
         "1.0"
        ],
        "has_nan": false,
        "mean": 0.5,
        "range": [
         0,
         1
        ],
        "std_dev": 0.7071067811865476,
        "sum": 1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b55d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains fields that are NOT part of cao_mapping: {'d_3_next', 'd_8_next', 'd_7_next', 'd_12_next', 'd_9_next', 'd_13_next', 'd_11_next', 'd_1_next', 'd_14_next', 'd_2_next', 'd_4_next', 'd_5_next', 'd_6_next', 'd_10_next'}\n",
      "Please add them to the cao_mapping dictionary and make sure the rest of the notebook handles them correctly.\n",
      "The cao_mapping contains fields that are NOT part of the dataset: {'derived_metric'}\n",
      "Please remove them from the cao_mapping dictionary and make sure they are not used in the rest of the notebook.\n"
     ]
    }
   ],
   "source": [
    "cao_mapping = {'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['derived_metric']}\n",
    "cao_fields =  set(cao_mapping['context'] + cao_mapping['actions'] + cao_mapping['outcomes'])\n",
    "\n",
    "# Validate if the fields match with cao_mapping\n",
    "missing_fields = set(fields.keys()) - cao_fields # fields is a dictionary # type: ignore\n",
    "extra_fields =  cao_fields - set(fields.keys()) # fields is a dictionary # type: ignore\n",
    "if missing_fields != set():\n",
    "    print(f'The dataset contains fields that are NOT part of cao_mapping: {missing_fields}')\n",
    "    print('Please add them to the cao_mapping dictionary and make sure the rest of the notebook handles them correctly.')\n",
    "if extra_fields != set():\n",
    "    print(f'The cao_mapping contains fields that are NOT part of the dataset: {extra_fields}')\n",
    "    print('Please remove them from the cao_mapping dictionary and make sure they are not used in the rest of the notebook.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223ab569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "actions": [
        "target_d_1",
        "target_d_2",
        "target_d_3",
        "target_d_4",
        "target_d_5",
        "target_d_6",
        "target_d_7",
        "target_d_8",
        "target_d_9",
        "target_d_10",
        "target_d_11",
        "target_d_12",
        "target_d_13",
        "target_d_14"
       ],
       "context": [
        "d_1_ind",
        "d_2_ind",
        "d_3_ind",
        "d_4_ind",
        "d_5_ind",
        "d_6_ind",
        "d_7_ind",
        "d_8_ind",
        "d_9_ind",
        "d_10_ind",
        "d_11_ind",
        "d_12_ind",
        "d_13_ind",
        "d_14_ind",
        "d_1_score",
        "d_2_score",
        "d_3_score",
        "d_4_score",
        "d_5_score",
        "d_6_score",
        "d_7_score",
        "d_8_score",
        "d_9_score",
        "d_10_score",
        "d_11_score",
        "d_12_score",
        "d_13_score",
        "d_14_score"
       ],
       "outcomes": [
        "derived_metric"
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(cao_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148181ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_cols = [f'd_{i}_next' for i in range(1, 15)]\n",
    "missing = [c for c in next_cols if c not in data_source_df.columns]\n",
    "assert not missing, f\"Missing raw next-domain columns: {missing}\"\n",
    "\n",
    "# Example metric: average of 14 true next scores (replace with your formula)\n",
    "data_source_df['derived_metric'] = data_source_df[next_cols].mean(axis=1).astype(float)\n",
    "\n",
    "# Ensure 'derived_metric' exists in the fields dict so the encoder will build a transformer\n",
    "# Clone the schema of an existing numeric next-domain column to be safe:\n",
    "template_key = next_cols[0]  # e.g., 'd_1_next'\n",
    "if 'derived_metric' not in fields:\n",
    "    fields['derived_metric'] = dict(fields[template_key])\n",
    "    # If your fields have a 'name' or 'original_name' key, update it:\n",
    "    for k in ('name', 'original_name'):\n",
    "        if k in fields['derived_metric']:\n",
    "            fields['derived_metric'][k] = 'derived_metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ea5e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DataEncoder(fields, cao_mapping) # fields is a dictionary # type: ignore\n",
    "encoded_data_source_df = encoder.encode_as_df(data_source_df)\n",
    "encoded_data_source_df.head()\n",
    "assert 'derived_metric' in encoded_data_source_df.columns, \"derived_metric missing from encoded_data_source_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa44693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_d_1</th>\n",
       "      <th>target_d_2</th>\n",
       "      <th>target_d_3</th>\n",
       "      <th>target_d_4</th>\n",
       "      <th>target_d_5</th>\n",
       "      <th>target_d_6</th>\n",
       "      <th>target_d_7</th>\n",
       "      <th>target_d_8</th>\n",
       "      <th>target_d_9</th>\n",
       "      <th>target_d_10</th>\n",
       "      <th>target_d_11</th>\n",
       "      <th>target_d_12</th>\n",
       "      <th>target_d_13</th>\n",
       "      <th>target_d_14</th>\n",
       "      <th>d_1_score</th>\n",
       "      <th>d_1_ind</th>\n",
       "      <th>d_2_score</th>\n",
       "      <th>d_2_ind</th>\n",
       "      <th>d_3_score</th>\n",
       "      <th>d_3_ind</th>\n",
       "      <th>d_4_score</th>\n",
       "      <th>d_4_ind</th>\n",
       "      <th>d_5_score</th>\n",
       "      <th>d_5_ind</th>\n",
       "      <th>d_6_score</th>\n",
       "      <th>d_6_ind</th>\n",
       "      <th>d_7_score</th>\n",
       "      <th>d_7_ind</th>\n",
       "      <th>d_8_score</th>\n",
       "      <th>d_8_ind</th>\n",
       "      <th>d_9_score</th>\n",
       "      <th>d_9_ind</th>\n",
       "      <th>d_10_score</th>\n",
       "      <th>d_10_ind</th>\n",
       "      <th>d_11_score</th>\n",
       "      <th>d_11_ind</th>\n",
       "      <th>d_12_score</th>\n",
       "      <th>d_12_ind</th>\n",
       "      <th>d_13_score</th>\n",
       "      <th>d_13_ind</th>\n",
       "      <th>d_14_score</th>\n",
       "      <th>d_14_ind</th>\n",
       "      <th>d_1_next</th>\n",
       "      <th>d_2_next</th>\n",
       "      <th>d_3_next</th>\n",
       "      <th>d_4_next</th>\n",
       "      <th>d_5_next</th>\n",
       "      <th>d_6_next</th>\n",
       "      <th>d_7_next</th>\n",
       "      <th>d_8_next</th>\n",
       "      <th>d_9_next</th>\n",
       "      <th>d_10_next</th>\n",
       "      <th>d_11_next</th>\n",
       "      <th>d_12_next</th>\n",
       "      <th>d_13_next</th>\n",
       "      <th>d_14_next</th>\n",
       "      <th>derived_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_d_1  target_d_2  target_d_3  target_d_4  target_d_5  target_d_6  \\\n",
       "0         1.0         1.0         1.0         1.0         1.0         1.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   target_d_7  target_d_8  target_d_9  target_d_10  target_d_11  target_d_12  \\\n",
       "0         1.0         1.0         1.0          1.0          1.0          1.0   \n",
       "1         0.0         0.0         0.0          0.0          0.0          0.0   \n",
       "\n",
       "   target_d_13  target_d_14  d_1_score  d_1_ind  d_2_score  d_2_ind  \\\n",
       "0          1.0          1.0        1.0      1.0        1.0      1.0   \n",
       "1          0.0          0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_3_score  d_3_ind  d_4_score  d_4_ind  d_5_score  d_5_ind  d_6_score  \\\n",
       "0        1.0      1.0        1.0      1.0        1.0      1.0        1.0   \n",
       "1        0.0      0.0        0.0      0.0        0.0      0.0        0.0   \n",
       "\n",
       "   d_6_ind  d_7_score  d_7_ind  d_8_score  d_8_ind  d_9_score  d_9_ind  \\\n",
       "0      1.0        1.0      1.0        1.0      1.0        1.0      1.0   \n",
       "1      0.0        0.0      0.0        0.0      0.0        0.0      0.0   \n",
       "\n",
       "   d_10_score  d_10_ind  d_11_score  d_11_ind  d_12_score  d_12_ind  \\\n",
       "0         1.0       1.0         1.0       1.0         1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0         0.0       0.0   \n",
       "\n",
       "   d_13_score  d_13_ind  d_14_score  d_14_ind  d_1_next  d_2_next  d_3_next  \\\n",
       "0         1.0       1.0         1.0       1.0       1.0       1.0       1.0   \n",
       "1         0.0       0.0         0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   d_4_next  d_5_next  d_6_next  d_7_next  d_8_next  d_9_next  d_10_next  \\\n",
       "0       1.0       1.0       1.0       1.0       1.0       1.0        1.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "   d_11_next  d_12_next  d_13_next  d_14_next  derived_metric  \n",
       "0        1.0        1.0        1.0        1.0             1.0  \n",
       "1        0.0        0.0        0.0        0.0             0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcb394",
   "metadata": {},
   "source": [
    "## Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f466999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initiate global variables\n",
    "\n",
    "REGRESSOR = 'regressor'\n",
    "CLASSIFIER = 'classifier'\n",
    "TYPES = [REGRESSOR, CLASSIFIER]\n",
    "predictors_by_id = {}\n",
    "model_metrics_by_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorType:\n",
    "    \"\"\"\n",
    "    This class defines the type of Predictor Possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predictor_type: str):\n",
    "        \"\"\"\n",
    "        The constructor confirms if the type of predictor is supported.\n",
    "        :param predictor_type: String describing a name for the type of the\n",
    "        predictor.\n",
    "        \"\"\"\n",
    "        assert predictor_type in TYPES, \"Invalid Predictor Type\"\n",
    "        self.type = predictor_type\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        This function overrides the string representation of the\n",
    "        class.\n",
    "        :return self.type: String\n",
    "        \"\"\"\n",
    "        return self.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c661dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(DataFramePredictor, ABC):\n",
    "    \"\"\"\n",
    "    This class contains the contract that any predictor\n",
    "    must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict[str, float],\n",
    "                 model_params: Dict = None, # type: ignore\n",
    "                 metadata: Dict = None): # type: ignore\n",
    "        \"\"\"\n",
    "        Initializes a predictor, its params and the metadata.\n",
    "        :param data_df: DataFrame containing all processed data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes`\n",
    "        keys where each key returns a List of the selected column names as strings.\n",
    "        :param data_split: Dictionary containing the training splits indexed\n",
    "        by \"train_pct\" and \"val_pct\".\n",
    "        :param model_params: Parameters of the model\n",
    "        :param metadata: Dictionary describing any other information\n",
    "        that must be stored along with the model.\n",
    "        This might help in uniquely identifying the model\n",
    "        :returns nothing\n",
    "        \"\"\"\n",
    "        # Split the data between train, val and test sets\n",
    "        self.data_split = data_split\n",
    "\n",
    "        self.cao_mapping = cao_mapping\n",
    "        self.context_actions_columns = self.cao_mapping[\"context\"] + self.cao_mapping[\"actions\"]\n",
    "        # Check\n",
    "        if len(cao_mapping[\"outcomes\"]) > 1:\n",
    "            if not self.does_support_multiobjective():\n",
    "                raise ValueError(f\"{self.predictor_name} does NOT support multiple outputs\")\n",
    "\n",
    "        self.column_length = {}\n",
    "        if data_df is not None:\n",
    "            train_df, val_df, test_df = self.generate_data_split(data_df, self.data_split)\n",
    "\n",
    "            # Split the data between features (x) and labels(y)\n",
    "            self.train_x_df, self.train_y_df = self.get_data_xy_split(train_df, cao_mapping)\n",
    "            self.val_x_df, self.val_y_df = self.get_data_xy_split(val_df, cao_mapping)\n",
    "            self.test_x_df, self.test_y_df = self.get_data_xy_split(test_df, cao_mapping)\n",
    "\n",
    "            # Keep track of how many values are used to encode each outcome\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                first_value = self.train_y_df[column].head(1).values[0]\n",
    "                if isinstance(first_value, numbers.Number):\n",
    "                    # Value is a single scalar\n",
    "                    self.column_length[column] = 1\n",
    "                else:\n",
    "                    # value is a one-hot encoded vector, i.e. a list. Get its size.\n",
    "                    self.column_length[column] = len(self.train_y_df[column].head(1).values[0])\n",
    "        else:\n",
    "            # No data provided, assuming outcomes are numerical (not categorical)\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                self.column_length[column] = 1\n",
    "\n",
    "        if model_params is None:\n",
    "            model_params = {}\n",
    "        self.model_params = model_params\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Internal Parameters that are used to store the\n",
    "        # latest state of the model.\n",
    "        self._trained_model = None\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_type(self) -> PredictorType:\n",
    "        \"\"\"\n",
    "        :return the PredictorType of this Predictor: Regressor or Classifier\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def library(self) -> str:\n",
    "        \"\"\"\n",
    "        :return the underlying library that implements this predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_name(self) -> str:\n",
    "        \"\"\"\n",
    "        :return: the name of the Predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self, model_params: Dict):\n",
    "        \"\"\"\n",
    "        This function must be overridden to build the model using the model\n",
    "        parameters if desired and return a model.\n",
    "        :param model_params: Dictionary containing the model parameters\n",
    "        :return model: The built model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model(self, model,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray]) -> Type:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model\n",
    "        \"\"\"\n",
    "\n",
    "    def set_trained_model(self, trained_model) -> None:\n",
    "        \"\"\"\n",
    "        Sets the underlying trained model to the passed one.\n",
    "        :param trained_model: a trained model\n",
    "        :return Nothing:\n",
    "        \"\"\"\n",
    "        self._trained_model = trained_model\n",
    "\n",
    "    def get_trained_model(self):\n",
    "        \"\"\"\n",
    "        Returns the trained model if it has been set, None otherwise\n",
    "        :return self._trained_model:\n",
    "        \"\"\"\n",
    "        return self._trained_model\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_split(data_df: pd.DataFrame,\n",
    "                            data_split: Dict[str, Any]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the data between train, validation (optional) and test sets\n",
    "        :param data_df: the full dataset as a Pandas DataFrame\n",
    "        :param data_split: a dictionary with the\n",
    "        :return: a tuple of Pandas DataFrame: one for train, one for validation (or None), and one for test\n",
    "        \"\"\"\n",
    "\n",
    "        # First, split the data set in train and test sets.\n",
    "        # Use the provided random_state, if any\n",
    "        random_state = data_split.get(\"random_state\", None)\n",
    "        shuffle = data_split.get(\"shuffle\", True)\n",
    "        train_df, test_df = train_test_split(data_df,\n",
    "                                             test_size=data_split[\"test_pct\"],\n",
    "                                             random_state=random_state,\n",
    "                                             shuffle=shuffle)\n",
    "\n",
    "        # If we also need a validation set, split the train set into train and validation sets.\n",
    "        val_pct = data_split.get(\"val_pct\", 0)\n",
    "        if val_pct > 0:\n",
    "            train_df, val_df = train_test_split(train_df,\n",
    "                                                test_size=val_pct,\n",
    "                                                random_state=random_state,\n",
    "                                                shuffle=shuffle)\n",
    "        else:\n",
    "            val_df = None\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def predict(self, encoded_context_actions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method uses the trained model to make a prediction for the passed Pandas DataFrame\n",
    "        of context and actions. Returns the predicted outcomes in a Pandas DataFrame.\n",
    "        :param encoded_context_actions_df: a Pandas DataFrame containing encoded rows of context and actions for\n",
    "        which a prediction is requested. Categorical columns contain one-hot vectors, e.g. [1, 0, 0]. Which means\n",
    "        a row can be a list of arrays (1 per column), e.g.: [1, 0, 0], [1,0].\n",
    "        :return a Pandas DataFrame of the predicted outcomes for each context and actions row.\n",
    "        \"\"\"\n",
    "        # Default implementation\n",
    "        if self._trained_model:\n",
    "            # Predict using the model's input columns, in case encoded_context_actions_df contains more columns\n",
    "            # or is in a different order\n",
    "            context_action_df = encoded_context_actions_df[self.context_actions_columns]\n",
    "            # Convert one-hot vector columns into a single feature vector\n",
    "            features = DataEncoder.encoded_df_to_np(context_action_df)\n",
    "            # Check if model type is onnx runtime or not\n",
    "            if isinstance(self._trained_model, InferenceSession):\n",
    "                predictions = self._trained_model.run(None, {\"X\": features.astype(np.float32)})[0]\n",
    "            else:\n",
    "                predictions = self._trained_model.predict(features)\n",
    "            if isinstance(predictions, pd.DataFrame):\n",
    "                # Predictions are already in a DataFrame. Make sure they have the correct outcome names\n",
    "                predictions_df = predictions\n",
    "                predictions_df.columns = self.cao_mapping[\"outcomes\"]\n",
    "                # Convert predictions to float64 as it's JSON serializable, while float32 is not\n",
    "                predictions_df = predictions_df.astype(\"float64\")\n",
    "            else:\n",
    "                # Assuming predictions is a ndarray, convert it to a DataFrame with the output column names\n",
    "                predictions_df = DataEncoder.np_to_encoded_df(predictions,\n",
    "                                                              self.column_length)\n",
    "        else:\n",
    "            raise ValueError(\"Can't make predictions because the model has not been trained\")\n",
    "        return predictions_df\n",
    "\n",
    "    @staticmethod\n",
    "    def export_metrics(metrics_dict: Dict[str, Any], file_path: str):\n",
    "        \"\"\"\n",
    "        Save the model's training metrics to the specified location\n",
    "        :param metrics_dict: a dictionary containing metrics\n",
    "        :param file_path: the name and path of the file to persist the bytes to\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as my_file:\n",
    "            json.dump(metrics_dict, my_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_xy_split(data_df: Optional[pd.DataFrame],\n",
    "                          cao_mapping: Dict) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        This function takes a dataframe and a dictionary mapping indices to context,\n",
    "        action, or outcome. This then splits the dataframe into two dataframes based\n",
    "        on it's CAO tagging.\n",
    "\n",
    "        data_x: Context and Actions\n",
    "        data_y: Outcomes\n",
    "\n",
    "        :param data_df: a Pandas DataFrame with all the data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes` keys where each key returns a List\n",
    "         ofthe selected column names as strings.\n",
    "        :return: A tuple containing two dataframes: data_x with the features, and data_y with the labels (outcomes)\n",
    "        \"\"\"\n",
    "        if data_df is None:\n",
    "            return None, None\n",
    "\n",
    "        data_x_df = data_df[cao_mapping[\"context\"] + cao_mapping[\"actions\"]]\n",
    "        data_y_df = data_df[cao_mapping[\"outcomes\"]]\n",
    "\n",
    "        return data_x_df, data_y_df\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.predictor_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cabbb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Regressor Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(REGRESSOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dc5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Classifier Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(CLASSIFIER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad072a9c",
   "metadata": {},
   "source": [
    "## Predictor 704abc67-fe1c-409b-87c1-8e59864b7fe4\n",
    "### CAO columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2357ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLUMNS = ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score']\n",
    "ACTION_COLUMNS = ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14']\n",
    "OUTCOME_COLUMNS = ['derived_metric']\n",
    "CONTEXT_ACTION_COLUMNS = CONTEXT_COLUMNS + ACTION_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450b53f",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10218d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {\"train_pct\": 0.8, \"test_pct\": 0.2, \"random_state\": 42}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0177020",
   "metadata": {},
   "source": [
    "### Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f5313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Predictor):\n",
    "    \"\"\"\n",
    "    This class implements a linear regression model from the SKLearn library.\n",
    "    \"\"\"\n",
    "    predictor_type = Regressor()\n",
    "    library = \"sklearn\"\n",
    "    predictor_name = name = f\"{library} Linear Regression\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict = None,\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        The constructor initializes the base params.\n",
    "        \"\"\"\n",
    "        super().__init__(data_df=data_df,\n",
    "                         cao_mapping=cao_mapping,\n",
    "                         data_split=data_split,\n",
    "                         model_params=model_params,\n",
    "                         metadata=metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "        multioutput = True\n",
    "        return multioutput\n",
    "\n",
    "    def build_model(self, model_params: Dict[str, Any]) -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function instantiates a Linear Regression model with the given params.\n",
    "        :return model: a LinearRegression instance\n",
    "        \"\"\"\n",
    "        model = linear_model.LinearRegression(**model_params)\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model: linear_model.LinearRegression,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray])\\\n",
    "            -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model and the desired metrics as a dictionary.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model: The linear regression model trained\n",
    "        \"\"\"\n",
    "        trained_model = model.fit(train_x, train_y)\n",
    "        return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d160c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Base: your PyTorch 14-output model ---------------------------------------\n",
    "class _TorchDomainPredictor(torch.nn.Module):\n",
    "    def __init__(self, n_domains=14):\n",
    "        super().__init__()\n",
    "        self.n_domains = n_domains\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_domains * 3, 100),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(100, n_domains),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class _TorchInferenceWrapper:\n",
    "    def __init__(self, torch_model, device=\"cpu\"):\n",
    "        import torch\n",
    "        self.m = torch_model.to(device).eval()\n",
    "        self.device = device\n",
    "        self.torch = torch\n",
    "    def predict(self, X_np):\n",
    "        with self.torch.no_grad():\n",
    "            X = self.torch.from_numpy(X_np).float().to(self.device)\n",
    "            Y = self.m(X).detach().cpu().numpy()  # shape (n, 14)\n",
    "        return Y\n",
    "\n",
    "class TorchRegressorPredictor(Predictor):\n",
    "    predictor_type = Regressor()\n",
    "    library = \"pytorch\"\n",
    "    predictor_name = name = \"PyTorch 14-output Regressor\"\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        return True  # 14 outputs\n",
    "\n",
    "    def build_model(self, model_params):\n",
    "        import torch\n",
    "        n_domains = int(model_params.get(\"n_domains\", 14))\n",
    "        device = str(model_params.get(\"device\", \"cpu\"))\n",
    "        model = _TorchDomainPredictor(n_domains=n_domains)\n",
    "        return {\"model\": model, \"device\": device}\n",
    "\n",
    "    def train_model(self, model_bundle, train_x, train_y, val_x, val_y):\n",
    "        import torch, os\n",
    "        model_path = self.model_params.get(\"model_path\")\n",
    "        assert model_path and os.path.exists(model_path), f\"model_path not found: {model_path}\"\n",
    "        model = model_bundle[\"model\"]\n",
    "        device = model_bundle[\"device\"]\n",
    "        state = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "        model.load_state_dict(state)\n",
    "        return _TorchInferenceWrapper(model, device=device)\n",
    "\n",
    "# ---- Wrapper: compute a single derived metric from the 14 predictions ----------\n",
    "class DerivedMetricPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Calls a 14-output base predictor, then maps the 14 ŷ to a 1D derived_metric.\n",
    "    \"\"\"\n",
    "    predictor_type = Regressor()\n",
    "    library = \"meta\"\n",
    "    predictor_name = name = \"Derived Metric from 14-output model\"\n",
    "\n",
    "    def __init__(self, *args, base_predictor=None, metric_name=\"derived_metric\", metric_params=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert base_predictor is not None, \"Pass base_predictor=...\"\n",
    "        self.base_predictor = base_predictor\n",
    "        self.metric_name = metric_name\n",
    "        self.metric_params = metric_params or {}\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        return False  # returns a single column\n",
    "\n",
    "    def build_model(self, model_params):\n",
    "        return None  # wrapper does not build a separate model\n",
    "\n",
    "    def train_model(self, model, train_x, train_y, val_x, val_y):\n",
    "        return self  # wrapper is already ready\n",
    "\n",
    "    def predict(self, features_df_or_np):\n",
    "        # 0) Ensure we have a DataFrame with the expected feature column order\n",
    "        if isinstance(features_df_or_np, pd.DataFrame):\n",
    "            X_df = features_df_or_np\n",
    "        else:\n",
    "            # Base class usually has self.context_actions_columns in order (context + actions)\n",
    "            # If your base class exposes a different attribute, use that here.\n",
    "            X_df = pd.DataFrame(features_df_or_np, columns=self.context_actions_columns)\n",
    "\n",
    "        # 1) Get the 14 predictions from the base model as a DataFrame\n",
    "        base_df = self.base_predictor.predict(X_df)     # shape (n, 14)\n",
    "        yhat = base_df.to_numpy()                       # (n, 14)\n",
    "\n",
    "        # 2) Extract actions and context as NumPy for fast indexing\n",
    "        actions_cols = self.cao_mapping[\"actions\"]      # length 14, ordered as your one-hot\n",
    "        context_cols = self.cao_mapping[\"context\"]      # expected like [d_1, d_1_ind, d_2, d_2_ind, ...]\n",
    "        A = X_df[actions_cols].to_numpy(copy=False)     # (n, 14), values in {0,1}\n",
    "\n",
    "        C = X_df[context_cols].to_numpy(copy=True)      # (n, 28) if alternating score/indicator\n",
    "        # If your “context” really is pairs [score, indicator] per domain, take the score columns:\n",
    "        # indices 0,2,4,...,26 → 14 columns\n",
    "        score_idxs = np.arange(0, C.shape[1], 2)\n",
    "        C_scores = C[:, score_idxs]                     # (n, 14) current scores by domain\n",
    "\n",
    "        # 3) Replace current score with predicted value when the action for that domain == 1\n",
    "        # Loop version (clear & fine for 14 columns)\n",
    "        for i in range(14):\n",
    "            mask = (A[:, i] == 1)\n",
    "            if mask.any():\n",
    "                C_scores[mask, i] = yhat[mask, i]\n",
    "\n",
    "        # Vectorized alternative (optional):\n",
    "        # masks = (A == 1)\n",
    "        # C_scores[masks] = yhat[masks]\n",
    "\n",
    "        # 4) Replace d_i_score with yhat[:, i] where target_i == 1\n",
    "        mask = (A == 1)                          # (n, 14) boolean/int\n",
    "        replaced_scores = C_scores.copy()\n",
    "        replaced_scores[mask] = yhat[mask]\n",
    "\n",
    "        # 5) Average ONLY over selected i; if none selected in a row, fallback to mean of original scores\n",
    "        sel_counts = mask.sum(axis=1)                         # (n,)\n",
    "        sum_selected = (replaced_scores * mask).sum(axis=1)   # (n,)\n",
    "        fallback = C_scores.mean(axis=1)                      # (n,)\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            derived = np.where(\n",
    "                sel_counts > 0,\n",
    "                sum_selected / sel_counts,\n",
    "                fallback\n",
    "            ).reshape(-1, 1)                                   # (n,1)\n",
    "\n",
    "        # 6) Return as a 1-col DataFrame with the expected outcome name\n",
    "        return pd.DataFrame(derived, columns=[self.metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afc98746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor_node_id = '704abc67-fe1c-409b-87c1-8e59864b7fe4'\n",
    "# predictor = LinearRegression(encoded_data_source_df,\n",
    "#     cao_mapping={'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['d_3_next']},\n",
    "# data_split=data_split,\n",
    "# model_params={},\n",
    "# metadata={})\n",
    "# --- Instantiate base Torch predictor (14 outputs) + wrapper (1 derived output) ---\n",
    "base_id    = \"torch-base-14\"\n",
    "derived_id = \"torch-derived-metric\"\n",
    "\n",
    "# outcomes for the BASE predictor: ensure order matches your model's 14 outputs\n",
    "base_outcomes = [f'd_{i}_next' for i in range(1, 15)]\n",
    "\n",
    "base_predictor = TorchRegressorPredictor(\n",
    "    encoded_data_source_df,\n",
    "    cao_mapping={**cao_mapping, \"outcomes\": base_outcomes},  # provide 14 names for base\n",
    "    data_split=data_split,\n",
    "    model_params={\n",
    "        \"model_path\": \"model.pt\",   # <-- set this to your REAL weights path\n",
    "        \"device\": \"cpu\",\n",
    "        \"n_domains\": 14\n",
    "    },\n",
    "    metadata={\"role\": \"base_14_output\"}\n",
    ")\n",
    "\n",
    "predictor = DerivedMetricPredictor(\n",
    "    encoded_data_source_df,\n",
    "    cao_mapping=cao_mapping,  # ['derived_metric']\n",
    "    data_split=data_split,\n",
    "    model_params={},\n",
    "    metadata={\"role\": \"derived_objective\"},\n",
    "    base_predictor=base_predictor\n",
    ")\n",
    "\n",
    "predictor_node_id = derived_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84165bf9",
   "metadata": {},
   "source": [
    "### Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9802378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = DataEncoder.encoded_df_to_np(predictor.train_x_df)\n",
    "train_y = DataEncoder.encoded_df_to_np(predictor.train_y_df)\n",
    "if predictor.val_x_df is not None:\n",
    "    val_x = DataEncoder.encoded_df_to_np(predictor.val_x_df)\n",
    "    val_y = DataEncoder.encoded_df_to_np(predictor.val_y_df)\n",
    "else:\n",
    "    val_x = None\n",
    "    val_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11aad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Torch weights for base; initialize wrapper; register wrapper for ESP\n",
    "train = True\n",
    "\n",
    "# === Prepare features/targets just like Cell 26 ===\n",
    "train_x = DataEncoder.encoded_df_to_np(predictor.train_x_df)\n",
    "train_y = DataEncoder.encoded_df_to_np(predictor.train_y_df)\n",
    "if predictor.val_x_df is not None:\n",
    "    val_x = DataEncoder.encoded_df_to_np(predictor.val_x_df)\n",
    "    val_y = DataEncoder.encoded_df_to_np(predictor.val_y_df)\n",
    "else:\n",
    "    val_x = val_y = None\n",
    "\n",
    "# 1️⃣ Base: build and “train” (really just loads the state_dict)\n",
    "_ = base_predictor.train_model(\n",
    "        base_predictor.build_model(base_predictor.model_params),\n",
    "        train_x, None, val_x, None)\n",
    "base_predictor.set_trained_model(_)\n",
    "\n",
    "# 2️⃣ Wrapper: build & finalize (no real training)\n",
    "_ = predictor.train_model(\n",
    "        predictor.build_model(predictor.model_params),\n",
    "        train_x, train_y, val_x, val_y)\n",
    "predictor.set_trained_model(_)\n",
    "\n",
    "# 3️⃣ Register only the derived predictor for ESP\n",
    "predictors_by_id[predictor_node_id] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660239",
   "metadata": {},
   "source": [
    "### Predictor metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eab3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor trained. Metrics: {'train_Mean Squared Error_derived_metric': 0.9888663113062405, 'test_Mean Squared Error_derived_metric': 0.0}\n"
     ]
    }
   ],
   "source": [
    "model_metrics_by_id[predictor_node_id] = [MetricsManager.get_calculator('Mean Squared Error')]\n",
    "metrics = MetricsManager.compute_metrics(predictor,\n",
    "model_metrics_by_id[predictor_node_id],predictor.train_x_df, predictor.train_y_df,predictor.val_x_df, predictor.val_y_df,predictor.test_x_df, predictor.test_y_df,encoder)\n",
    "\n",
    "print(f'Predictor trained. Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a19b68",
   "metadata": {},
   "source": [
    "## Prescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1dc11",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026d8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnileafPrescriptor(EspEvaluator):\n",
    "    \"\"\"\n",
    "    An Unileaf Prescriptor makes prescriptions given an ESP candidate and a context DataFrame.\n",
    "    It is also an EspEvaluator implementation that returns metrics for ESP candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: Dict[str, Any],\n",
    "                 evaluation_df: pd.DataFrame,\n",
    "                 data_encoder: DataEncoder,\n",
    "                 predictors: List[Predictor]):\n",
    "        \"\"\"\n",
    "        Constructs a prescriptor evaluator\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :param evaluation_df: the Pandas DataFrame to use to evaluate the candidates\n",
    "        :param data_encoder: the DataEncoder used to encode the dataset\n",
    "        :param predictors: the predictors this prescriptor relies on\n",
    "        \"\"\"\n",
    "        # Instantiate EspEvaluator\n",
    "        # Note: sets self.config\n",
    "        super().__init__(config)\n",
    "\n",
    "        # CAO\n",
    "        self.cao_mapping = {\"context\": self.get_context_field_names(config),\n",
    "                            \"actions\": self.get_action_field_names(config),\n",
    "                            \"outcomes\": self.get_fitness_metrics(config)}\n",
    "        self.context_df = evaluation_df[self.cao_mapping[\"context\"]]\n",
    "        self.row_index = self.context_df.index\n",
    "\n",
    "        # Convert the context DataFrame to a format a NN can ingest\n",
    "        self.context_as_nn_input = self.convert_to_nn_input(self.context_df)\n",
    "\n",
    "        # Data encoder\n",
    "        self.data_encoder = data_encoder\n",
    "\n",
    "        # Predictors\n",
    "        self.predictors = predictors\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_nn_input(context_df: pd.DataFrame) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Converts a context DataFrame to a list of numpy arrays a neural network can ingest\n",
    "        :param context_df: a DataFrame containing inputs for a neural network. Number of inputs and size must match\n",
    "        :return: a list of numpy ndarray, on ndarray per neural network input\n",
    "        \"\"\"\n",
    "        # The NN expects a list of i inputs by s samples (e.g. 9 x 299).\n",
    "        # So convert the data frame to a numpy array (gives shape 299 x 9), transpose it (gives 9 x 299)\n",
    "        # and convert to list(list of 9 arrays of 299)\n",
    "        context_as_nn_input = list(context_df.to_numpy().transpose())\n",
    "        # Convert each column's list of 1D array to a 2D array\n",
    "        context_as_nn_input = [np.stack(context_as_nn_input[i], axis=0) for i in\n",
    "                               range(len(context_as_nn_input))]\n",
    "        return context_as_nn_input\n",
    "\n",
    "    def evaluate_candidate(self, candidate):\n",
    "        \"\"\"\n",
    "        Evaluates a single Prescriptor candidate and returns its metrics.\n",
    "        Implements the EspEvaluator interface\n",
    "        :param candidate: a Keras neural network or rule based Prescriptor candidate\n",
    "        :return metrics: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Prescribe actions\n",
    "        prescribed_actions_df = self.prescribe(candidate)\n",
    "\n",
    "        # Aggregate the context and actions dataframes.\n",
    "        context_actions_df = pd.concat([self.context_df,\n",
    "                                        prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "\n",
    "        # Compute the metrics\n",
    "        metrics = self._compute_metrics(context_actions_df)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_metrics(self, context_actions_df):\n",
    "        \"\"\"\n",
    "        Computes metrics from the passed context/actions DataFrame using the instance's trained predictors.\n",
    "        :param context_actions_df: a DataFrame of context / prescribed actions\n",
    "        :return: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Get the predicted outcomes from the predictors\n",
    "        metrics = {}\n",
    "        for predictor in self.predictors:\n",
    "            predicted_outcomes = predictor.predict(context_actions_df)\n",
    "\n",
    "            # UN-853: Decode predictions before computing numerical metrics, if a data_encoder is available\n",
    "            if self.data_encoder is not None:\n",
    "                decoded_predicted_outcomes = self.data_encoder.decode_as_df(predicted_outcomes)\n",
    "            else:\n",
    "                decoded_predicted_outcomes = predicted_outcomes\n",
    "\n",
    "            # Only add a metric for the outcomes the prescriptor is interested in\n",
    "            for outcome in self.cao_mapping[\"outcomes\"]:\n",
    "                # Add the metrics that have been produced by this predictor\n",
    "                if outcome in predictor.cao_mapping[\"outcomes\"]:\n",
    "                    # Check the type of metric: numerical or categorical?\n",
    "                    if decoded_predicted_outcomes[[outcome]].iloc[:, 0].dtype == object:\n",
    "                        # Categorical outcome. Use the *encoded* predicted outcome.\n",
    "                        preds = predicted_outcomes[outcome]\n",
    "                        # Classifiers return the category's index in the list of categories, so we can take the mean\n",
    "                        # of the encoded outcomes. Note: this works because Outcomes are encoded using LabelEncoder\n",
    "                        # AND the user defined order for each Outcome categories.\n",
    "                        metrics[outcome] = preds.mean()\n",
    "                    else:\n",
    "                        # UN-853: Numerical outcome. Use the *decoded*, i.e. scaled back, predicted outcome\n",
    "                        preds = decoded_predicted_outcomes[outcome]\n",
    "                        # Regressors produce floats: take the mean of the decoded outcome\n",
    "                        metrics[outcome] = preds.mean()\n",
    "        return metrics\n",
    "\n",
    "    def prescribe(self, candidate, context_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed candidate and context\n",
    "        :param candidate: an ESP candidate, either neural network or rules\n",
    "        :param context_df: a DataFrame containing the context to prescribe for,\n",
    "         or None to use the instance one\n",
    "        :return: a DataFrame containing actions prescribed for each context\n",
    "        \"\"\"\n",
    "        if context_df is None:\n",
    "            # No context is provided, use the instance's one\n",
    "            context_as_nn_input = self.context_as_nn_input\n",
    "            row_index = self.row_index\n",
    "        else:\n",
    "            # Convert the context DataFrame to something more suitable for neural networks\n",
    "            context_as_nn_input = self.convert_to_nn_input(context_df)\n",
    "            # Use the context's row index\n",
    "            row_index = context_df.index\n",
    "\n",
    "        is_rule_based = isinstance(candidate, RuleSet)\n",
    "        if is_rule_based:\n",
    "            actions = self._prescribe_from_rules(candidate, context_as_nn_input)\n",
    "        else:\n",
    "            actions = self._prescribe_from_nn(candidate, context_as_nn_input)\n",
    "\n",
    "        # Convert the prescribed actions to a DataFrame\n",
    "        prescribed_actions_df = pd.DataFrame(actions,\n",
    "                                             columns=self.cao_mapping[\"actions\"],\n",
    "                                             index=row_index)\n",
    "        \n",
    "        # --- BEGIN: normalize array-valued action columns to scalars ---\n",
    "        actions = self.cao_mapping[\"actions\"]  # ['target_d_1', ..., 'target_d_14']\n",
    "\n",
    "        def _to_scalar_action(x):\n",
    "            \"\"\"\n",
    "            Accepts a scalar, list, tuple, or ndarray.\n",
    "            Returns a single 0/1 integer for the action.\n",
    "            Rules:\n",
    "            - scalar -> int(scalar)\n",
    "            - 1-length vector -> int(value)\n",
    "            - 2-length vector -> treat as [p0, p1] and use hard 1 if p1 >= 0.5 else 0\n",
    "            - >2-length vector -> argmax==1 -> 1 else 0 (fallback policy)\n",
    "            \"\"\"\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                a = np.asarray(x).ravel()\n",
    "                if a.size == 1:\n",
    "                    return int(a[0])\n",
    "                if a.size == 2:\n",
    "                    return int(a[1] >= 0.5)\n",
    "                # fallback for multi-class: treat class 1 as \"on\"\n",
    "                return int(np.argmax(a) == 1)\n",
    "            # scalar/None\n",
    "            return int(x) if (x is not None and x == x) else 0  # None/NaN -> 0\n",
    "\n",
    "        # Normalize only the action columns\n",
    "        for c in actions:\n",
    "            # If any array-like present, map to scalar\n",
    "            if prescribed_actions_df[c].apply(lambda v: isinstance(v, (list, tuple, np.ndarray))).any():\n",
    "                prescribed_actions_df[c] = prescribed_actions_df[c].map(_to_scalar_action).astype(int)\n",
    "            else:\n",
    "                # ensure ints if they already look scalar\n",
    "                prescribed_actions_df[c] = prescribed_actions_df[c].fillna(0).astype(int)\n",
    "\n",
    "        # Optional debug: confirm no arrays remain\n",
    "        print(\"array-valued left:\",\n",
    "              prescribed_actions_df[actions].apply(lambda s: s.apply(lambda v: isinstance(v,(list,tuple,np.ndarray)))).any())\n",
    "\n",
    "        # --- END: normalize array-valued action columns ---\n",
    "\n",
    "\n",
    "        # ### DEBUG:\n",
    "        # print(\"-------------DEBUG: Prescribed Actions (encoded):\")\n",
    "        # print(\"shape: \", prescribed_actions_df.shape)\n",
    "        # print(\"dtype: \", prescribed_actions_df.dtypes)\n",
    "        # has_arrays = prescribed_actions_df.applymap(lambda x: isinstance(x, (list, np.ndarray))).any() # type: ignore\n",
    "        # print(\"array-valued columns:\\n\", has_arrays[has_arrays].index.tolist())\n",
    "\n",
    "        # UN-2430 Decode the softmaxes, if any, back into categories\n",
    "        prescribed_actions_df = self.data_encoder.decode_as_df(prescribed_actions_df)\n",
    "        # UN0-240 Re-encode the actions into what the predictors expect (e.g. one-hots for categorical data)\n",
    "        prescribed_actions_df = self.data_encoder.encode_as_df(prescribed_actions_df)\n",
    "        return prescribed_actions_df\n",
    "\n",
    "    def _prescribe_from_rules(self, candidate, context_as_nn_input: List[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed rules model candidate and context\n",
    "        :param candidate: a rules model candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to list of action values\n",
    "        \"\"\"\n",
    "        cand_states = RuleSetConfigHelper.get_states(self.config)\n",
    "        cand_actions = RuleSetConfigHelper.get_actions(self.config)\n",
    "        candidate = RuleSetBinding(candidate, cand_states, cand_actions)\n",
    "        rules_encoder = RulesDataEncoder(candidate.actions)\n",
    "        evaluator = RuleSetBindingEvaluator()\n",
    "        rules_input = rules_encoder.encode_to_rules_data(context_as_nn_input)\n",
    "        rules_output = evaluator.evaluate(candidate, rules_input)\n",
    "        actions = rules_encoder.decode_from_rules_data(rules_output)\n",
    "        return actions\n",
    "\n",
    "    def _prescribe_from_nn(self, candidate, context_as_nn_input: List[np.ndarray]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed neural network candidate and context\n",
    "        :param candidate: a Keras neural network candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to action value or list of action values\n",
    "        \"\"\"\n",
    "        # Get the prescribed actions\n",
    "        prescribed_actions = candidate.predict(context_as_nn_input)\n",
    "        actions = {}\n",
    "\n",
    "        if self._is_single_action_prescriptor():\n",
    "            # Put the single action in an array to process it like multiple actions\n",
    "            prescribed_actions = [prescribed_actions]\n",
    "\n",
    "        for index, action_col in enumerate(self.cao_mapping[\"actions\"]):\n",
    "            if self._is_scalar(prescribed_actions[index]):\n",
    "                # We have a single row and this action is numerical. Convert it to a scalar.\n",
    "                actions[action_col] = prescribed_actions[index].item()\n",
    "            else:\n",
    "                actions[action_col] = prescribed_actions[index].tolist()\n",
    "        return actions\n",
    "\n",
    "    def _is_single_action_prescriptor(self):\n",
    "        \"\"\"\n",
    "        Checks how many Actions have been defined in the Context, Actions, Outcomes mapping.\n",
    "        :return: True if only 1 action is defined, False otherwise\n",
    "        \"\"\"\n",
    "        return len(self.cao_mapping[\"actions\"]) == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_scalar(prescribed_action):\n",
    "        \"\"\"\n",
    "        Checks if the prescribed action contains a single value, i.e. a scalar, or an array.\n",
    "        A prescribed action contains a single value if it has been prescribed for a single context sample\n",
    "        :param prescribed_action: a scalar or an array\n",
    "        :return: True if the prescribed action contains a scalar, False otherwise.\n",
    "        \"\"\"\n",
    "        return prescribed_action.shape[0] == 1 and prescribed_action.shape[1] == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_context_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Context column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Context column names\n",
    "        \"\"\"\n",
    "        nn_inputs = config[\"network\"][\"inputs\"]\n",
    "        contexts = [nn_input[\"name\"] for nn_input in nn_inputs]\n",
    "        return contexts\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Action column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Action column names\n",
    "        \"\"\"\n",
    "        nn_outputs = config[\"network\"][\"outputs\"]\n",
    "        actions = [nn_output[\"name\"] for nn_output in nn_outputs]\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fitness_metrics(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of fitness metric names (Outcomes) to optimize.\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of fitness metric names\n",
    "        \"\"\"\n",
    "        metrics = config[\"evolution\"][\"fitness\"]\n",
    "        fitness_metrics = [metric[\"metric_name\"] for metric in metrics]\n",
    "        return fitness_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e500f",
   "metadata": {},
   "source": [
    "### Prescriptor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a68c60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'evolution': {'fitness': [{'maximize': True, 'metric_name': 'derived_metric'}], 'nb_elites': 5, 'mutation_type': 'gaussian_noise_percentage', 'nb_generations': 5, 'mutation_factor': 0.1, 'population_size': 10, 'parent_selection': 'tournament', 'initialization_range': 1, 'mutation_probability': 0.1, 'remove_population_pct': 0.8, 'initialization_distribution': 'orthogonal'}, 'network': {'inputs': [{'name': 'd_1_ind', 'size': 1, 'values': ['float']}, {'name': 'd_2_ind', 'size': 1, 'values': ['float']}, {'name': 'd_3_ind', 'size': 1, 'values': ['float']}, {'name': 'd_4_ind', 'size': 1, 'values': ['float']}, {'name': 'd_5_ind', 'size': 1, 'values': ['float']}, {'name': 'd_6_ind', 'size': 1, 'values': ['float']}, {'name': 'd_7_ind', 'size': 1, 'values': ['float']}, {'name': 'd_8_ind', 'size': 1, 'values': ['float']}, {'name': 'd_9_ind', 'size': 1, 'values': ['float']}, {'name': 'd_10_ind', 'size': 1, 'values': ['float']}, {'name': 'd_11_ind', 'size': 1, 'values': ['float']}, {'name': 'd_12_ind', 'size': 1, 'values': ['float']}, {'name': 'd_13_ind', 'size': 1, 'values': ['float']}, {'name': 'd_14_ind', 'size': 1, 'values': ['float']}, {'name': 'd_1_score', 'size': 1, 'values': ['float']}, {'name': 'd_2_score', 'size': 1, 'values': ['float']}, {'name': 'd_3_score', 'size': 1, 'values': ['float']}, {'name': 'd_4_score', 'size': 1, 'values': ['float']}, {'name': 'd_5_score', 'size': 1, 'values': ['float']}, {'name': 'd_6_score', 'size': 1, 'values': ['float']}, {'name': 'd_7_score', 'size': 1, 'values': ['float']}, {'name': 'd_8_score', 'size': 1, 'values': ['float']}, {'name': 'd_9_score', 'size': 1, 'values': ['float']}, {'name': 'd_10_score', 'size': 1, 'values': ['float']}, {'name': 'd_11_score', 'size': 1, 'values': ['float']}, {'name': 'd_12_score', 'size': 1, 'values': ['float']}, {'name': 'd_13_score', 'size': 1, 'values': ['float']}, {'name': 'd_14_score', 'size': 1, 'values': ['float']}], 'outputs': [{'name': 'target_d_1', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_2', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_3', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_4', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_5', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_6', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_7', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_8', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_9', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_10', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_11', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_12', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_13', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_14', 'size': 2, 'activation': 'softmax', 'use_bias': True, 'values': ['0.0', '1.0']}], 'hidden_layers': [{'layer_name': 'hidden_1', 'layer_type': 'Dense', 'layer_params': {'units': 16, 'use_bias': True, 'activation': 'tanh'}}]}, 'LEAF': {'representation': 'NNWeights', 'experiment_id': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'version': '1.0.0', 'persistence_dir': 'trained_prescriptors/', 'candidates_to_persist': 'best'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b12f0ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "LEAF": {
        "candidates_to_persist": "best",
        "experiment_id": "UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726",
        "persistence_dir": "trained_prescriptors/",
        "representation": "NNWeights",
        "version": "1.0.0"
       },
       "evolution": {
        "fitness": [
         {
          "maximize": true,
          "metric_name": "derived_metric"
         }
        ],
        "initialization_distribution": "orthogonal",
        "initialization_range": 1,
        "mutation_factor": 0.1,
        "mutation_probability": 0.1,
        "mutation_type": "gaussian_noise_percentage",
        "nb_elites": 5,
        "nb_generations": 5,
        "parent_selection": "tournament",
        "population_size": 10,
        "remove_population_pct": 0.8
       },
       "network": {
        "hidden_layers": [
         {
          "layer_name": "hidden_1",
          "layer_params": {
           "activation": "tanh",
           "units": 16,
           "use_bias": true
          },
          "layer_type": "Dense"
         }
        ],
        "inputs": [
         {
          "name": "d_1_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_2_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_3_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_4_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_5_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_6_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_7_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_8_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_9_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_10_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_11_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_12_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_13_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_14_ind",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_1_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_2_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_3_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_4_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_5_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_6_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_7_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_8_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_9_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_10_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_11_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_12_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_13_score",
          "size": 1,
          "values": [
           "float"
          ]
         },
         {
          "name": "d_14_score",
          "size": 1,
          "values": [
           "float"
          ]
         }
        ],
        "outputs": [
         {
          "activation": "softmax",
          "name": "target_d_1",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_2",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_3",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_4",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_5",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_6",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_7",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_8",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_9",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_10",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_11",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_12",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_13",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         },
         {
          "activation": "softmax",
          "name": "target_d_14",
          "size": 2,
          "use_bias": true,
          "values": [
           "0.0",
           "1.0"
          ]
         }
        ]
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7928d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_predictor_ids = ['704abc67-fe1c-409b-87c1-8e59864b7fe4']\n",
    "# all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]\n",
    "required_predictor_ids = ['torch-derived-metric']\n",
    "all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d3be7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "  experiment_id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726\n",
      "  checkpoint_id: None\n",
      "  timestamp: 20251027-144621\n",
      "Asking ESP for a seed generation...\n",
      "Seed generation received.\n",
      "Evaluating PopulationResponse for generation 1...:\n",
      "PopulationResponse:\n",
      "  Generation: 1\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1/20251027-204621\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "Evaluation done.\n",
      "Reporting evaluated population for generation 1 and asking ESP for generation 2...:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 data persisted.\n",
      "Evaluated candidates:\n",
      "Id: 1_10 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_10', 'origin': '(none)'} Metrics: ['derived_metric: 0.0023693626117164968', 'is_elite: False']\n",
      "Id: 1_3 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_3', 'origin': '(none)'} Metrics: ['derived_metric: 0.004043909732914356', 'is_elite: False']\n",
      "Id: 1_7 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_7', 'origin': '(none)'} Metrics: ['derived_metric: 0.004524801183054348', 'is_elite: False']\n",
      "Id: 1_6 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_6', 'origin': '(none)'} Metrics: ['derived_metric: 0.005024726961153192', 'is_elite: False']\n",
      "Id: 1_2 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_2', 'origin': '(none)'} Metrics: ['derived_metric: 0.00519452586195257', 'is_elite: False']\n",
      "Id: 1_4 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_4', 'origin': '(none)'} Metrics: ['derived_metric: 0.006144969959374672', 'is_elite: True']\n",
      "Id: 1_1 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_1', 'origin': '(none)'} Metrics: ['derived_metric: 0.0062119227281073105', 'is_elite: True']\n",
      "Id: 1_8 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_8', 'origin': '(none)'} Metrics: ['derived_metric: 0.006632314455297698', 'is_elite: True']\n",
      "Id: 1_5 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_5', 'origin': '(none)'} Metrics: ['derived_metric: 0.0073334602490528296', 'is_elite: True']\n",
      "Id: 1_9 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_9', 'origin': '(none)'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: True']\n",
      "\n",
      "Done with generation 1.\n",
      "--------\n",
      "\n",
      "Evaluating PopulationResponse for generation 2...:\n",
      "PopulationResponse:\n",
      "  Generation: 2\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/2/20251027-204622\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "Evaluation done.\n",
      "Reporting evaluated population for generation 2 and asking ESP for generation 3...:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 data persisted.\n",
      "Evaluated candidates:\n",
      "Id: 2_9 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_5'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_9', 'origin': '1_9~CUW~1_5#MGNP'} Metrics: ['derived_metric: 0.005270203925907221', 'is_elite: False']\n",
      "Id: 1_4 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_4', 'origin': '(none)'} Metrics: ['derived_metric: 0.006144969959374672', 'is_elite: False']\n",
      "Id: 1_1 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_1', 'origin': '(none)'} Metrics: ['derived_metric: 0.0062119227281073105', 'is_elite: False']\n",
      "Id: 1_8 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_8', 'origin': '(none)'} Metrics: ['derived_metric: 0.006632314455297698', 'is_elite: False']\n",
      "Id: 1_5 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_5', 'origin': '(none)'} Metrics: ['derived_metric: 0.0073334602490528296', 'is_elite: False']\n",
      "Id: 2_6 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_6', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.009437807092971135', 'is_elite: True']\n",
      "Id: 2_7 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_7', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.009437807092971135', 'is_elite: True']\n",
      "Id: 1_9 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_9', 'origin': '(none)'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: True']\n",
      "Id: 2_8 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_8', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: True']\n",
      "Id: 2_10 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_5', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_10', 'origin': '1_5~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "\n",
      "Done with generation 2.\n",
      "--------\n",
      "\n",
      "Evaluating PopulationResponse for generation 3...:\n",
      "PopulationResponse:\n",
      "  Generation: 3\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/3/20251027-204624\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "Evaluation done.\n",
      "Reporting evaluated population for generation 3 and asking ESP for generation 4...:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 data persisted.\n",
      "Evaluated candidates:\n",
      "Id: 3_6 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '1_9'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_6', 'origin': '2_10~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.0036398212405401864', 'is_elite: False']\n",
      "Id: 3_8 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_8', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.008937229578285561', 'is_elite: False']\n",
      "Id: 2_6 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_6', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.009437807092971135', 'is_elite: False']\n",
      "Id: 2_7 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_7', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.009437807092971135', 'is_elite: False']\n",
      "Id: 1_9 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_9', 'origin': '(none)'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: True']\n",
      "Id: 2_8 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_9', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_8', 'origin': '1_9~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: False']\n",
      "Id: 2_10 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_5', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_10', 'origin': '1_5~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_7 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_7', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_9 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_9', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_10 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_10', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "\n",
      "Done with generation 3.\n",
      "--------\n",
      "\n",
      "Evaluating PopulationResponse for generation 4...:\n",
      "PopulationResponse:\n",
      "  Generation: 4\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/4/20251027-204625\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "Evaluation done.\n",
      "Reporting evaluated population for generation 4 and asking ESP for generation 5...:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 data persisted.\n",
      "Evaluated candidates:\n",
      "Id: 4_10 Identity: {'ancestor_count': 3, 'ancestor_ids': ['2_10', '3_7'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_10', 'origin': '2_10~CUW~3_7#MGNP'} Metrics: ['derived_metric: 0.00944546999198792', 'is_elite: False']\n",
      "Id: 1_9 Identity: {'ancestor_count': 0, 'ancestor_ids': [], 'birth_generation': 1, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '1_9', 'origin': '(none)'} Metrics: ['derived_metric: 0.010282149704820883', 'is_elite: False']\n",
      "Id: 4_7 Identity: {'ancestor_count': 3, 'ancestor_ids': ['3_7', '2_10'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_7', 'origin': '3_7~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.012071414009551518', 'is_elite: False']\n",
      "Id: 4_8 Identity: {'ancestor_count': 3, 'ancestor_ids': ['2_10', '3_7'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_8', 'origin': '2_10~CUW~3_7#MGNP'} Metrics: ['derived_metric: 0.012768478980722158', 'is_elite: False']\n",
      "Id: 2_10 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_5', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_10', 'origin': '1_5~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_7 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_7', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_9 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_9', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_10 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_10', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 4_6 Identity: {'ancestor_count': 3, 'ancestor_ids': ['2_10', '3_7'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_6', 'origin': '2_10~CUW~3_7#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: False']\n",
      "Id: 4_9 Identity: {'ancestor_count': 3, 'ancestor_ids': ['2_10', '3_7'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_9', 'origin': '2_10~CUW~3_7#MGNP'} Metrics: ['derived_metric: 0.014142172971332911', 'is_elite: True']\n",
      "\n",
      "Done with generation 4.\n",
      "--------\n",
      "\n",
      "Evaluating PopulationResponse for generation 5...:\n",
      "PopulationResponse:\n",
      "  Generation: 5\n",
      "  Population size: 10\n",
      "  Checkpoint id: UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/5/20251027-204626\n",
      "Evaluating candidates synchronously because max_workers == 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "array-valued left: target_d_1     False\n",
      "target_d_2     False\n",
      "target_d_3     False\n",
      "target_d_4     False\n",
      "target_d_5     False\n",
      "target_d_6     False\n",
      "target_d_7     False\n",
      "target_d_8     False\n",
      "target_d_9     False\n",
      "target_d_10    False\n",
      "target_d_11    False\n",
      "target_d_12    False\n",
      "target_d_13    False\n",
      "target_d_14    False\n",
      "dtype: bool\n",
      "Evaluation done.\n",
      "Reporting evaluated population for generation 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 data persisted.\n",
      "Evaluated candidates:\n",
      "Id: 5_10 Identity: {'ancestor_count': 4, 'ancestor_ids': ['4_9', '2_10'], 'birth_generation': 5, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '5_10', 'origin': '4_9~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.00950224204098049', 'is_elite: False']\n",
      "Id: 5_6 Identity: {'ancestor_count': 4, 'ancestor_ids': ['2_10', '4_9'], 'birth_generation': 5, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '5_6', 'origin': '2_10~CUW~4_9#MGNP'} Metrics: ['derived_metric: 0.013276719394424517', 'is_elite: False']\n",
      "Id: 2_10 Identity: {'ancestor_count': 1, 'ancestor_ids': ['1_5', '1_9'], 'birth_generation': 2, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '2_10', 'origin': '1_5~CUW~1_9#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: True']\n",
      "Id: 3_7 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_7', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: False']\n",
      "Id: 3_9 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_9', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: False']\n",
      "Id: 3_10 Identity: {'ancestor_count': 2, 'ancestor_ids': ['2_10', '2_10'], 'birth_generation': 3, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '3_10', 'origin': '2_10~CUW~2_10#MGNP'} Metrics: ['derived_metric: 0.013333491443417087', 'is_elite: False']\n",
      "Id: 5_8 Identity: {'ancestor_count': 4, 'ancestor_ids': ['4_9', '4_9'], 'birth_generation': 5, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '5_8', 'origin': '4_9~CUW~4_9#MGNP'} Metrics: ['derived_metric: 0.013577160508637982', 'is_elite: True']\n",
      "Id: 5_9 Identity: {'ancestor_count': 4, 'ancestor_ids': ['2_10', '4_9'], 'birth_generation': 5, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '5_9', 'origin': '2_10~CUW~4_9#MGNP'} Metrics: ['derived_metric: 0.013583558663412987', 'is_elite: True']\n",
      "Id: 5_7 Identity: {'ancestor_count': 4, 'ancestor_ids': ['4_9', '4_9'], 'birth_generation': 5, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '5_7', 'origin': '4_9~CUW~4_9#MGNP'} Metrics: ['derived_metric: 0.01362100918777287', 'is_elite: True']\n",
      "Id: 4_9 Identity: {'ancestor_count': 3, 'ancestor_ids': ['2_10', '3_7'], 'birth_generation': 4, 'domain_name': None, 'experiment_version': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'unique_id': '4_9', 'origin': '2_10~CUW~3_7#MGNP'} Metrics: ['derived_metric: 0.014142172971332911', 'is_elite: True']\n",
      "\n",
      "Done with generation 5.\n",
      "--------\n",
      "\n",
      "Done training in 6.34 seconds.\n",
      "representation: NNWeights\n",
      "next_population average time: 0.0824\n",
      "evaluate_population average time: 1.0766\n",
      "next_population times: [0.10984611511230469, 0.0753941535949707, 0.07683515548706055, 0.07611703872680664, 0.0738821029663086]\n",
      "evaluate_population times: [1.2009401321411133, 1.1069157123565674, 1.017591953277588, 1.0460460186004639, 1.011709213256836]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the EspService\n",
    "esp_service = EspService(config)\n",
    "esp_evaluator = UnileafPrescriptor(config,\n",
    "                                   encoded_data_source_df,\n",
    "                                   encoder,\n",
    "                                   all_predictors)\n",
    "experiment_results_dir = esp_service.train(esp_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a3f30",
   "metadata": {},
   "source": [
    "## ESP Summary Stats\n",
    "esp_service.train(...) returned the directory in which the experiment results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fe5ba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generation</th>\n",
       "      <th>checkpoint_id</th>\n",
       "      <th>max_derived_metric</th>\n",
       "      <th>min_derived_metric</th>\n",
       "      <th>mean_derived_metric</th>\n",
       "      <th>elites_mean_derived_metric</th>\n",
       "      <th>cid_min_derived_metric</th>\n",
       "      <th>cid_max_derived_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1...</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>1_10</td>\n",
       "      <td>1_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/2...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>2_9</td>\n",
       "      <td>2_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/3...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>3_6</td>\n",
       "      <td>2_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/4...</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>4_10</td>\n",
       "      <td>4_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/5...</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>5_10</td>\n",
       "      <td>4_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   generation                                      checkpoint_id  \\\n",
       "0           1  UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1...   \n",
       "1           2  UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/2...   \n",
       "2           3  UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/3...   \n",
       "3           4  UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/4...   \n",
       "4           5  UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/5...   \n",
       "\n",
       "   max_derived_metric  min_derived_metric  mean_derived_metric  \\\n",
       "0            0.010282            0.002369             0.005776   \n",
       "1            0.013333            0.005270             0.008437   \n",
       "2            0.013333            0.003640             0.010535   \n",
       "3            0.014142            0.009445             0.012538   \n",
       "4            0.014142            0.009502             0.013104   \n",
       "\n",
       "   elites_mean_derived_metric cid_min_derived_metric cid_max_derived_metric  \n",
       "0                    0.007321                   1_10                    1_9  \n",
       "1                    0.010555                    2_9                   2_10  \n",
       "2                    0.012723                    3_6                   2_10  \n",
       "3                    0.013495                   4_10                    4_9  \n",
       "4                    0.013651                   5_10                    4_9  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_file = os.path.join(experiment_results_dir, 'experiment_stats.csv')\n",
    "with open(stats_file) as csv_file:\n",
    "    stats_df = pd.read_csv(csv_file, sep=',')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637bf3",
   "metadata": {},
   "source": [
    "## ESP Summary Plot\n",
    "esp_service.train(...) generated a plot summarizing the experiment's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a817dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1753f8a",
   "metadata": {},
   "source": [
    "### derived_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51a0d524",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_prescriptors/UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1.0.0_20251027-144621/known_domain_avg.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m plot_file = os.path.join(experiment_results_dir, \u001b[33m'\u001b[39m\u001b[33mknown_domain_avg.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/IPython/core/display.py:1025\u001b[39m, in \u001b[36mImage.__init__\u001b[39m\u001b[34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28mself\u001b[39m.unconfined = unconfined\n\u001b[32m   1024\u001b[39m \u001b[38;5;28mself\u001b[39m.alt = alt\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.get(\u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m, {}):\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28mself\u001b[39m.width = metadata[\u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/IPython/core/display.py:343\u001b[39m, in \u001b[36mDisplayObject.__init__\u001b[39m\u001b[34m(self, data, url, filename, metadata)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata = {}\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28mself\u001b[39m._check_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/IPython/core/display.py:1060\u001b[39m, in \u001b[36mImage.reload\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed:\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retina:\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28mself\u001b[39m._retina_shape()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/IPython/core/display.py:369\u001b[39m, in \u001b[36mDisplayObject.reload\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    368\u001b[39m     encoding = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    370\u001b[39m         \u001b[38;5;28mself\u001b[39m.data = f.read()\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'trained_prescriptors/UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726/1.0.0_20251027-144621/known_domain_avg.png'"
     ]
    }
   ],
   "source": [
    "plot_file = os.path.join(experiment_results_dir, 'known_domain_avg.png')\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71375ad",
   "metadata": {},
   "source": [
    "# Models usage\n",
    "## Load the Prescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last row of the stats DataFrame, i.e. the last generation, to find the best model\n",
    "last_gen = stats_df['generation'].iloc[-1]\n",
    "best_score = stats_df['max_derived_metric'].iloc[-1]\n",
    "cid_best_score = stats_df['cid_max_derived_metric'].iloc[-1]\n",
    "prescriptor_model_filename = os.path.join(experiment_results_dir,\n",
    "                                          str(last_gen),\n",
    "                                          cid_best_score + '.h5')\n",
    "print(f'Best max_derived_metric average is {best_score:.3f} for candidate id {cid_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(f'Loading prescriptor model: {prescriptor_model_filename}')\n",
    "prescriptor_model = load_model(prescriptor_model_filename, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77623b3",
   "metadata": {},
   "source": [
    "## Get a sample context\n",
    "Get the context from one of the rows in the dataset, and make a prescription for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82356a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_source_df.sample(1)\n",
    "sample_context_df = sample_df[CONTEXT_COLUMNS]\n",
    "sample_context_action_df = sample_df[CONTEXT_ACTION_COLUMNS]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6202e",
   "metadata": {},
   "source": [
    "### Prescribe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c30780",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_context_df = encoder.encode_as_df(sample_context_df)\n",
    "encoded_prescribed_actions_df = esp_evaluator.prescribe(prescriptor_model, encoded_sample_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effa5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the context and actions dataframes.\n",
    "encoded_context_actions_df = pd.concat([encoded_sample_context_df,\n",
    "                                        encoded_prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "sample_context_prescribed_action_df = encoder.decode_as_df(encoded_context_actions_df)\n",
    "sample_context_prescribed_action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f37e8",
   "metadata": {},
   "source": [
    "### Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(predictors, context_action_df, encoder):\n",
    "    pred_array = []\n",
    "    for predictor in predictors:\n",
    "        pred = predictor.predict(encoder.encode_as_df(context_action_df))\n",
    "        pred_array.append(encoder.decode_as_df(pred))\n",
    "    preds_df = pd.concat(pred_array, axis=1)\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccba3d",
   "metadata": {},
   "source": [
    "#### With original actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_actions_preds = get_predictions(all_predictors, sample_context_action_df, encoder)\n",
    "original_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416793",
   "metadata": {},
   "source": [
    "#### With prescribed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescribed_actions_preds = get_predictions(all_predictors, sample_context_prescribed_action_df, encoder)\n",
    "prescribed_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957136",
   "metadata": {},
   "source": [
    "#### With custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_custom_action_df = sample_context_prescribed_action_df.copy()\n",
    " # TODO: Uncomment and replace by the name of the actions(s) to customize\n",
    "# sample_context_custom_action_df['SOME_ACTION_TO_CUSTOMIZE'] = 42\n",
    "sample_context_custom_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_actions_preds = get_predictions(all_predictors, sample_context_custom_action_df, encoder)\n",
    "custom_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70319159",
   "metadata": {},
   "source": [
    "### Compare\n",
    "Compare 3 rows in a single table:\n",
    "- the original sample\n",
    "- the sample with the prescribed actions\n",
    "- the sample with some custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME_COLUMNS = list(original_actions_preds.columns)\n",
    "OUTCOME_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed OUTCOMES for the sample\n",
    "pd.DataFrame(sample_df[OUTCOME_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context and actions for 3 rows:\n",
    "# - the original sample\n",
    "# - the sample with the prescribed actions\n",
    "# - the sample with some custom actions\n",
    "comp_df = pd.concat([sample_context_action_df,\n",
    "                     sample_context_prescribed_action_df,\n",
    "                     sample_context_custom_action_df], axis=0)\n",
    "\n",
    "# Compute the outcomes\n",
    "outcomes_dict = {}\n",
    "for outcome in OUTCOME_COLUMNS:\n",
    "    # Observed outcome from the sample in the dataset\n",
    "    outcomes_dict[outcome] = [sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0]]\n",
    "    # Predicted outcome\n",
    "    outcomes_dict[f'{outcome}_predicted'] = [original_actions_preds[outcome].iloc[0],\n",
    "                                             prescribed_actions_preds[outcome].iloc[0],\n",
    "                                             custom_actions_preds[outcome].iloc[0]]\n",
    "    # For numerical outcomes, compute the diff between predicted and observed\n",
    "    if is_numeric_dtype(outcomes_dict[outcome][0]):\n",
    "        diff = [a - b for a, b in zip(outcomes_dict[f'{outcome}_predicted'],\n",
    "                                      outcomes_dict[outcome])]\n",
    "        outcomes_dict[f'{outcome}_diff'] = diff\n",
    "    \n",
    "outcomes_df = pd.DataFrame(outcomes_dict)\n",
    "comp_df[list(outcomes_dict.keys())] = outcomes_df[list(outcomes_dict.keys())].values\n",
    "comp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f111",
   "metadata": {},
   "source": [
    "## Initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = sample_df[CONTEXT_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = sample_df[ACTION_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = sample_df[OUTCOME_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(outcomes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

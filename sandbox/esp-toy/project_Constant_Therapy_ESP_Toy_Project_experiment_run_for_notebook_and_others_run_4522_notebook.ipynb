{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7963fa52",
   "metadata": {},
   "source": [
    "# Project: Constant Therapy ESP Toy Project\n",
    "**Project Description**: Playing with ESP for Constant Therapy  \n",
    "**Experiment**: run for notebook and others  \n",
    "**Run ID**: 4522  \n",
    "**Notebook**: Neuro AI - Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numbers\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "import pandas as pd\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from io import StringIO\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Type\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from leaf_common.candidates.representation_types import RepresentationType\n",
    "from leaf_common.representation.rule_based.config.rule_set_config_helper import RuleSetConfigHelper\n",
    "from leaf_common.representation.rule_based.data.features import Features\n",
    "from leaf_common.representation.rule_based.data.rule_set import RuleSet\n",
    "from leaf_common.representation.rule_based.data.rule_set_binding import RuleSetBinding\n",
    "from leaf_common.representation.rule_based.evaluation.rule_set_binding_evaluator import RuleSetBindingEvaluator\n",
    "from leaf_common.representation.rule_based.persistence.rule_set_file_persistence import RuleSetFilePersistence\n",
    "from esp_sdk.esp_evaluator import EspEvaluator\n",
    "from esp_sdk.esp_service import EspService\n",
    "from unileaf_util.framework.data.profiling.dataframe_profiler import DataFrameProfiler\n",
    "from unileaf_util.framework.interfaces.data_frame_predictor import DataFramePredictor\n",
    "from unileaf_util.framework.metrics.metrics_manager import MetricsManager\n",
    "from unileaf_util.framework.transformers.data_encoder import DataEncoder\n",
    "from unileaf_util.framework.transformers.rules_data_encoder import RulesDataEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d18e",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "By default, load the dataset exported with the notebook, but you may plug your own dataset by changing the path for DATASET_CSV here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset csv file\n",
    "DATASET_CSV = 'esp_toy_real.csv'\n",
    "with open(DATASET_CSV) as df_file:\n",
    "    data_source_df = pd.read_csv(df_file)\n",
    "data_source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ab931",
   "metadata": {},
   "source": [
    "## Encode the dataset\n",
    "Encode the dataset using the fields definition from the Experiment's data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = DataFrameProfiler()\n",
    "data_profile = profiler.profile_data_frame(data_source_df)\n",
    "\n",
    "# Get fields from the data profile\n",
    "fields = data_profile.get('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e18793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b55d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cao_mapping = {'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['derived_metric']}\n",
    "cao_fields =  set(cao_mapping['context'] + cao_mapping['actions'] + cao_mapping['outcomes'])\n",
    "\n",
    "# Validate if the fields match with cao_mapping\n",
    "missing_fields = set(fields.keys()) - cao_fields # fields is a dictionary # type: ignore\n",
    "extra_fields =  cao_fields - set(fields.keys()) # fields is a dictionary # type: ignore\n",
    "if missing_fields != set():\n",
    "    print(f'The dataset contains fields that are NOT part of cao_mapping: {missing_fields}')\n",
    "    print('Please add them to the cao_mapping dictionary and make sure the rest of the notebook handles them correctly.')\n",
    "if extra_fields != set():\n",
    "    print(f'The cao_mapping contains fields that are NOT part of the dataset: {extra_fields}')\n",
    "    print('Please remove them from the cao_mapping dictionary and make sure they are not used in the rest of the notebook.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(cao_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148181ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_cols = [f'd_{i}_next' for i in range(1, 15)]\n",
    "missing = [c for c in next_cols if c not in data_source_df.columns]\n",
    "assert not missing, f\"Missing raw next-domain columns: {missing}\"\n",
    "\n",
    "# Example metric: average of 14 true next scores (replace with your formula)\n",
    "data_source_df['derived_metric'] = data_source_df[next_cols].mean(axis=1).astype(float)\n",
    "\n",
    "# Ensure 'derived_metric' exists in the fields dict so the encoder will build a transformer\n",
    "# Clone the schema of an existing numeric next-domain column to be safe:\n",
    "template_key = next_cols[0]  # e.g., 'd_1_next'\n",
    "if 'derived_metric' not in fields:\n",
    "    fields['derived_metric'] = dict(fields[template_key])\n",
    "    # If your fields have a 'name' or 'original_name' key, update it:\n",
    "    for k in ('name', 'original_name'):\n",
    "        if k in fields['derived_metric']:\n",
    "            fields['derived_metric'][k] = 'derived_metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DataEncoder(fields, cao_mapping) # fields is a dictionary # type: ignore\n",
    "encoded_data_source_df = encoder.encode_as_df(data_source_df)\n",
    "encoded_data_source_df.head()\n",
    "assert 'derived_metric' in encoded_data_source_df.columns, \"derived_metric missing from encoded_data_source_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa44693",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcb394",
   "metadata": {},
   "source": [
    "## Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f466999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initiate global variables\n",
    "\n",
    "REGRESSOR = 'regressor'\n",
    "CLASSIFIER = 'classifier'\n",
    "TYPES = [REGRESSOR, CLASSIFIER]\n",
    "predictors_by_id = {}\n",
    "model_metrics_by_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorType:\n",
    "    \"\"\"\n",
    "    This class defines the type of Predictor Possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predictor_type: str):\n",
    "        \"\"\"\n",
    "        The constructor confirms if the type of predictor is supported.\n",
    "        :param predictor_type: String describing a name for the type of the\n",
    "        predictor.\n",
    "        \"\"\"\n",
    "        assert predictor_type in TYPES, \"Invalid Predictor Type\"\n",
    "        self.type = predictor_type\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        This function overrides the string representation of the\n",
    "        class.\n",
    "        :return self.type: String\n",
    "        \"\"\"\n",
    "        return self.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(DataFramePredictor, ABC):\n",
    "    \"\"\"\n",
    "    This class contains the contract that any predictor\n",
    "    must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict[str, float],\n",
    "                 model_params: Dict = None, # type: ignore\n",
    "                 metadata: Dict = None): # type: ignore\n",
    "        \"\"\"\n",
    "        Initializes a predictor, its params and the metadata.\n",
    "        :param data_df: DataFrame containing all processed data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes`\n",
    "        keys where each key returns a List of the selected column names as strings.\n",
    "        :param data_split: Dictionary containing the training splits indexed\n",
    "        by \"train_pct\" and \"val_pct\".\n",
    "        :param model_params: Parameters of the model\n",
    "        :param metadata: Dictionary describing any other information\n",
    "        that must be stored along with the model.\n",
    "        This might help in uniquely identifying the model\n",
    "        :returns nothing\n",
    "        \"\"\"\n",
    "        # Split the data between train, val and test sets\n",
    "        self.data_split = data_split\n",
    "\n",
    "        self.cao_mapping = cao_mapping\n",
    "        self.context_actions_columns = self.cao_mapping[\"context\"] + self.cao_mapping[\"actions\"]\n",
    "        # Check\n",
    "        if len(cao_mapping[\"outcomes\"]) > 1:\n",
    "            if not self.does_support_multiobjective():\n",
    "                raise ValueError(f\"{self.predictor_name} does NOT support multiple outputs\")\n",
    "\n",
    "        self.column_length = {}\n",
    "        if data_df is not None:\n",
    "            train_df, val_df, test_df = self.generate_data_split(data_df, self.data_split)\n",
    "\n",
    "            # Split the data between features (x) and labels(y)\n",
    "            self.train_x_df, self.train_y_df = self.get_data_xy_split(train_df, cao_mapping)\n",
    "            self.val_x_df, self.val_y_df = self.get_data_xy_split(val_df, cao_mapping)\n",
    "            self.test_x_df, self.test_y_df = self.get_data_xy_split(test_df, cao_mapping)\n",
    "\n",
    "            # Keep track of how many values are used to encode each outcome\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                first_value = self.train_y_df[column].head(1).values[0]\n",
    "                if isinstance(first_value, numbers.Number):\n",
    "                    # Value is a single scalar\n",
    "                    self.column_length[column] = 1\n",
    "                else:\n",
    "                    # value is a one-hot encoded vector, i.e. a list. Get its size.\n",
    "                    self.column_length[column] = len(self.train_y_df[column].head(1).values[0])\n",
    "        else:\n",
    "            # No data provided, assuming outcomes are numerical (not categorical)\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                self.column_length[column] = 1\n",
    "\n",
    "        if model_params is None:\n",
    "            model_params = {}\n",
    "        self.model_params = model_params\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Internal Parameters that are used to store the\n",
    "        # latest state of the model.\n",
    "        self._trained_model = None\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_type(self) -> PredictorType:\n",
    "        \"\"\"\n",
    "        :return the PredictorType of this Predictor: Regressor or Classifier\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def library(self) -> str:\n",
    "        \"\"\"\n",
    "        :return the underlying library that implements this predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_name(self) -> str:\n",
    "        \"\"\"\n",
    "        :return: the name of the Predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self, model_params: Dict):\n",
    "        \"\"\"\n",
    "        This function must be overridden to build the model using the model\n",
    "        parameters if desired and return a model.\n",
    "        :param model_params: Dictionary containing the model parameters\n",
    "        :return model: The built model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model(self, model,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray]) -> Type:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model\n",
    "        \"\"\"\n",
    "\n",
    "    def set_trained_model(self, trained_model) -> None:\n",
    "        \"\"\"\n",
    "        Sets the underlying trained model to the passed one.\n",
    "        :param trained_model: a trained model\n",
    "        :return Nothing:\n",
    "        \"\"\"\n",
    "        self._trained_model = trained_model\n",
    "\n",
    "    def get_trained_model(self):\n",
    "        \"\"\"\n",
    "        Returns the trained model if it has been set, None otherwise\n",
    "        :return self._trained_model:\n",
    "        \"\"\"\n",
    "        return self._trained_model\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_split(data_df: pd.DataFrame,\n",
    "                            data_split: Dict[str, Any]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the data between train, validation (optional) and test sets\n",
    "        :param data_df: the full dataset as a Pandas DataFrame\n",
    "        :param data_split: a dictionary with the\n",
    "        :return: a tuple of Pandas DataFrame: one for train, one for validation (or None), and one for test\n",
    "        \"\"\"\n",
    "\n",
    "        # First, split the data set in train and test sets.\n",
    "        # Use the provided random_state, if any\n",
    "        random_state = data_split.get(\"random_state\", None)\n",
    "        shuffle = data_split.get(\"shuffle\", True)\n",
    "        train_df, test_df = train_test_split(data_df,\n",
    "                                             test_size=data_split[\"test_pct\"],\n",
    "                                             random_state=random_state,\n",
    "                                             shuffle=shuffle)\n",
    "\n",
    "        # If we also need a validation set, split the train set into train and validation sets.\n",
    "        val_pct = data_split.get(\"val_pct\", 0)\n",
    "        if val_pct > 0:\n",
    "            train_df, val_df = train_test_split(train_df,\n",
    "                                                test_size=val_pct,\n",
    "                                                random_state=random_state,\n",
    "                                                shuffle=shuffle)\n",
    "        else:\n",
    "            val_df = None\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def predict(self, encoded_context_actions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method uses the trained model to make a prediction for the passed Pandas DataFrame\n",
    "        of context and actions. Returns the predicted outcomes in a Pandas DataFrame.\n",
    "        :param encoded_context_actions_df: a Pandas DataFrame containing encoded rows of context and actions for\n",
    "        which a prediction is requested. Categorical columns contain one-hot vectors, e.g. [1, 0, 0]. Which means\n",
    "        a row can be a list of arrays (1 per column), e.g.: [1, 0, 0], [1,0].\n",
    "        :return a Pandas DataFrame of the predicted outcomes for each context and actions row.\n",
    "        \"\"\"\n",
    "        # Default implementation\n",
    "        if self._trained_model:\n",
    "            # Predict using the model's input columns, in case encoded_context_actions_df contains more columns\n",
    "            # or is in a different order\n",
    "            context_action_df = encoded_context_actions_df[self.context_actions_columns]\n",
    "            # Convert one-hot vector columns into a single feature vector\n",
    "            features = DataEncoder.encoded_df_to_np(context_action_df)\n",
    "            # Check if model type is onnx runtime or not\n",
    "            if isinstance(self._trained_model, InferenceSession):\n",
    "                predictions = self._trained_model.run(None, {\"X\": features.astype(np.float32)})[0]\n",
    "            else:\n",
    "                predictions = self._trained_model.predict(features)\n",
    "            if isinstance(predictions, pd.DataFrame):\n",
    "                # Predictions are already in a DataFrame. Make sure they have the correct outcome names\n",
    "                predictions_df = predictions\n",
    "                predictions_df.columns = self.cao_mapping[\"outcomes\"]\n",
    "                # Convert predictions to float64 as it's JSON serializable, while float32 is not\n",
    "                predictions_df = predictions_df.astype(\"float64\")\n",
    "            else:\n",
    "                # Assuming predictions is a ndarray, convert it to a DataFrame with the output column names\n",
    "                predictions_df = DataEncoder.np_to_encoded_df(predictions,\n",
    "                                                              self.column_length)\n",
    "        else:\n",
    "            raise ValueError(\"Can't make predictions because the model has not been trained\")\n",
    "        return predictions_df\n",
    "\n",
    "    @staticmethod\n",
    "    def export_metrics(metrics_dict: Dict[str, Any], file_path: str):\n",
    "        \"\"\"\n",
    "        Save the model's training metrics to the specified location\n",
    "        :param metrics_dict: a dictionary containing metrics\n",
    "        :param file_path: the name and path of the file to persist the bytes to\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as my_file:\n",
    "            json.dump(metrics_dict, my_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_xy_split(data_df: Optional[pd.DataFrame],\n",
    "                          cao_mapping: Dict) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        This function takes a dataframe and a dictionary mapping indices to context,\n",
    "        action, or outcome. This then splits the dataframe into two dataframes based\n",
    "        on it's CAO tagging.\n",
    "\n",
    "        data_x: Context and Actions\n",
    "        data_y: Outcomes\n",
    "\n",
    "        :param data_df: a Pandas DataFrame with all the data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes` keys where each key returns a List\n",
    "         ofthe selected column names as strings.\n",
    "        :return: A tuple containing two dataframes: data_x with the features, and data_y with the labels (outcomes)\n",
    "        \"\"\"\n",
    "        if data_df is None:\n",
    "            return None, None\n",
    "\n",
    "        data_x_df = data_df[cao_mapping[\"context\"] + cao_mapping[\"actions\"]]\n",
    "        data_y_df = data_df[cao_mapping[\"outcomes\"]]\n",
    "\n",
    "        return data_x_df, data_y_df\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.predictor_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Regressor Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(REGRESSOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Classifier Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(CLASSIFIER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad072a9c",
   "metadata": {},
   "source": [
    "## Predictor 704abc67-fe1c-409b-87c1-8e59864b7fe4\n",
    "### CAO columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLUMNS = ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score']\n",
    "ACTION_COLUMNS = ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14']\n",
    "OUTCOME_COLUMNS = ['derived_metric']\n",
    "CONTEXT_ACTION_COLUMNS = CONTEXT_COLUMNS + ACTION_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450b53f",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10218d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {\"train_pct\": 0.8, \"test_pct\": 0.2, \"random_state\": 42}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0177020",
   "metadata": {},
   "source": [
    "### Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Predictor):\n",
    "    \"\"\"\n",
    "    This class implements a linear regression model from the SKLearn library.\n",
    "    \"\"\"\n",
    "    predictor_type = Regressor()\n",
    "    library = \"sklearn\"\n",
    "    predictor_name = name = f\"{library} Linear Regression\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict = None,\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        The constructor initializes the base params.\n",
    "        \"\"\"\n",
    "        super().__init__(data_df=data_df,\n",
    "                         cao_mapping=cao_mapping,\n",
    "                         data_split=data_split,\n",
    "                         model_params=model_params,\n",
    "                         metadata=metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "        multioutput = True\n",
    "        return multioutput\n",
    "\n",
    "    def build_model(self, model_params: Dict[str, Any]) -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function instantiates a Linear Regression model with the given params.\n",
    "        :return model: a LinearRegression instance\n",
    "        \"\"\"\n",
    "        model = linear_model.LinearRegression(**model_params)\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model: linear_model.LinearRegression,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray])\\\n",
    "            -> linear_model.LinearRegression:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model and the desired metrics as a dictionary.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model: The linear regression model trained\n",
    "        \"\"\"\n",
    "        trained_model = model.fit(train_x, train_y)\n",
    "        return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d160c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Base: your PyTorch 14-output model ---------------------------------------\n",
    "class _TorchDomainPredictor(torch.nn.Module):\n",
    "    def __init__(self, n_domains=14):\n",
    "        super().__init__()\n",
    "        self.n_domains = n_domains\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_domains * 3, 100),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(100, n_domains),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class _TorchInferenceWrapper:\n",
    "    def __init__(self, torch_model, device=\"cpu\"):\n",
    "        import torch\n",
    "        self.m = torch_model.to(device).eval()\n",
    "        self.device = device\n",
    "        self.torch = torch\n",
    "    def predict(self, X_np):\n",
    "        with self.torch.no_grad():\n",
    "            X = self.torch.from_numpy(X_np).float().to(self.device)\n",
    "            Y = self.m(X).detach().cpu().numpy()  # shape (n, 14)\n",
    "        return Y\n",
    "\n",
    "class TorchRegressorPredictor(Predictor):\n",
    "    predictor_type = Regressor()\n",
    "    library = \"pytorch\"\n",
    "    predictor_name = name = \"PyTorch 14-output Regressor\"\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        return True  # 14 outputs\n",
    "\n",
    "    def build_model(self, model_params):\n",
    "        import torch\n",
    "        n_domains = int(model_params.get(\"n_domains\", 14))\n",
    "        device = str(model_params.get(\"device\", \"cpu\"))\n",
    "        model = _TorchDomainPredictor(n_domains=n_domains)\n",
    "        return {\"model\": model, \"device\": device}\n",
    "\n",
    "    def train_model(self, model_bundle, train_x, train_y, val_x, val_y):\n",
    "        import torch, os\n",
    "        model_path = self.model_params.get(\"model_path\")\n",
    "        assert model_path and os.path.exists(model_path), f\"model_path not found: {model_path}\"\n",
    "        model = model_bundle[\"model\"]\n",
    "        device = model_bundle[\"device\"]\n",
    "        state = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "        model.load_state_dict(state)\n",
    "        return _TorchInferenceWrapper(model, device=device)\n",
    "\n",
    "# ---- Wrapper: compute a single derived metric from the 14 predictions ----------\n",
    "class DerivedMetricPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Calls a 14-output base predictor, then maps the 14 ŷ to a 1D derived_metric.\n",
    "    \"\"\"\n",
    "    predictor_type = Regressor()\n",
    "    library = \"meta\"\n",
    "    predictor_name = name = \"Derived Metric from 14-output model\"\n",
    "\n",
    "    def __init__(self, *args, base_predictor=None, metric_name=\"derived_metric\", metric_params=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert base_predictor is not None, \"Pass base_predictor=...\"\n",
    "        self.base_predictor = base_predictor\n",
    "        self.metric_name = metric_name\n",
    "        self.metric_params = metric_params or {}\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        return False  # returns a single column\n",
    "\n",
    "    def build_model(self, model_params):\n",
    "        return None  # wrapper does not build a separate model\n",
    "\n",
    "    def train_model(self, model, train_x, train_y, val_x, val_y):\n",
    "        return self  # wrapper is already ready\n",
    "\n",
    "    def predict(self, features_df_or_np):\n",
    "        # 0) Ensure we have a DataFrame with the expected feature column order\n",
    "        if isinstance(features_df_or_np, pd.DataFrame):\n",
    "            X_df = features_df_or_np\n",
    "        else:\n",
    "            # Base class usually has self.context_actions_columns in order (context + actions)\n",
    "            # If your base class exposes a different attribute, use that here.\n",
    "            X_df = pd.DataFrame(features_df_or_np, columns=self.context_actions_columns)\n",
    "\n",
    "        # 1) Get the 14 predictions from the base model as a DataFrame\n",
    "        base_df = self.base_predictor.predict(X_df)     # shape (n, 14)\n",
    "        yhat = base_df.to_numpy()                       # (n, 14)\n",
    "\n",
    "        # 2) Extract actions and context as NumPy for fast indexing\n",
    "        actions_cols = self.cao_mapping[\"actions\"]      # length 14, ordered as your one-hot\n",
    "        context_cols = self.cao_mapping[\"context\"]      # expected like [d_1, d_1_ind, d_2, d_2_ind, ...]\n",
    "        A = X_df[actions_cols].to_numpy(copy=False)     # (n, 14), values in {0,1}\n",
    "\n",
    "        C = X_df[context_cols].to_numpy(copy=True)      # (n, 28) if alternating score/indicator\n",
    "        # If your “context” really is pairs [score, indicator] per domain, take the score columns:\n",
    "        # indices 0,2,4,...,26 → 14 columns\n",
    "        score_idxs = np.arange(0, C.shape[1], 2)\n",
    "        C_scores = C[:, score_idxs]                     # (n, 14) current scores by domain\n",
    "\n",
    "        # 3) Replace current score with predicted value when the action for that domain == 1\n",
    "        # Loop version (clear & fine for 14 columns)\n",
    "        # for i in range(14):\n",
    "        #     mask = (A[:, i] == 1)\n",
    "        #     if mask.any():\n",
    "        #         C_scores[mask, i] = yhat[mask, i]\n",
    "\n",
    "        # Vectorized alternative (optional):\n",
    "        # masks = (A == 1)\n",
    "        # C_scores[masks] = yhat[masks]\n",
    "\n",
    "        # 4) Replace d_i_score with yhat[:, i] where target_i == 1\n",
    "        mask = (A == 1)                          # (n, 14) boolean/int\n",
    "        replaced_scores = C_scores.copy()\n",
    "        replaced_scores[mask] = yhat[mask]\n",
    "\n",
    "        # 5) Average across all domains\n",
    "        derived = replaced_scores.mean(axis=1)\n",
    "\n",
    "        # 6) Return as a 1-col DataFrame with the expected outcome name\n",
    "        return pd.DataFrame(derived, columns=[self.metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc98746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor_node_id = '704abc67-fe1c-409b-87c1-8e59864b7fe4'\n",
    "# predictor = LinearRegression(encoded_data_source_df,\n",
    "#     cao_mapping={'context': ['d_1_ind', 'd_2_ind', 'd_3_ind', 'd_4_ind', 'd_5_ind', 'd_6_ind', 'd_7_ind', 'd_8_ind', 'd_9_ind', 'd_10_ind', 'd_11_ind', 'd_12_ind', 'd_13_ind', 'd_14_ind', 'd_1_score', 'd_2_score', 'd_3_score', 'd_4_score', 'd_5_score', 'd_6_score', 'd_7_score', 'd_8_score', 'd_9_score', 'd_10_score', 'd_11_score', 'd_12_score', 'd_13_score', 'd_14_score'], 'actions': ['target_d_1', 'target_d_2', 'target_d_3', 'target_d_4', 'target_d_5', 'target_d_6', 'target_d_7', 'target_d_8', 'target_d_9', 'target_d_10', 'target_d_11', 'target_d_12', 'target_d_13', 'target_d_14'], 'outcomes': ['d_3_next']},\n",
    "# data_split=data_split,\n",
    "# model_params={},\n",
    "# metadata={})\n",
    "# --- Instantiate base Torch predictor (14 outputs) + wrapper (1 derived output) ---\n",
    "base_id    = \"torch-base-14\"\n",
    "derived_id = \"torch-derived-metric\"\n",
    "\n",
    "# outcomes for the BASE predictor: ensure order matches your model's 14 outputs\n",
    "base_outcomes = [f'd_{i}_next' for i in range(1, 15)]\n",
    "\n",
    "base_predictor = TorchRegressorPredictor(\n",
    "    encoded_data_source_df,\n",
    "    cao_mapping={**cao_mapping, \"outcomes\": base_outcomes},  # provide 14 names for base\n",
    "    data_split=data_split,\n",
    "    model_params={\n",
    "        \"model_path\": \"model.pt\",   # <-- set this to your REAL weights path\n",
    "        \"device\": \"cpu\",\n",
    "        \"n_domains\": 14\n",
    "    },\n",
    "    metadata={\"role\": \"base_14_output\"}\n",
    ")\n",
    "\n",
    "predictor = DerivedMetricPredictor(\n",
    "    encoded_data_source_df,\n",
    "    cao_mapping=cao_mapping,  # ['derived_metric']\n",
    "    data_split=data_split,\n",
    "    model_params={},\n",
    "    metadata={\"role\": \"derived_objective\"},\n",
    "    base_predictor=base_predictor\n",
    ")\n",
    "\n",
    "predictor_node_id = derived_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84165bf9",
   "metadata": {},
   "source": [
    "### Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Torch weights for base; initialize wrapper; register wrapper for ESP\n",
    "train = True\n",
    "\n",
    "# === Prepare features/targets just like Cell 26 ===\n",
    "train_x = DataEncoder.encoded_df_to_np(predictor.train_x_df)\n",
    "train_y = DataEncoder.encoded_df_to_np(predictor.train_y_df)\n",
    "if predictor.val_x_df is not None:\n",
    "    val_x = DataEncoder.encoded_df_to_np(predictor.val_x_df)\n",
    "    val_y = DataEncoder.encoded_df_to_np(predictor.val_y_df)\n",
    "else:\n",
    "    val_x = val_y = None\n",
    "\n",
    "# 1️⃣ Base: build and “train” (really just loads the state_dict)\n",
    "_ = base_predictor.train_model(\n",
    "        base_predictor.build_model(base_predictor.model_params),\n",
    "        train_x, None, val_x, None)\n",
    "base_predictor.set_trained_model(_)\n",
    "\n",
    "# 2️⃣ Wrapper: build & finalize (no real training)\n",
    "_ = predictor.train_model(\n",
    "        predictor.build_model(predictor.model_params),\n",
    "        train_x, train_y, val_x, val_y)\n",
    "predictor.set_trained_model(_)\n",
    "\n",
    "# 3️⃣ Register only the derived predictor for ESP\n",
    "predictors_by_id[predictor_node_id] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660239",
   "metadata": {},
   "source": [
    "### Predictor metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_by_id[predictor_node_id] = [MetricsManager.get_calculator('Mean Squared Error')]\n",
    "metrics = MetricsManager.compute_metrics(predictor,\n",
    "model_metrics_by_id[predictor_node_id],predictor.train_x_df, predictor.train_y_df,predictor.val_x_df, predictor.val_y_df,predictor.test_x_df, predictor.test_y_df,encoder)\n",
    "\n",
    "print(f'Predictor trained. Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a19b68",
   "metadata": {},
   "source": [
    "## Prescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1dc11",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnileafPrescriptor(EspEvaluator):\n",
    "    \"\"\"\n",
    "    An Unileaf Prescriptor makes prescriptions given an ESP candidate and a context DataFrame.\n",
    "    It is also an EspEvaluator implementation that returns metrics for ESP candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: Dict[str, Any],\n",
    "                 evaluation_df: pd.DataFrame,\n",
    "                 data_encoder: DataEncoder,\n",
    "                 predictors: List[Predictor]):\n",
    "        \"\"\"\n",
    "        Constructs a prescriptor evaluator\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :param evaluation_df: the Pandas DataFrame to use to evaluate the candidates\n",
    "        :param data_encoder: the DataEncoder used to encode the dataset\n",
    "        :param predictors: the predictors this prescriptor relies on\n",
    "        \"\"\"\n",
    "        # Instantiate EspEvaluator\n",
    "        # Note: sets self.config\n",
    "        super().__init__(config)\n",
    "\n",
    "        # CAO\n",
    "        self.cao_mapping = {\"context\": self.get_context_field_names(config),\n",
    "                            \"actions\": self.get_action_field_names(config),\n",
    "                            \"outcomes\": self.get_fitness_metrics(config)}\n",
    "        self.context_df = evaluation_df[self.cao_mapping[\"context\"]]\n",
    "        self.row_index = self.context_df.index\n",
    "\n",
    "        # Convert the context DataFrame to a format a NN can ingest\n",
    "        self.context_as_nn_input = self.convert_to_nn_input(self.context_df)\n",
    "\n",
    "        # Data encoder\n",
    "        self.data_encoder = data_encoder\n",
    "\n",
    "        # Predictors\n",
    "        self.predictors = predictors\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_nn_input(context_df: pd.DataFrame) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Converts a context DataFrame to a list of numpy arrays a neural network can ingest\n",
    "        :param context_df: a DataFrame containing inputs for a neural network. Number of inputs and size must match\n",
    "        :return: a list of numpy ndarray, on ndarray per neural network input\n",
    "        \"\"\"\n",
    "        # The NN expects a list of i inputs by s samples (e.g. 9 x 299).\n",
    "        # So convert the data frame to a numpy array (gives shape 299 x 9), transpose it (gives 9 x 299)\n",
    "        # and convert to list(list of 9 arrays of 299)\n",
    "        context_as_nn_input = list(context_df.to_numpy().transpose())\n",
    "        # Convert each column's list of 1D array to a 2D array\n",
    "        context_as_nn_input = [np.stack(context_as_nn_input[i], axis=0) for i in\n",
    "                               range(len(context_as_nn_input))]\n",
    "        return context_as_nn_input\n",
    "\n",
    "    def evaluate_candidate(self, candidate):\n",
    "        \"\"\"\n",
    "        Evaluates a single Prescriptor candidate and returns its metrics.\n",
    "        Implements the EspEvaluator interface\n",
    "        :param candidate: a Keras neural network or rule based Prescriptor candidate\n",
    "        :return metrics: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Prescribe actions\n",
    "        prescribed_actions_df = self.prescribe(candidate)\n",
    "\n",
    "        # Aggregate the context and actions dataframes.\n",
    "        context_actions_df = pd.concat([self.context_df,\n",
    "                                        prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "\n",
    "        # Compute the metrics\n",
    "        metrics = self._compute_metrics(context_actions_df)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_metrics(self, context_actions_df):\n",
    "        \"\"\"\n",
    "        Computes metrics from the passed context/actions DataFrame using the instance's trained predictors.\n",
    "        :param context_actions_df: a DataFrame of context / prescribed actions\n",
    "        :return: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Get the predicted outcomes from the predictors\n",
    "        metrics = {}\n",
    "        for predictor in self.predictors:\n",
    "            predicted_outcomes = predictor.predict(context_actions_df)\n",
    "\n",
    "            # UN-853: Decode predictions before computing numerical metrics, if a data_encoder is available\n",
    "            if self.data_encoder is not None:\n",
    "                decoded_predicted_outcomes = self.data_encoder.decode_as_df(predicted_outcomes)\n",
    "            else:\n",
    "                decoded_predicted_outcomes = predicted_outcomes\n",
    "\n",
    "            # Only add a metric for the outcomes the prescriptor is interested in\n",
    "            for outcome in self.cao_mapping[\"outcomes\"]:\n",
    "                # Add the metrics that have been produced by this predictor\n",
    "                if outcome in predictor.cao_mapping[\"outcomes\"]:\n",
    "                    # Check the type of metric: numerical or categorical?\n",
    "                    if decoded_predicted_outcomes[[outcome]].iloc[:, 0].dtype == object:\n",
    "                        # Categorical outcome. Use the *encoded* predicted outcome.\n",
    "                        preds = predicted_outcomes[outcome]\n",
    "                        # Classifiers return the category's index in the list of categories, so we can take the mean\n",
    "                        # of the encoded outcomes. Note: this works because Outcomes are encoded using LabelEncoder\n",
    "                        # AND the user defined order for each Outcome categories.\n",
    "                        metrics[outcome] = preds.mean()\n",
    "                    else:\n",
    "                        # UN-853: Numerical outcome. Use the *decoded*, i.e. scaled back, predicted outcome\n",
    "                        preds = decoded_predicted_outcomes[outcome]\n",
    "                        # Regressors produce floats: take the mean of the decoded outcome\n",
    "                        metrics[outcome] = preds.mean()\n",
    "        return metrics\n",
    "\n",
    "    def prescribe(self, candidate, context_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed candidate and context\n",
    "        :param candidate: an ESP candidate, either neural network or rules\n",
    "        :param context_df: a DataFrame containing the context to prescribe for,\n",
    "         or None to use the instance one\n",
    "        :return: a DataFrame containing actions prescribed for each context\n",
    "        \"\"\"\n",
    "        if context_df is None:\n",
    "            # No context is provided, use the instance's one\n",
    "            context_as_nn_input = self.context_as_nn_input\n",
    "            row_index = self.row_index\n",
    "        else:\n",
    "            # Convert the context DataFrame to something more suitable for neural networks\n",
    "            context_as_nn_input = self.convert_to_nn_input(context_df)\n",
    "            # Use the context's row index\n",
    "            row_index = context_df.index\n",
    "\n",
    "        is_rule_based = isinstance(candidate, RuleSet)\n",
    "        if is_rule_based:\n",
    "            actions = self._prescribe_from_rules(candidate, context_as_nn_input)\n",
    "        else:\n",
    "            actions = self._prescribe_from_nn(candidate, context_as_nn_input)\n",
    "\n",
    "        # Convert the prescribed actions to a DataFrame\n",
    "        prescribed_actions_df = pd.DataFrame(actions,\n",
    "                                             columns=self.cao_mapping[\"actions\"],\n",
    "                                             index=row_index)\n",
    "        \n",
    "        # --- BEGIN: normalize array-valued action columns to scalars ---\n",
    "        actions = self.cao_mapping[\"actions\"]  # e.g., ['target_d_1', ..., 'target_d_14']\n",
    "\n",
    "        for i, col in enumerate(actions):\n",
    "            def to_scalar(v):\n",
    "                # Scalar or None -> coerce to {0,1}\n",
    "                if isinstance(v, (int, float, np.integer, np.floating)) or v is None:\n",
    "                    return int(bool(v)) if v is not None and v == v else 0\n",
    "\n",
    "                # Array/list -> pick i-th element (0-indexed: i=0 -> target_d_1)\n",
    "                arr = np.asarray(v).ravel()\n",
    "                if arr.size > i:\n",
    "                    return int(arr[i] >= 0.5)\n",
    "\n",
    "                # Fallback if array is shorter than expected:\n",
    "                # mark \"on\" if any element >= 0.5 (conservative)\n",
    "                return int(np.any(arr >= 0.5))\n",
    "\n",
    "            prescribed_actions_df[col] = prescribed_actions_df[col].map(to_scalar).astype(int)\n",
    "\n",
    "# Optional sanity: ensure all actions are 0/1 scalars now\n",
    "# assert set(np.unique(prescribed_actions_df[actions].to_numpy())).issubset({0, 1})\n",
    "# --- END: normalize array-valued action columns ---\n",
    "\n",
    "        # Optional debug: confirm no arrays remain\n",
    "        # print(\"array-valued left:\",\n",
    "        #       prescribed_actions_df[actions].apply(lambda s: s.apply(lambda v: isinstance(v,(list,tuple,np.ndarray)))).any())\n",
    "\n",
    "        # --- END: normalize array-valued action columns ---\n",
    "\n",
    "\n",
    "        # ### DEBUG:\n",
    "        # print(\"-------------DEBUG: Prescribed Actions (encoded):\")\n",
    "        # print(\"shape: \", prescribed_actions_df.shape)\n",
    "        # print(\"dtype: \", prescribed_actions_df.dtypes)\n",
    "        # has_arrays = prescribed_actions_df.applymap(lambda x: isinstance(x, (list, np.ndarray))).any() # type: ignore\n",
    "        # print(\"array-valued columns:\\n\", has_arrays[has_arrays].index.tolist())\n",
    "\n",
    "        # UN-2430 Decode the sigmoides, if any, back into categories\n",
    "        prescribed_actions_df = self.data_encoder.decode_as_df(prescribed_actions_df)\n",
    "        # UN0-240 Re-encode the actions into what the predictors expect (e.g. one-hots for categorical data)\n",
    "        prescribed_actions_df = self.data_encoder.encode_as_df(prescribed_actions_df)\n",
    "        return prescribed_actions_df\n",
    "\n",
    "    def _prescribe_from_rules(self, candidate, context_as_nn_input: List[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed rules model candidate and context\n",
    "        :param candidate: a rules model candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to list of action values\n",
    "        \"\"\"\n",
    "        cand_states = RuleSetConfigHelper.get_states(self.config)\n",
    "        cand_actions = RuleSetConfigHelper.get_actions(self.config)\n",
    "        candidate = RuleSetBinding(candidate, cand_states, cand_actions)\n",
    "        rules_encoder = RulesDataEncoder(candidate.actions)\n",
    "        evaluator = RuleSetBindingEvaluator()\n",
    "        rules_input = rules_encoder.encode_to_rules_data(context_as_nn_input)\n",
    "        rules_output = evaluator.evaluate(candidate, rules_input)\n",
    "        actions = rules_encoder.decode_from_rules_data(rules_output)\n",
    "        return actions\n",
    "\n",
    "    def _prescribe_from_nn(self, candidate, context_as_nn_input: List[np.ndarray]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed neural network candidate and context\n",
    "        :param candidate: a Keras neural network candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to action value or list of action values\n",
    "        \"\"\"\n",
    "        # Get the prescribed actions\n",
    "        prescribed_actions = candidate.predict(context_as_nn_input)\n",
    "        actions = {}\n",
    "\n",
    "        if self._is_single_action_prescriptor():\n",
    "            # Put the single action in an array to process it like multiple actions\n",
    "            prescribed_actions = [prescribed_actions]\n",
    "\n",
    "        for index, action_col in enumerate(self.cao_mapping[\"actions\"]):\n",
    "            if self._is_scalar(prescribed_actions[index]):\n",
    "                # We have a single row and this action is numerical. Convert it to a scalar.\n",
    "                actions[action_col] = prescribed_actions[index].item()\n",
    "            else:\n",
    "                actions[action_col] = prescribed_actions[index].tolist()\n",
    "        return actions\n",
    "\n",
    "    def _is_single_action_prescriptor(self):\n",
    "        \"\"\"\n",
    "        Checks how many Actions have been defined in the Context, Actions, Outcomes mapping.\n",
    "        :return: True if only 1 action is defined, False otherwise\n",
    "        \"\"\"\n",
    "        return len(self.cao_mapping[\"actions\"]) == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_scalar(prescribed_action):\n",
    "        \"\"\"\n",
    "        Checks if the prescribed action contains a single value, i.e. a scalar, or an array.\n",
    "        A prescribed action contains a single value if it has been prescribed for a single context sample\n",
    "        :param prescribed_action: a scalar or an array\n",
    "        :return: True if the prescribed action contains a scalar, False otherwise.\n",
    "        \"\"\"\n",
    "        return prescribed_action.shape[0] == 1 and prescribed_action.shape[1] == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_context_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Context column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Context column names\n",
    "        \"\"\"\n",
    "        nn_inputs = config[\"network\"][\"inputs\"]\n",
    "        contexts = [nn_input[\"name\"] for nn_input in nn_inputs]\n",
    "        return contexts\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Action column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Action column names\n",
    "        \"\"\"\n",
    "        nn_outputs = config[\"network\"][\"outputs\"]\n",
    "        actions = [nn_output[\"name\"] for nn_output in nn_outputs]\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fitness_metrics(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of fitness metric names (Outcomes) to optimize.\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of fitness metric names\n",
    "        \"\"\"\n",
    "        metrics = config[\"evolution\"][\"fitness\"]\n",
    "        fitness_metrics = [metric[\"metric_name\"] for metric in metrics]\n",
    "        return fitness_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e500f",
   "metadata": {},
   "source": [
    "### Prescriptor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'evolution': {'fitness': [{'maximize': True, 'metric_name': 'derived_metric'}], 'nb_elites': 1, 'mutation_type': 'gaussian_noise_percentage', 'nb_generations': 2, 'mutation_factor': 0.1, 'population_size': 3, 'parent_selection': 'tournament', 'initialization_range': 1, 'mutation_probability': 0.1, 'remove_population_pct': 0.8, 'initialization_distribution': 'orthogonal'}, 'network': {'inputs': [{'name': 'd_1_ind', 'size': 1, 'values': ['float']}, {'name': 'd_2_ind', 'size': 1, 'values': ['float']}, {'name': 'd_3_ind', 'size': 1, 'values': ['float']}, {'name': 'd_4_ind', 'size': 1, 'values': ['float']}, {'name': 'd_5_ind', 'size': 1, 'values': ['float']}, {'name': 'd_6_ind', 'size': 1, 'values': ['float']}, {'name': 'd_7_ind', 'size': 1, 'values': ['float']}, {'name': 'd_8_ind', 'size': 1, 'values': ['float']}, {'name': 'd_9_ind', 'size': 1, 'values': ['float']}, {'name': 'd_10_ind', 'size': 1, 'values': ['float']}, {'name': 'd_11_ind', 'size': 1, 'values': ['float']}, {'name': 'd_12_ind', 'size': 1, 'values': ['float']}, {'name': 'd_13_ind', 'size': 1, 'values': ['float']}, {'name': 'd_14_ind', 'size': 1, 'values': ['float']}, {'name': 'd_1_score', 'size': 1, 'values': ['float']}, {'name': 'd_2_score', 'size': 1, 'values': ['float']}, {'name': 'd_3_score', 'size': 1, 'values': ['float']}, {'name': 'd_4_score', 'size': 1, 'values': ['float']}, {'name': 'd_5_score', 'size': 1, 'values': ['float']}, {'name': 'd_6_score', 'size': 1, 'values': ['float']}, {'name': 'd_7_score', 'size': 1, 'values': ['float']}, {'name': 'd_8_score', 'size': 1, 'values': ['float']}, {'name': 'd_9_score', 'size': 1, 'values': ['float']}, {'name': 'd_10_score', 'size': 1, 'values': ['float']}, {'name': 'd_11_score', 'size': 1, 'values': ['float']}, {'name': 'd_12_score', 'size': 1, 'values': ['float']}, {'name': 'd_13_score', 'size': 1, 'values': ['float']}, {'name': 'd_14_score', 'size': 1, 'values': ['float']}], 'outputs': [{'name': 'target_d_1', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_2', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_3', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_4', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_5', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_6', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_7', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_8', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_9', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_10', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_11', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_12', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_13', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}, {'name': 'target_d_14', 'size': 2, 'activation': 'sigmoid', 'use_bias': True, 'values': ['0.0', '1.0']}], 'hidden_layers': [{'layer_name': 'hidden_1', 'layer_type': 'Dense', 'layer_params': {'units': 16, 'use_bias': True, 'activation': 'tanh'}}]}, 'LEAF': {'representation': 'NNWeights', 'experiment_id': 'UniLEAF_de95fcd7-7056-43d3-9e30-9d309e76f726', 'version': '1.0.0', 'persistence_dir': 'trained_prescriptors/', 'candidates_to_persist': 'best'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_predictor_ids = ['704abc67-fe1c-409b-87c1-8e59864b7fe4']\n",
    "# all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]\n",
    "required_predictor_ids = ['torch-derived-metric']\n",
    "all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3be7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the EspService\n",
    "esp_service = EspService(config)\n",
    "esp_evaluator = UnileafPrescriptor(config,\n",
    "                                   encoded_data_source_df,\n",
    "                                   encoder,\n",
    "                                   all_predictors)\n",
    "experiment_results_dir = esp_service.train(esp_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a3f30",
   "metadata": {},
   "source": [
    "## ESP Summary Stats\n",
    "esp_service.train(...) returned the directory in which the experiment results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = os.path.join(experiment_results_dir, 'experiment_stats.csv')\n",
    "with open(stats_file) as csv_file:\n",
    "    stats_df = pd.read_csv(csv_file, sep=',')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637bf3",
   "metadata": {},
   "source": [
    "## ESP Summary Plot\n",
    "esp_service.train(...) generated a plot summarizing the experiment's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1753f8a",
   "metadata": {},
   "source": [
    "### derived_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = os.path.join(experiment_results_dir, 'derived_metric_plot.png')\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71375ad",
   "metadata": {},
   "source": [
    "# Models usage\n",
    "## Load the Prescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last row of the stats DataFrame, i.e. the last generation, to find the best model\n",
    "last_gen = stats_df['generation'].iloc[-1]\n",
    "best_score = stats_df['max_derived_metric'].iloc[-1]\n",
    "cid_best_score = stats_df['cid_max_derived_metric'].iloc[-1]\n",
    "prescriptor_model_filename = os.path.join(experiment_results_dir,\n",
    "                                          str(last_gen),\n",
    "                                          cid_best_score + '.h5')\n",
    "print(f'Best max_derived_metric average is {best_score:.3f} for candidate id {cid_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(f'Loading prescriptor model: {prescriptor_model_filename}')\n",
    "prescriptor_model = load_model(prescriptor_model_filename, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77623b3",
   "metadata": {},
   "source": [
    "## Get a sample context\n",
    "Get the context from one of the rows in the dataset, and make a prescription for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82356a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_source_df.sample(1)\n",
    "sample_context_df = sample_df[CONTEXT_COLUMNS]\n",
    "sample_context_action_df = sample_df[CONTEXT_ACTION_COLUMNS]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6202e",
   "metadata": {},
   "source": [
    "### Prescribe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c30780",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_context_df = encoder.encode_as_df(sample_context_df)\n",
    "encoded_prescribed_actions_df = esp_evaluator.prescribe(prescriptor_model, encoded_sample_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effa5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the context and actions dataframes.\n",
    "encoded_context_actions_df = pd.concat([encoded_sample_context_df,\n",
    "                                        encoded_prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "sample_context_prescribed_action_df = encoder.decode_as_df(encoded_context_actions_df)\n",
    "sample_context_prescribed_action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f37e8",
   "metadata": {},
   "source": [
    "### Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(predictors, context_action_df, encoder):\n",
    "    pred_array = []\n",
    "    for predictor in predictors:\n",
    "        pred = predictor.predict(encoder.encode_as_df(context_action_df))\n",
    "        pred_array.append(encoder.decode_as_df(pred))\n",
    "    preds_df = pd.concat(pred_array, axis=1)\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccba3d",
   "metadata": {},
   "source": [
    "#### With original actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_actions_preds = get_predictions(all_predictors, sample_context_action_df, encoder)\n",
    "original_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416793",
   "metadata": {},
   "source": [
    "#### With prescribed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescribed_actions_preds = get_predictions(all_predictors, sample_context_prescribed_action_df, encoder)\n",
    "prescribed_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957136",
   "metadata": {},
   "source": [
    "#### With custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_custom_action_df = sample_context_prescribed_action_df.copy()\n",
    " # TODO: Uncomment and replace by the name of the actions(s) to customize\n",
    "# sample_context_custom_action_df['SOME_ACTION_TO_CUSTOMIZE'] = 42\n",
    "sample_context_custom_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_actions_preds = get_predictions(all_predictors, sample_context_custom_action_df, encoder)\n",
    "custom_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70319159",
   "metadata": {},
   "source": [
    "### Compare\n",
    "Compare 3 rows in a single table:\n",
    "- the original sample\n",
    "- the sample with the prescribed actions\n",
    "- the sample with some custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME_COLUMNS = list(original_actions_preds.columns)\n",
    "OUTCOME_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed OUTCOMES for the sample\n",
    "pd.DataFrame(sample_df[OUTCOME_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context and actions for 3 rows:\n",
    "# - the original sample\n",
    "# - the sample with the prescribed actions\n",
    "# - the sample with some custom actions\n",
    "comp_df = pd.concat([sample_context_action_df,\n",
    "                     sample_context_prescribed_action_df,\n",
    "                     sample_context_custom_action_df], axis=0)\n",
    "\n",
    "# Compute the outcomes\n",
    "outcomes_dict = {}\n",
    "for outcome in OUTCOME_COLUMNS:\n",
    "    # Observed outcome from the sample in the dataset\n",
    "    outcomes_dict[outcome] = [sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0],\n",
    "                              sample_df[outcome].iloc[0]]\n",
    "    # Predicted outcome\n",
    "    outcomes_dict[f'{outcome}_predicted'] = [original_actions_preds[outcome].iloc[0],\n",
    "                                             prescribed_actions_preds[outcome].iloc[0],\n",
    "                                             custom_actions_preds[outcome].iloc[0]]\n",
    "    # For numerical outcomes, compute the diff between predicted and observed\n",
    "    if is_numeric_dtype(outcomes_dict[outcome][0]):\n",
    "        diff = [a - b for a, b in zip(outcomes_dict[f'{outcome}_predicted'],\n",
    "                                      outcomes_dict[outcome])]\n",
    "        outcomes_dict[f'{outcome}_diff'] = diff\n",
    "    \n",
    "outcomes_df = pd.DataFrame(outcomes_dict)\n",
    "comp_df[list(outcomes_dict.keys())] = outcomes_df[list(outcomes_dict.keys())].values\n",
    "comp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f111",
   "metadata": {},
   "source": [
    "## Initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = sample_df[CONTEXT_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = sample_df[ACTION_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = sample_df[OUTCOME_COLUMNS].to_dict('records')[0]\n",
    "IPython.display.JSON(outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ce3d6",
   "metadata": {},
   "source": [
    "Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Strict 14-step greedy over TRUE action columns only ===\n",
    "import re, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# ----- 0) Resolve ACTION_COLS strictly -----\n",
    "def get_action_cols_from_notebook():\n",
    "    # Priority 1: from instantiated evaluator CAO mapping\n",
    "    if 'esp_evaluator' in globals():\n",
    "        try:\n",
    "            acts = list(esp_evaluator.cao_mapping.get('actions', []))\n",
    "            if acts:\n",
    "                return acts\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Priority 2: from class helper with your config\n",
    "    if 'UnileafPrescriptor' in globals() and 'config' in globals():\n",
    "        try:\n",
    "            acts = list(UnileafPrescriptor.get_action_field_names(config))\n",
    "            if acts:\n",
    "                return acts\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# pick a base df that exists in your notebook\n",
    "if 'sample_context_action_df' in globals():\n",
    "    base_df = sample_context_action_df.copy()\n",
    "    raise RuntimeError(\"Need `sample_context_action_df` or `evaluation_df` defined earlier.\")\n",
    "\n",
    "ACTION_COLS = get_action_cols_from_notebook()\n",
    "if ACTION_COLS is None:\n",
    "    # Fallback: set explicitly if you know them\n",
    "    ACTION_COLS = [f\"target_d_{i}\" for i in range(1, 15)]  # <-- edit if your names differ\n",
    "\n",
    "# Sanity: actions must exist in df\n",
    "missing_actions = [c for c in ACTION_COLS if c not in base_df.columns]\n",
    "if missing_actions:\n",
    "    raise ValueError(f\"These ACTION_COLS are not in the DataFrame: {missing_actions}\")\n",
    "\n",
    "# ----- 1) Compute NON-ACTIONS to lock at 0 -----\n",
    "# Anything that looks like an action-like feature but is NOT a true action\n",
    "# e.g., *_ind, *_score, *_prob, *_pct, *_mean, *_std, etc.\n",
    "bad_suffixes = (\n",
    "    \"_ind\",\"_indicator\",\"_score\",\"_prob\",\"_pct\",\"_perc\",\"_mean\",\"_std\",\"_z\",\n",
    "    \"_min\",\"_max\",\"_sum\",\"_count\",\"_rate\"\n",
    ")\n",
    "def is_bad_derivative(col):\n",
    "    lc = col.lower()\n",
    "    return lc.endswith(bad_suffixes) or any(k in lc for k in (\"indicator\",\"prob\",\"percent\",\"pct\",\"zscore\"))\n",
    "\n",
    "LOCKED_NON_ACTIONS = [c for c in base_df.columns if is_bad_derivative(c) and c not in ACTION_COLS]\n",
    "\n",
    "# ----- 2) Build fake \"never-done-anything\" context row -----\n",
    "def build_fake_context(context_df: pd.DataFrame) -> pd.Series:\n",
    "    fake = {}\n",
    "    for c in context_df.columns:\n",
    "        s = context_df[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            fake[c] = float(s.median()) if s.notna().any() else 0.0\n",
    "        else:\n",
    "            vals = s.dropna().tolist()\n",
    "            fake[c] = Counter(vals).most_common(1)[0][0] if vals else \"\"\n",
    "    return pd.Series(fake)\n",
    "\n",
    "# separate context from actions\n",
    "CONTEXT_COLS = [c for c in base_df.columns if c not in ACTION_COLS]\n",
    "fake_ctx = build_fake_context(base_df[CONTEXT_COLS])\n",
    "\n",
    "# Lock history/usage-ish to 0 for a \"never-done\" patient\n",
    "for c in list(fake_ctx.index):\n",
    "    lc = c.lower()\n",
    "    if any(p in lc for p in (\"n_sessions\",\"num_\",\"count_\",\"_count\",\"total_\",\"_total\",\"history\",\"usage\",\"minutes\",\"streak\")):\n",
    "        try: fake_ctx[c] = 0\n",
    "        except: pass\n",
    "\n",
    "# ----- 3) Scoring wrapper using your notebook's helper -----\n",
    "def get_scored(df):\n",
    "    preds = get_predictions(all_predictors, df, encoder)\n",
    "    out = df.join(preds)\n",
    "    # Use derived_metric directly if present, else compute as average of domain preds\n",
    "    if \"derived_metric\" not in out.columns:\n",
    "        # Average all numeric prediction columns (exclude housekeeping)\n",
    "        pred_cols = [c for c in out.columns if pd.api.types.is_numeric_dtype(out[c]) and c not in df.columns]\n",
    "        if not pred_cols:\n",
    "            raise ValueError(\"No prediction columns found; ensure get_predictions returns numeric outputs.\")\n",
    "        out[\"derived_metric\"] = out[pred_cols].mean(axis=1)\n",
    "    out[\"__score__\"] = out[\"derived_metric\"]\n",
    "    return out\n",
    "\n",
    "# ----- 4) Prepare initial state: actions OFF, locked non-actions OFF -----\n",
    "state = fake_ctx.to_dict()\n",
    "for a in ACTION_COLS:\n",
    "    state[a] = 0\n",
    "for b in LOCKED_NON_ACTIONS:\n",
    "    # ensure these never influence selection; hold them at 0\n",
    "    if pd.api.types.is_numeric_dtype(base_df[b]):\n",
    "        state[b] = 0\n",
    "\n",
    "# ----- 5) Fixed 14 steps, each step toggles ONE NEW TRUE ACTION ONLY -----\n",
    "remaining = ACTION_COLS.copy()\n",
    "trajectory = []\n",
    "\n",
    "# step 0 baseline\n",
    "s0 = get_scored(pd.DataFrame([state])).iloc[0]\n",
    "trajectory.append({\"step\": 0, \"added\": None, \"derived_metric\": float(s0[\"__score__\"])})\n",
    "\n",
    "for step in range(1, 15):  # 1..14\n",
    "    if not remaining:\n",
    "        # if fewer than 14 true actions exist, just carry forward the last score\n",
    "        last = trajectory[-1].copy()\n",
    "        last[\"step\"] = step\n",
    "        trajectory.append(last)\n",
    "        continue\n",
    "\n",
    "    trials = []\n",
    "    labels = []\n",
    "    for dom in remaining:\n",
    "        t = state.copy()\n",
    "        # lock non-actions to zero on every trial (belt-and-suspenders)\n",
    "        for b in LOCKED_NON_ACTIONS:\n",
    "            if b in t:\n",
    "                t[b] = 0\n",
    "        # turn on exactly ONE candidate action\n",
    "        t[dom] = 1\n",
    "        trials.append(t); labels.append(dom)\n",
    "\n",
    "    scored = get_scored(pd.DataFrame(trials)).reset_index(drop=True)\n",
    "    top_idx = scored[\"__score__\"].values.argmax()\n",
    "    chosen_dom = labels[top_idx]\n",
    "    chosen_score = float(scored.loc[top_idx, \"__score__\"])\n",
    "\n",
    "    # guardrail: ensure we're only ever choosing a TRUE action\n",
    "    if chosen_dom not in ACTION_COLS:\n",
    "        raise AssertionError(f\"Chosen domain '{chosen_dom}' is not in ACTION_COLS! Check column resolution.\")\n",
    "\n",
    "    # commit\n",
    "    state[chosen_dom] = 1\n",
    "    remaining.remove(chosen_dom)\n",
    "    trajectory.append({\"step\": step, \"added\": chosen_dom, \"derived_metric\": chosen_score})\n",
    "\n",
    "trajectory_df = pd.DataFrame(trajectory)\n",
    "display(trajectory_df[[\"step\",\"added\",\"derived_metric\"]])\n",
    "\n",
    "# ----- 6) Plot (and save so it always renders) -----\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(trajectory_df[\"step\"], trajectory_df[\"derived_metric\"], marker=\"o\")\n",
    "plt.title(\"Derived Metric Across 14 Domains\")\n",
    "plt.xlabel(\"Step (0 = baseline, 1–14 = domains added)\")\n",
    "plt.ylabel(\"derived_metric\")\n",
    "plt.xticks(np.arange(0, 15, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"derived_metric_trajectory.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "display(Image(\"derived_metric_trajectory.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

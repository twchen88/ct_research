{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869b337d",
   "metadata": {},
   "source": [
    "# UniLEAF Experiment\n",
    "\n",
    "Exported UniLEAF run.\n",
    "Downloaded from previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f223c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from io import StringIO\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Type\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from leaf_common.candidates.representation_types import RepresentationType\n",
    "from leaf_common.representation.rule_based.config.rule_set_config_helper import RuleSetConfigHelper\n",
    "from leaf_common.representation.rule_based.data.features import Features\n",
    "from leaf_common.representation.rule_based.data.rule_set import RuleSet\n",
    "from leaf_common.representation.rule_based.data.rule_set_binding import RuleSetBinding\n",
    "from leaf_common.representation.rule_based.evaluation.rule_set_binding_evaluator import RuleSetBindingEvaluator\n",
    "from leaf_common.representation.rule_based.persistence.rule_set_file_persistence import RuleSetFilePersistence\n",
    "from esp_sdk.esp_evaluator import EspEvaluator\n",
    "from esp_sdk.esp_service import EspService\n",
    "from unileaf_util.framework.interfaces.data_frame_predictor import DataFramePredictor\n",
    "from unileaf_util.framework.metrics.metrics_manager import MetricsManager\n",
    "from unileaf_util.framework.transformers.data_encoder import DataEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898b0a0",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf38334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_CSV = \"./train.csv\" # path to dataset CSV file\n",
    "with open(DATASET_CSV) as df_file:\n",
    "    data_source_df = pd.read_csv(df_file)\n",
    "data_source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951dec34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39a321",
   "metadata": {},
   "source": [
    "## Encode the dataset\n",
    "Encode the dataset using the fields definition from the Experiment's data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89afd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'Age': {'data_type': 'FLOAT', 'has_nan': False, 'mean': 35.674427, 'range': [0.92, 80], 'std_dev': 15.643866, 'sum': 6528.42, 'valued': 'CONTINUOUS'}, 'Sex': {'data_type': 'STRING', 'has_nan': False, 'mean': 0, 'range': [0, 0], 'std_dev': 0, 'sum': 0, 'valued': 'CATEGORICAL', 'discrete_categorical_values': ['female', 'male']}, 'Fare': {'data_type': 'FLOAT', 'has_nan': False, 'mean': 78.68247, 'range': [0, 512.3292], 'std_dev': 76.34784, 'sum': 14398.892, 'valued': 'CONTINUOUS'}, 'Parch': {'data_type': 'INT', 'has_nan': False, 'mean': 0.47540984, 'range': [0, 4], 'std_dev': 0.7546171, 'sum': 87, 'valued': 'CONTINUOUS'}, 'SibSp': {'data_type': 'INT', 'has_nan': False, 'mean': 0.46448088, 'range': [0, 3], 'std_dev': 0.64415854, 'sum': 85, 'valued': 'CONTINUOUS'}, 'Pclass': {'data_type': 'INT', 'has_nan': False, 'mean': 1.1912569, 'range': [1, 3], 'std_dev': 0.515187, 'sum': 218, 'valued': 'CONTINUOUS'}, 'Cabin_n': {'data_type': 'INT', 'has_nan': False, 'mean': 1.1639345, 'range': [1, 4], 'std_dev': 0.5193083, 'sum': 213, 'valued': 'CONTINUOUS'}, 'Cabin_r': {'data_type': 'STRING', 'has_nan': False, 'mean': 0, 'range': [0, 0], 'std_dev': 0, 'sum': 0, 'valued': 'CATEGORICAL', 'discrete_categorical_values': ['A', 'B', 'C', 'D', 'E', 'F', 'F G', 'G', 'T']}, 'Embarked': {'data_type': 'STRING', 'has_nan': False, 'mean': 0, 'range': [0, 0], 'std_dev': 0, 'sum': 0, 'valued': 'CATEGORICAL', 'discrete_categorical_values': ['C', 'Q', 'S']}, 'Survived': {'data_type': 'INT', 'has_nan': False, 'mean': 0.6721311, 'range': [0, 1], 'std_dev': 0.47072464, 'sum': 123, 'valued': 'CONTINUOUS'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8869d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Age": {
        "data_type": "FLOAT",
        "has_nan": false,
        "mean": 35.674427,
        "range": [
         0.92,
         80
        ],
        "std_dev": 15.643866,
        "sum": 6528.42,
        "valued": "CONTINUOUS"
       },
       "Cabin_n": {
        "data_type": "INT",
        "has_nan": false,
        "mean": 1.1639345,
        "range": [
         1,
         4
        ],
        "std_dev": 0.5193083,
        "sum": 213,
        "valued": "CONTINUOUS"
       },
       "Cabin_r": {
        "data_type": "STRING",
        "discrete_categorical_values": [
         "A",
         "B",
         "C",
         "D",
         "E",
         "F",
         "F G",
         "G",
         "T"
        ],
        "has_nan": false,
        "mean": 0,
        "range": [
         0,
         0
        ],
        "std_dev": 0,
        "sum": 0,
        "valued": "CATEGORICAL"
       },
       "Embarked": {
        "data_type": "STRING",
        "discrete_categorical_values": [
         "C",
         "Q",
         "S"
        ],
        "has_nan": false,
        "mean": 0,
        "range": [
         0,
         0
        ],
        "std_dev": 0,
        "sum": 0,
        "valued": "CATEGORICAL"
       },
       "Fare": {
        "data_type": "FLOAT",
        "has_nan": false,
        "mean": 78.68247,
        "range": [
         0,
         512.3292
        ],
        "std_dev": 76.34784,
        "sum": 14398.892,
        "valued": "CONTINUOUS"
       },
       "Parch": {
        "data_type": "INT",
        "has_nan": false,
        "mean": 0.47540984,
        "range": [
         0,
         4
        ],
        "std_dev": 0.7546171,
        "sum": 87,
        "valued": "CONTINUOUS"
       },
       "Pclass": {
        "data_type": "INT",
        "has_nan": false,
        "mean": 1.1912569,
        "range": [
         1,
         3
        ],
        "std_dev": 0.515187,
        "sum": 218,
        "valued": "CONTINUOUS"
       },
       "Sex": {
        "data_type": "STRING",
        "discrete_categorical_values": [
         "female",
         "male"
        ],
        "has_nan": false,
        "mean": 0,
        "range": [
         0,
         0
        ],
        "std_dev": 0,
        "sum": 0,
        "valued": "CATEGORICAL"
       },
       "SibSp": {
        "data_type": "INT",
        "has_nan": false,
        "mean": 0.46448088,
        "range": [
         0,
         3
        ],
        "std_dev": 0.64415854,
        "sum": 85,
        "valued": "CONTINUOUS"
       },
       "Survived": {
        "data_type": "INT",
        "has_nan": false,
        "mean": 0.6721311,
        "range": [
         0,
         1
        ],
        "std_dev": 0.47072464,
        "sum": 123,
        "valued": "CONTINUOUS"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd31dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cao_mapping = {'context': ['Age', 'Sex', 'Fare', 'Pclass'], 'actions': ['Parch', 'SibSp', 'Cabin_n', 'Cabin_r', 'Embarked'], 'outcomes': ['Survived']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b6c155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "actions": [
        "Parch",
        "SibSp",
        "Cabin_n",
        "Cabin_r",
        "Embarked"
       ],
       "context": [
        "Age",
        "Sex",
        "Fare",
        "Pclass"
       ],
       "outcomes": [
        "Survived"
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(cao_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96e39e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['nan'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m encoder = DataEncoder(fields, cao_mapping)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoded_data_source_df = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_as_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_source_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m encoded_data_source_df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/unileaf_util/framework/transformers/data_encoder.py:161\u001b[39m, in \u001b[36mDataEncoder.encode_as_df\u001b[39m\u001b[34m(self, data_df)\u001b[39m\n\u001b[32m    158\u001b[39m     raw_values = np.ravel(raw_values, order=\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Encode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m encoded_values = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_categorical(column):\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Categorical value, encoded as a numpy array.\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Convert it to list for Pandas to handle it nicely\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Source:\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# https://stackoverflow.com/questions/45548426/store-numpy-array-in-cells-of-a-pandas-dataframe\u001b[39;00m\n\u001b[32m    168\u001b[39m     values_by_column[column] = encoded_values.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:1043\u001b[39m, in \u001b[36mOneHotEncoder.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1038\u001b[39m     warn_on_unknown = \u001b[38;5;28mself\u001b[39m.drop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m   1039\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1040\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minfrequent_if_exist\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1041\u001b[39m     }\n\u001b[32m   1042\u001b[39m     handle_unknown = \u001b[38;5;28mself\u001b[39m.handle_unknown\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m X_int, X_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m n_samples, n_features = X_int.shape\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CT/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:218\u001b[39m, in \u001b[36m_BaseEncoder._transform\u001b[39m\u001b[34m(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown == \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    214\u001b[39m     msg = (\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m during transform\u001b[39m\u001b[33m\"\u001b[39m.format(diff, i)\n\u001b[32m    217\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[31mValueError\u001b[39m: Found unknown categories ['nan'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "encoder = DataEncoder(fields, cao_mapping)\n",
    "encoded_data_source_df = encoder.encode_as_df(data_source_df)\n",
    "encoded_data_source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fc2f4",
   "metadata": {},
   "source": [
    "## Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSOR = 'regressor'\n",
    "CLASSIFIER = 'classifier'\n",
    "TYPES = [REGRESSOR, CLASSIFIER]\n",
    "predictors_by_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36331e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorType:\n",
    "    \"\"\"\n",
    "    This class defines the type of Predictor Possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, predictor_type: str):\n",
    "        \"\"\"\n",
    "        The constructor confirms if the type of predictor is supported.\n",
    "        :param predictor_type: String describing a name for the type of the\n",
    "        predictor.\n",
    "        \"\"\"\n",
    "        assert predictor_type in TYPES, \"Invalid Predictor Type\"\n",
    "        self.type = predictor_type\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        This function overrides the string representation of the\n",
    "        class.\n",
    "        :return self.type: String\n",
    "        \"\"\"\n",
    "        return self.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(DataFramePredictor, ABC):\n",
    "    \"\"\"\n",
    "    This class contains the contract that any predictor\n",
    "    must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict[str, float],\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        Initializes a predictor, its params and the metadata.\n",
    "        :param data_df: DataFrame containing all processed data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes`\n",
    "        keys where each key returns a List of the selected column names as strings.\n",
    "        :param data_split: Dictionary containing the training splits indexed\n",
    "        by \"train_pct\" and \"val_pct\".\n",
    "        :param model_params: Parameters of the model\n",
    "        :param metadata: Dictionary describing any other information\n",
    "        that must be stored along with the model.\n",
    "        This might help in uniquely identifying the model\n",
    "        :returns nothing\n",
    "        \"\"\"\n",
    "        # Split the data between train, val and test sets\n",
    "        self.data_split = data_split\n",
    "\n",
    "        self.cao_mapping = cao_mapping\n",
    "        self.context_actions_columns = self.cao_mapping[\"context\"] + self.cao_mapping[\"actions\"]\n",
    "        # Check\n",
    "        if len(cao_mapping[\"outcomes\"]) > 1:\n",
    "            if not self.does_support_multiobjective():\n",
    "                raise ValueError(f\"{self.predictor_name} does NOT support multiple outputs\")\n",
    "\n",
    "        self.column_length = {}\n",
    "        if data_df is not None:\n",
    "            train_df, val_df, test_df = self.generate_data_split(data_df, self.data_split)\n",
    "\n",
    "            # Split the data between features (x) and labels(y)\n",
    "            self.train_x_df, self.train_y_df = self.get_data_xy_split(train_df, cao_mapping)\n",
    "            self.val_x_df, self.val_y_df = self.get_data_xy_split(val_df, cao_mapping)\n",
    "            self.test_x_df, self.test_y_df = self.get_data_xy_split(test_df, cao_mapping)\n",
    "\n",
    "            # Keep track of how many values are used to encode each outcome\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                first_value = self.train_y_df[column].head(1).values[0]\n",
    "                if isinstance(first_value, numbers.Number):\n",
    "                    # Value is a single scalar\n",
    "                    self.column_length[column] = 1\n",
    "                else:\n",
    "                    # value is a one-hot encoded vector, i.e. a list. Get its size.\n",
    "                    self.column_length[column] = len(self.train_y_df[column].head(1).values[0])\n",
    "        else:\n",
    "            # No data provided, assuming outcomes are numerical (not categorical)\n",
    "            for column in self.cao_mapping[\"outcomes\"]:\n",
    "                self.column_length[column] = 1\n",
    "\n",
    "        if model_params is None:\n",
    "            model_params = {}\n",
    "        self.model_params = model_params\n",
    "\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        self.metadata = metadata\n",
    "\n",
    "        # Internal Parameters that are used to store the\n",
    "        # latest state of the model.\n",
    "        self._trained_model = None\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_type(self) -> PredictorType:\n",
    "        \"\"\"\n",
    "        :return the PredictorType of this Predictor: Regressor or Classifier\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def library(self) -> str:\n",
    "        \"\"\"\n",
    "        :return the underlying library that implements this predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def predictor_name(self) -> str:\n",
    "        \"\"\"\n",
    "        :return: the name of the Predictor, as a string\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_model(self, model_params: Dict):\n",
    "        \"\"\"\n",
    "        This function must be overridden to build the model using the model\n",
    "        parameters if desired and return a model.\n",
    "        :param model_params: Dictionary containing the model parameters\n",
    "        :return model: The built model.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_model(self, model,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray]) -> Type:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_default_params() -> Dict:\n",
    "        \"\"\"\n",
    "        This function should return the default Parameters of the model\n",
    "        as Dictionary along with a description.\n",
    "        :return default_model_params: Dictionary\n",
    "        Format: {\n",
    "            \"parameter_name\": {\n",
    "                \"default_value\": \"\",\n",
    "                \"description\": \"\"\n",
    "            },\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "    def save_trained_model_state(self, trained_model) -> None:\n",
    "        \"\"\"\n",
    "        This function stores the state of the trained model.\n",
    "        :param trained_model: Trained model\n",
    "        :return Nothing:\n",
    "        \"\"\"\n",
    "        self._trained_model = trained_model\n",
    "\n",
    "    def get_trained_model(self):\n",
    "        \"\"\"\n",
    "        This function returns the trained model if it exists.\n",
    "        :return self._trained_model:\n",
    "        \"\"\"\n",
    "        if self._trained_model:\n",
    "            return self._trained_model\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_split(data_df: pd.DataFrame,\n",
    "                            data_split: Dict[str, Any]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the data between train, validation (optional) and test sets\n",
    "        :param data_df: the full dataset as a Pandas DataFrame\n",
    "        :param data_split: a dictionary with the\n",
    "        :return: a tuple of Pandas DataFrame: one for train, one for validation (or None), and one for test\n",
    "        \"\"\"\n",
    "\n",
    "        # First, split the data set in train and test sets.\n",
    "        # Use the provided random_state, if any\n",
    "        random_state = data_split.get(\"random_state\", None)\n",
    "        shuffle = data_split.get(\"shuffle\", True)\n",
    "        train_df, test_df = train_test_split(data_df,\n",
    "                                             test_size=data_split[\"test_pct\"],\n",
    "                                             random_state=random_state,\n",
    "                                             shuffle=shuffle)\n",
    "\n",
    "        # If we also need a validation set, split the train set into train and validation sets.\n",
    "        val_pct = data_split.get(\"val_pct\", 0)\n",
    "        if val_pct > 0:\n",
    "            train_df, val_df = train_test_split(train_df,\n",
    "                                                test_size=val_pct,\n",
    "                                                random_state=random_state,\n",
    "                                                shuffle=shuffle)\n",
    "        else:\n",
    "            val_df = None\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def predict(self, encoded_context_actions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method uses the trained model to make a prediction for the passed Pandas DataFrame\n",
    "        of context and actions. Returns the predicted outcomes in a Pandas DataFrame.\n",
    "        :param encoded_context_actions_df: a Pandas DataFrame containing encoded rows of context and actions for\n",
    "        which a prediction is requested. Categorical columns contain one-hot vectors, e.g. [1, 0, 0]. Which means\n",
    "        a row can be a list of arrays (1 per column), e.g.: [1, 0, 0], [1,0].\n",
    "        :return a Pandas DataFrame of the predicted outcomes for each context and actions row.\n",
    "        \"\"\"\n",
    "        # Default implementation\n",
    "        if self._trained_model:\n",
    "            # Predict using the model's input columns, in case encoded_context_actions_df contains more columns\n",
    "            # or is in a different order\n",
    "            context_action_df = encoded_context_actions_df[self.context_actions_columns]\n",
    "            # Convert one-hot vector columns into a single feature vector\n",
    "            features = DataEncoder.encoded_df_to_np(context_action_df)\n",
    "            predictions = self._trained_model.predict(features)\n",
    "            if isinstance(predictions, pd.DataFrame):\n",
    "                # Predictions are already in a DataFrame. Make sure they have the correct outcome names\n",
    "                predictions_df = predictions\n",
    "                predictions_df.columns = self.cao_mapping[\"outcomes\"]\n",
    "                # Convert predictions to float64 as it's JSON serializable, while float32 is not\n",
    "                predictions_df = predictions_df.astype(\"float64\")\n",
    "            else:\n",
    "                # Assuming predictions is a ndarray, convert it to a DataFrame with the output column names\n",
    "                predictions_df = DataEncoder.np_to_encoded_df(predictions,\n",
    "                                                              self.column_length)\n",
    "        else:\n",
    "            raise ValueError(\"Can't make predictions because the model has not been trained\")\n",
    "        return predictions_df\n",
    "\n",
    "    @staticmethod\n",
    "    def export_model(model_bytes: bytes, file_path: str):\n",
    "        \"\"\"\n",
    "        Saves the model's bytes to the specified location\n",
    "        :param model_bytes: the model bytes\n",
    "        :param file_path: the name and path of the file to persist the bytes to\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        # Save the Model Locally\n",
    "        with open(file_path, \"wb\") as my_file:\n",
    "            my_file.write(model_bytes)\n",
    "\n",
    "    @staticmethod\n",
    "    def export_metrics(metrics_dict: Dict[str, Any], file_path: str):\n",
    "        \"\"\"\n",
    "        Save the model's training metrics to the specified location\n",
    "        :param metrics_dict: a dictionary containing metrics\n",
    "        :param file_path: the name and path of the file to persist the bytes to\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as my_file:\n",
    "            json.dump(metrics_dict, my_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_xy_split(data_df: Optional[pd.DataFrame],\n",
    "                          cao_mapping: Dict) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        This function takes a dataframe and a dictionary mapping indices to context,\n",
    "        action, or outcome. This then splits the dataframe into two dataframes based\n",
    "        on it's CAO tagging.\n",
    "\n",
    "        data_x: Context and Actions\n",
    "        data_y: Outcomes\n",
    "\n",
    "        :param data_df: a Pandas DataFrame with all the data\n",
    "        :param cao_mapping: a dictionary with `context`, `actions` and `outcomes` keys where each key returns a List\n",
    "         ofthe selected column names as strings.\n",
    "        :return: A tuple containing two dataframes: data_x with the features, and data_y with the labels (outcomes)\n",
    "        \"\"\"\n",
    "        if data_df is None:\n",
    "            return None, None\n",
    "\n",
    "        data_x_df = data_df[cao_mapping[\"context\"] + cao_mapping[\"actions\"]]\n",
    "        data_y_df = data_df[cao_mapping[\"outcomes\"]]\n",
    "\n",
    "        return data_x_df, data_y_df\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.predictor_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Regressor Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(REGRESSOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f76916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(PredictorType):\n",
    "    \"\"\"\n",
    "    This class defines a Classifier Type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor initializes the super class.\n",
    "        \"\"\"\n",
    "        super().__init__(CLASSIFIER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef8904",
   "metadata": {},
   "source": [
    "## Predictor 2627a716-23c3-3df7-ff1e-74af944ae7cf\n",
    "### CAO columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLUMNS = ['Age', 'Sex', 'Fare', 'Pclass']\n",
    "ACTION_COLUMNS = ['Parch', 'SibSp', 'Cabin_n', 'Cabin_r', 'Embarked']\n",
    "OUTCOME_COLUMNS = ['Survived']\n",
    "CONTEXT_ACTION_COLUMNS = CONTEXT_COLUMNS + ACTION_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde52cc",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41407341",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {\"train_pct\": 0.8, \"test_pct\": 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2895d8b",
   "metadata": {},
   "source": [
    "### Predictor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressorPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    This class implements a Random Forest regression model from the SKLearn library.\n",
    "    \"\"\"\n",
    "    # Properties:\n",
    "    predictor_type = Regressor()\n",
    "    library = \"sklearn\"\n",
    "    predictor_name = f\"{library} Random Forest Regressor\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_df: pd.DataFrame,\n",
    "                 cao_mapping: Dict[str, List[str]],\n",
    "                 data_split: Dict = None,\n",
    "                 model_params: Dict = None,\n",
    "                 metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        The constructor initializes the base params.\n",
    "        \"\"\"\n",
    "        super().__init__(data_df=data_df,\n",
    "                         data_split=data_split,\n",
    "                         model_params=model_params,\n",
    "                         metadata=metadata,\n",
    "                         cao_mapping=cao_mapping)\n",
    "\n",
    "    @staticmethod\n",
    "    def does_support_multiobjective() -> bool:\n",
    "        \"\"\"\n",
    "        This function returns if the predictor supports multiple outputs\n",
    "        or not.\n",
    "        :return multioutput: Bool\n",
    "        \"\"\"\n",
    "        multioutput = True\n",
    "        return multioutput\n",
    "\n",
    "    def build_model(self, model_params: Dict[str, Any]) -> RandomForestRegressor:\n",
    "        \"\"\"\n",
    "        This function instantiates a RandomForestRegressor with the given params.\n",
    "        :return model: a RandomForestRegressor instance\n",
    "        \"\"\"\n",
    "        model = RandomForestRegressor(**model_params)\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model: RandomForestRegressor,\n",
    "                    train_x: np.ndarray, train_y: np.ndarray,\n",
    "                    val_x: Optional[np.ndarray], val_y: Optional[np.ndarray]) -> RandomForestRegressor:\n",
    "        \"\"\"\n",
    "        This function must be overridden to train the built model from the build_model step\n",
    "        given the Data and must return the trained model and the desired metrics as a dictionary.\n",
    "        :param model: The model built in the build_model step\n",
    "        :param train_x: numpy array containing the processed input features split for training\n",
    "        :param train_y: numpy array containing the processed output features split for training\n",
    "        :param val_x: Optional numpy array containing the processed input features split for validation\n",
    "        :param val_y: Optional numpy array containing the processed output features split for validation\n",
    "\n",
    "        :return trained_model: The Random Forest model trained\n",
    "        \"\"\"\n",
    "        if train_y.shape[1] == 1:\n",
    "            # When there is only 1 label fit expects a 1d array.\n",
    "            model.fit(train_x, train_y.ravel())\n",
    "        else:\n",
    "            model.fit(train_x, train_y)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_params() -> Dict:\n",
    "        \"\"\"\n",
    "        This function returns the default parameters along with a description.\n",
    "        :return default_params: Default values with Description.\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            \"n_estimators\": {\n",
    "                \"default_value\": 100,\n",
    "                \"description\": \"The number of trees in the forest.\",\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"criterion\": {\n",
    "                \"default_value\": \"mse\",\n",
    "                \"description\": \"The function to measure the quality of a split. \"\n",
    "                               \"Supported criteria are “mse” for the mean squared error, \"\n",
    "                               \"which is equal to variance reduction as feature selection \"\n",
    "                               \"criterion, and “mae” for the mean absolute error. Options: {'mse', 'mae'}\",\n",
    "                \"type\": ['mse', 'mae']\n",
    "            },\n",
    "            \"max_depth\": {\n",
    "                \"default_value\": 100,\n",
    "                \"description\": \"The maximum depth of the tree. \"\n",
    "                               \"If None, then nodes are expanded until all \"\n",
    "                               \"leaves are pure or until all leaves contain less than \"\n",
    "                               \"min_samples_split samples.\",\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"min_samples_split\": {\n",
    "                \"default_value\": 2,\n",
    "                \"description\": \"The minimum number of samples required to split an internal node.\",\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"min_samples_leaf\": {\n",
    "                \"default_value\": 1,\n",
    "                \"description\": '''\n",
    "                The minimum number of samples required to be at a leaf node.\n",
    "                A split point at any depth will only be considered if it\n",
    "                leaves at least min_samples_leaf training samples in each of the left and right branches.\n",
    "                This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "                If int, then consider min_samples_leaf as the minimum number.\n",
    "                If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples)\n",
    "                are the minimum number of samples for each node.\n",
    "                ''',\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"min_weight_fraction_leaf\": {\n",
    "                \"default_value\": 0.0,\n",
    "                \"description\": \"The minimum weighted fraction of the sum total of\"\n",
    "                               \" weights (of all the input samples) required to be at a leaf node. \"\n",
    "                               \"Samples have equal weight when sample_weight is not provided.\",\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"max_features\": {\n",
    "                \"default_value\": \"auto\",\n",
    "                \"description\": '''\n",
    "                The number of features to consider when looking for the best split:\n",
    "\n",
    "                If int, then consider max_features features at each split.\n",
    "                If float, then max_features is a fraction and round(max_features * n_features) features are considered\n",
    "                at each split.\n",
    "                If “auto”, then max_features=n_features.\n",
    "                If “sqrt”, then max_features=sqrt(n_features).\n",
    "                If “log2”, then max_features=log2(n_features).\n",
    "                If None, then max_features=n_features.\n",
    "                Note: the search for a split does not stop until at least one valid partition of the node samples\n",
    "                is found, even if it requires to effectively inspect more than max_features features.\n",
    "                ''',\n",
    "                \"type\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "            },\n",
    "            \"max_leaf_nodes\": {\n",
    "                \"default_value\": 100,\n",
    "                \"description\": \"Grow trees with max_leaf_nodes in best-first fashion. \"\n",
    "                               \"Best nodes are defined as relative reduction in impurity. \"\n",
    "                               \"If None then unlimited number of leaf nodes.\",\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"min_impurity_decrease\": {\n",
    "                \"default_value\": 0.0,\n",
    "                \"description\": '''\n",
    "                A node will be split if this split induces a decrease of the impurity greater than or equal\n",
    "                to this value.\n",
    "\n",
    "                The weighted impurity decrease equation is the following:\n",
    "\n",
    "                N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                    - N_t_L / N_t * left_impurity)\n",
    "                where N is the total number of samples, N_t is the number of samples at the current node,\n",
    "                N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\n",
    "\n",
    "                N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.\n",
    "                ''',\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"bootstrap\": {\n",
    "                \"default_value\": True,\n",
    "                \"description\": \"Whether bootstrap samples are used when building trees. \"\n",
    "                               \"If False, the whole dataset is used to build each tree.\",\n",
    "                \"type\": \"bool\"\n",
    "            },\n",
    "            \"oob_score\": {\n",
    "                \"default_value\": False,\n",
    "                \"description\": \"whether to use out-of-bag samples to estimate the R^2 on unseen data\",\n",
    "                \"type\": \"bool\"\n",
    "            },\n",
    "            \"n_jobs\": {\n",
    "                \"default_value\": 1,\n",
    "                \"description\": \"The number of jobs to run in parallel. fit, predict, \"\n",
    "                               \"decision_path and apply are all parallelized over the trees. \"\n",
    "                               \"None means 1 unless in a joblib.parallel_backend context. \"\n",
    "                               \"-1 means using all processors. See Glossary for more details.\",\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"random_state\": {\n",
    "                \"default_value\": 0,\n",
    "                \"description\": \"Controls both the randomness of the bootstrapping of the samples \"\n",
    "                               \"used when building trees (if bootstrap=True) and the sampling of the \"\n",
    "                               \"features to consider when looking for the best split at each \"\n",
    "                               \"node (if max_features < n_features)\",\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"warm_start\": {\n",
    "                \"default_value\": False,\n",
    "                \"description\": \"When set to True, reuse the solution of the previous call to \"\n",
    "                               \"fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.\",\n",
    "                \"type\": \"bool\"\n",
    "            },\n",
    "            \"ccp_alpha\": {\n",
    "                \"default_value\": 0.0,\n",
    "                \"description\": \"Complexity parameter used for Minimal Cost-Complexity Pruning. \"\n",
    "                               \"The subtree with the largest cost complexity that is smaller than \"\n",
    "                               \"ccp_alpha will be chosen. By default, no pruning is performed. \",\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"max_samples\": {\n",
    "                \"default_value\": 1,\n",
    "                \"description\": '''\n",
    "                If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "                If None (default), then draw X.shape[0] samples.\n",
    "                If int, then draw max_samples samples.\n",
    "                If float, then draw max_samples * X.shape[0] samples.\n",
    "                Thus, max_samples should be in the interval (0, 1).\n",
    "                ''',\n",
    "                \"type\": \"float\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return default_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_node_id = '2627a716-23c3-3df7-ff1e-74af944ae7cf'\n",
    "predictor = RandomForestRegressorPredictor(encoded_data_source_df,\n",
    "    cao_mapping={'context': ['Age', 'Sex', 'Fare', 'Pclass'], 'actions': ['Parch', 'SibSp', 'Cabin_n', 'Cabin_r', 'Embarked'], 'outcomes': ['Survived']},\n",
    "data_split=data_split,\n",
    "model_params={},\n",
    "metadata={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83e418",
   "metadata": {},
   "source": [
    "### Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84222ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = DataEncoder.encoded_df_to_np(predictor.train_x_df)\n",
    "train_y = DataEncoder.encoded_df_to_np(predictor.train_y_df)\n",
    "if predictor.val_x_df is not None:\n",
    "    val_x = DataEncoder.encoded_df_to_np(predictor.val_x_df)\n",
    "    val_y = DataEncoder.encoded_df_to_np(predictor.val_y_df)\n",
    "else:\n",
    "    val_x = None\n",
    "    val_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda87b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = predictor.build_model(predictor.model_params)\n",
    "trained_model = predictor.train_model(model,\n",
    "                                      train_x, train_y,\n",
    "                                      val_x, val_y)\n",
    "predictor.save_trained_model_state(trained_model)\n",
    "predictors_by_id[predictor_node_id] = predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3d6bd",
   "metadata": {},
   "source": [
    "### Predictor metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = [MetricsManager.get_calculator('Mean Absolute Error')]\n",
    "metrics = MetricsManager.compute_metrics(predictor,\n",
    "model_metrics,predictor.train_x_df, predictor.train_y_df,predictor.val_x_df, predictor.val_y_df,predictor.test_x_df, predictor.test_y_df,encoder)\n",
    "\n",
    "print(f'Predictor trained. Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92f7ea",
   "metadata": {},
   "source": [
    "## Prescriptor\n",
    "### ESP credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_username = os.getenv('ESP_SERVICE_USER')\n",
    "esp_password = os.getenv('ESP_SERVICE_PASSWORD')\n",
    "if not esp_username or not esp_password:\n",
    "    print('Please set environment variables before proceeding.')\n",
    "else:\n",
    "    print('ESP Service username and password found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddfa38",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720131d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnileafPrescriptor(EspEvaluator):\n",
    "    \"\"\"\n",
    "    An Unileaf Prescriptor makes prescriptions given an ESP candidate and a context DataFrame.\n",
    "    It is also an EspEvaluator implementation that returns metrics for ESP candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: Dict[str, Any],\n",
    "                 evaluation_df: pd.DataFrame,\n",
    "                 data_encoder: DataEncoder,\n",
    "                 predictors: List[Predictor]):\n",
    "        \"\"\"\n",
    "        Constructs a prescriptor evaluator\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :param evaluation_df: the Pandas DataFrame to use to evaluate the candidates\n",
    "        :param data_encoder: the DataEncoder used to encode the dataset\n",
    "        :param predictors: the predictors this prescriptor relies on\n",
    "        \"\"\"\n",
    "        # Instantiate EspEvaluator\n",
    "        # Note: sets self.config\n",
    "        super().__init__(config)\n",
    "\n",
    "        # CAO\n",
    "        self.cao_mapping = {\"context\": self.get_context_field_names(config),\n",
    "                            \"actions\": self.get_action_field_names(config),\n",
    "                            \"outcomes\": self.get_fitness_metrics(config)}\n",
    "        self.context_df = evaluation_df[self.cao_mapping[\"context\"]]\n",
    "        self.row_index = self.context_df.index\n",
    "\n",
    "        # Convert the context DataFrame to a format a NN can ingest\n",
    "        self.context_as_nn_input = self.convert_to_nn_input(self.context_df)\n",
    "\n",
    "        # Data encoder\n",
    "        self.data_encoder = data_encoder\n",
    "\n",
    "        # Predictors\n",
    "        self.predictors = predictors\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_nn_input(context_df: pd.DataFrame) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Converts a context DataFrame to a list of numpy arrays a neural network can ingest\n",
    "        :param context_df: a DataFrame containing inputs for a neural network. Number of inputs and size must match\n",
    "        :return: a list of numpy ndarray, on ndarray per neural network input\n",
    "        \"\"\"\n",
    "        # The NN expects a list of i inputs by s samples (e.g. 9 x 299).\n",
    "        # So convert the data frame to a numpy array (gives shape 299 x 9), transpose it (gives 9 x 299)\n",
    "        # and convert to list(list of 9 arrays of 299)\n",
    "        context_as_nn_input = list(context_df.to_numpy().transpose())\n",
    "        # Convert each column's list of 1D array to a 2D array\n",
    "        context_as_nn_input = [np.stack(context_as_nn_input[i], axis=0) for i in\n",
    "                               range(len(context_as_nn_input))]\n",
    "        return context_as_nn_input\n",
    "\n",
    "    def evaluate_candidate(self, candidate):\n",
    "        \"\"\"\n",
    "        Evaluates a single Prescriptor candidate and returns its metrics.\n",
    "        Implements the EspEvaluator interface\n",
    "        :param candidate: a Keras neural network or rule based Prescriptor candidate\n",
    "        :return metrics: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Prescribe actions\n",
    "        prescribed_actions_df = self.prescribe(candidate)\n",
    "\n",
    "        # Aggregate the context and actions dataframes.\n",
    "        context_actions_df = pd.concat([self.context_df,\n",
    "                                        prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "\n",
    "        # Compute the metrics\n",
    "        metrics = self._compute_metrics(context_actions_df)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_metrics(self, context_actions_df):\n",
    "        \"\"\"\n",
    "        Computes metrics from the passed context/actions DataFrame using the instance's trained predictors.\n",
    "        :param context_actions_df: a DataFrame of context / prescribed actions\n",
    "        :return: A dictionary of {'metric_name': metric_value}\n",
    "        \"\"\"\n",
    "        # Get the predicted outcomes from the predictors\n",
    "        metrics = {}\n",
    "        for predictor in self.predictors:\n",
    "            predicted_outcomes = predictor.predict(context_actions_df)\n",
    "\n",
    "            # UN-853: Decode predictions before computing numerical metrics, if a data_encoder is available\n",
    "            if self.data_encoder is not None:\n",
    "                decoded_predicted_outcomes = self.data_encoder.decode_as_df(predicted_outcomes)\n",
    "            else:\n",
    "                decoded_predicted_outcomes = predicted_outcomes\n",
    "\n",
    "            # Only add a metric for the outcomes the prescriptor is interested in\n",
    "            for outcome in self.cao_mapping[\"outcomes\"]:\n",
    "                # Add the metrics that have been produced by this predictor\n",
    "                if outcome in predictor.cao_mapping[\"outcomes\"]:\n",
    "                    # Check the type of metric: numerical or categorical?\n",
    "                    if decoded_predicted_outcomes[[outcome]].iloc[:, 0].dtype == object:\n",
    "                        # Categorical outcome. Use the *encoded* predicted outcome.\n",
    "                        preds = predicted_outcomes[outcome]\n",
    "                        # Classifiers return the category's index in the list of categories, so we can take the mean\n",
    "                        # of the encoded outcomes. Note: this works because Outcomes are encoded using LabelEncoder\n",
    "                        # AND the user defined order for each Outcome categories.\n",
    "                        metrics[outcome] = preds.mean()\n",
    "                    else:\n",
    "                        # UN-853: Numerical outcome. Use the *decoded*, i.e. scaled back, predicted outcome\n",
    "                        preds = decoded_predicted_outcomes[outcome]\n",
    "                        # Regressors produce floats: take the mean of the decoded outcome\n",
    "                        metrics[outcome] = preds.mean()\n",
    "        return metrics\n",
    "\n",
    "    def prescribe(self, candidate, context_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed candidate and context\n",
    "        :param candidate: an ESP candidate, either neural network or rules\n",
    "        :param context_df: a DataFrame containing the context to prescribe for,\n",
    "         or None to use the instance one\n",
    "        :return: a DataFrame containing actions prescribed for each context\n",
    "        \"\"\"\n",
    "        if context_df is None:\n",
    "            # No context is provided, use the instance's one\n",
    "            context_as_nn_input = self.context_as_nn_input\n",
    "            row_index = self.row_index\n",
    "        else:\n",
    "            # Convert the context DataFrame to something more suitable for neural networks\n",
    "            context_as_nn_input = self.convert_to_nn_input(context_df)\n",
    "            # Use the context's row index\n",
    "            row_index = context_df.index\n",
    "\n",
    "        is_rule_based = isinstance(candidate, RuleSet)\n",
    "        if is_rule_based:\n",
    "            actions = self._prescribe_from_rules(candidate, context_as_nn_input)\n",
    "        else:\n",
    "            actions = self._prescribe_from_nn(candidate, context_as_nn_input)\n",
    "\n",
    "        # Convert the prescribed actions to a DataFrame\n",
    "        prescribed_actions_df = pd.DataFrame(actions,\n",
    "                                             columns=self.cao_mapping[\"actions\"],\n",
    "                                             index=row_index)\n",
    "        return prescribed_actions_df\n",
    "\n",
    "    def _prescribe_from_rules(self, candidate, context_as_nn_input: List[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed rules model candidate and context\n",
    "        :param candidate: a rules model candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to list of action values\n",
    "        \"\"\"\n",
    "        cand_states = RuleSetConfigHelper.get_states(self.config)\n",
    "        cand_actions = RuleSetConfigHelper.get_actions(self.config)\n",
    "        candidate = RuleSetBinding(candidate, cand_states, cand_actions)\n",
    "        rules_encoder = RulesDataEncoder(candidate.actions)\n",
    "        evaluator = RuleSetBindingEvaluator()\n",
    "        rules_input = rules_encoder.encode_to_rules_data(context_as_nn_input)\n",
    "        rules_output = evaluator.evaluate(candidate, rules_input)\n",
    "        actions = rules_encoder.decode_from_rules_data(rules_output)\n",
    "        return actions\n",
    "\n",
    "    def _prescribe_from_nn(self, candidate, context_as_nn_input: List[np.ndarray]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates prescriptions using the passed neural network candidate and context\n",
    "        :param candidate: a Keras neural network candidate\n",
    "        :param context_as_nn_input: a numpy array containing the context to prescribe for\n",
    "        :return: a dictionary of action name to action value or list of action values\n",
    "        \"\"\"\n",
    "        # Get the prescribed actions\n",
    "        prescribed_actions = candidate.predict(context_as_nn_input)\n",
    "        actions = {}\n",
    "\n",
    "        if self._is_single_action_prescriptor():\n",
    "            # Put the single action in an array to process it like multiple actions\n",
    "            prescribed_actions = [prescribed_actions]\n",
    "\n",
    "        for i, action_col in enumerate(self.cao_mapping[\"actions\"]):\n",
    "            if self._is_scalar(prescribed_actions[i]):\n",
    "                # We have a single row and this action is numerical. Convert it to a scalar.\n",
    "                actions[action_col] = prescribed_actions[i].item()\n",
    "            else:\n",
    "                actions[action_col] = prescribed_actions[i].tolist()\n",
    "        return actions\n",
    "\n",
    "    def _is_single_action_prescriptor(self):\n",
    "        \"\"\"\n",
    "        Checks how many Actions have been defined in the Context, Actions, Outcomes mapping.\n",
    "        :return: True if only 1 action is defined, False otherwise\n",
    "        \"\"\"\n",
    "        return len(self.cao_mapping[\"actions\"]) == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_scalar(prescribed_action):\n",
    "        \"\"\"\n",
    "        Checks if the prescribed action contains a single value, i.e. a scalar, or an array.\n",
    "        A prescribed action contains a single value if it has been prescribed for a single context sample\n",
    "        :param prescribed_action: a scalar or an array\n",
    "        :return: True if the prescribed action contains a scalar, False otherwise.\n",
    "        \"\"\"\n",
    "        return prescribed_action.shape[0] == 1 and prescribed_action.shape[1] == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_context_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Context column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Context column names\n",
    "        \"\"\"\n",
    "        nn_inputs = config[\"network\"][\"inputs\"]\n",
    "        contexts = [nn_input[\"name\"] for nn_input in nn_inputs]\n",
    "        return contexts\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_field_names(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of Action column names\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of Action column names\n",
    "        \"\"\"\n",
    "        nn_outputs = config[\"network\"][\"outputs\"]\n",
    "        actions = [nn_output[\"name\"] for nn_output in nn_outputs]\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fitness_metrics(config: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns the list of fitness metric names (Outcomes) to optimize.\n",
    "        :param config: the ESP experiment config dictionary\n",
    "        :return: the list of fitness metric names\n",
    "        \"\"\"\n",
    "        metrics = config[\"evolution\"][\"fitness\"]\n",
    "        fitness_metrics = [metric[\"metric_name\"] for metric in metrics]\n",
    "        return fitness_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18034bd1",
   "metadata": {},
   "source": [
    "### Prescriptor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85032fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'evolution': {'fitness': [{'maximize': True, 'metric_name': 'Survived'}], 'nb_elites': 5, 'mutation_type': 'gaussian_noise_percentage', 'nb_generations': 40, 'mutation_factor': 0.1, 'population_size': 10, 'parent_selection': 'tournament', 'initialization_range': 1, 'mutation_probability': 0.1, 'remove_population_pct': 0.8, 'initialization_distribution': 'orthogonal'}, 'network': {'inputs': [{'name': 'Age', 'size': 1, 'values': ['float']}, {'name': 'Sex', 'size': 2, 'values': ['female', 'male']}, {'name': 'Fare', 'size': 1, 'values': ['float']}, {'name': 'Pclass', 'size': 1, 'values': ['float']}], 'outputs': [{'name': 'Parch', 'size': 1, 'activation': 'sigmoid', 'use_bias': True, 'values': ['float']}, {'name': 'SibSp', 'size': 1, 'activation': 'sigmoid', 'use_bias': True, 'values': ['float']}, {'name': 'Cabin_n', 'size': 1, 'activation': 'sigmoid', 'use_bias': True, 'values': ['float']}, {'name': 'Cabin_r', 'size': 9, 'activation': 'softmax', 'use_bias': True, 'values': ['A', 'B', 'C', 'D', 'E', 'F', 'F G', 'G', 'T']}, {'name': 'Embarked', 'size': 3, 'activation': 'softmax', 'use_bias': True, 'values': ['C', 'Q', 'S']}], 'hidden_layers': [{'layer_name': 'hidden_1', 'layer_type': 'Dense', 'layer_params': {'units': 16, 'use_bias': True, 'activation': 'tanh'}}]}, 'LEAF': {'representation': 'NNWeights', 'experiment_id': 'UniLEAF_184be189-265f-ce68-351a-33cbccd4f49d', 'version': '1.0.0', 'persistence_dir': 'trained_prescriptors/', 'candidates_to_persist': 'best'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eecd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_predictor_ids = ['2627a716-23c3-3df7-ff1e-74af944ae7cf']\n",
    "all_predictors = [predictors_by_id[required_id] for required_id in required_predictor_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the EspService\n",
    "esp_service = EspService(config, esp_username, esp_password)\n",
    "esp_evaluator = UnileafPrescriptor(config,\n",
    "                                   encoded_data_source_df,\n",
    "                                   encoder,\n",
    "                                   all_predictors)\n",
    "experiment_results_dir = esp_service.train(esp_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17573832",
   "metadata": {},
   "source": [
    "## ESP Summary Stats\n",
    "esp_service.train(...) returned the directory in which the experiment results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = os.path.join(experiment_results_dir, 'experiment_stats.csv')\n",
    "with open(stats_file) as csv_file:\n",
    "    stats_df = pd.read_csv(csv_file, sep=',')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc9951",
   "metadata": {},
   "source": [
    "## ESP Summary Plot\n",
    "esp_service.train(...) generated a plot summarizing the experiment's progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1c4c3",
   "metadata": {},
   "source": [
    "### Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c535f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = os.path.join(experiment_results_dir, 'Survived_plot.png')\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb11b9",
   "metadata": {},
   "source": [
    "# Models usage\n",
    "## Load the Prescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last row of the stats DataFrame, i.e. the last generation, to find the best model\n",
    "last_gen = stats_df['generation'].iloc[-1]\n",
    "best_score = stats_df['max_Survived'].iloc[-1]\n",
    "cid_best_score = stats_df['cid_max_Survived'].iloc[-1]\n",
    "prescriptor_model_filename = os.path.join(experiment_results_dir,\n",
    "                                          str(last_gen),\n",
    "                                          cid_best_score + '.h5')\n",
    "print(f'Best max_Survived average is {best_score:.3f} for candidate id {cid_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b07d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(f'Loading prescriptor model: {prescriptor_model_filename}')\n",
    "prescriptor_model = load_model(prescriptor_model_filename, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012a6a3",
   "metadata": {},
   "source": [
    "## Get a sample context\n",
    "Get the context from one of the rows in the dataset, and make a prescription for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f247417",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_source_df.sample(1)\n",
    "sample_context_df = sample_df[CONTEXT_COLUMNS]\n",
    "sample_context_action_df = sample_df[CONTEXT_ACTION_COLUMNS]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2fc55",
   "metadata": {},
   "source": [
    "### Prescribe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_context_df = encoder.encode_as_df(sample_context_df)\n",
    "encoded_prescribed_actions_df = esp_evaluator.prescribe(prescriptor_model, encoded_sample_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the context and actions dataframes.\n",
    "encoded_context_actions_df = pd.concat([encoded_sample_context_df,\n",
    "                                        encoded_prescribed_actions_df],\n",
    "                                       axis=1)\n",
    "sample_context_prescribed_action_df = encoder.decode_as_df(encoded_context_actions_df)\n",
    "sample_context_prescribed_action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc8798",
   "metadata": {},
   "source": [
    "### Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa91cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(predictors, context_action_df, encoder):\n",
    "    pred_array = []\n",
    "    for predictor in predictors:\n",
    "        pred = predictor.predict(encoder.encode_as_df(context_action_df))\n",
    "        pred_array.append(encoder.decode_as_df(pred))\n",
    "    preds_df = pd.concat(pred_array, axis=1)\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d706af",
   "metadata": {},
   "source": [
    "#### With original actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2409fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_actions_preds = get_predictions(all_predictors, sample_context_action_df, encoder)\n",
    "original_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51ffce",
   "metadata": {},
   "source": [
    "#### With prescribed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58345f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescribed_actions_preds = get_predictions(all_predictors, sample_context_prescribed_action_df, encoder)\n",
    "prescribed_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd11fe0",
   "metadata": {},
   "source": [
    "#### With custom actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287546d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_custom_action_df = sample_context_prescribed_action_df.copy()\n",
    " # TODO: Uncomment and replace by the name of the actions(s) to customize\n",
    "# sample_context_custom_action_df['SOME_ACTION_TO_CUSTOMIZE'] = 42\n",
    "sample_context_custom_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_actions_preds = get_predictions(all_predictors, sample_context_custom_action_df, encoder)\n",
    "custom_actions_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1b90b",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME_COLUMNS = list(original_actions_preds.columns)\n",
    "OUTCOME_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed OUTCOMES for the sample\n",
    "pd.DataFrame(sample_df[OUTCOME_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.concat([sample_context_action_df,\n",
    "                     sample_context_prescribed_action_df,\n",
    "                     sample_context_custom_action_df], axis=0)\n",
    "# Observed OUTCOMES\n",
    "comp_df[OUTCOME_COLUMNS] = sample_df[OUTCOME_COLUMNS].iloc[0]\n",
    "# Predicted OUTCOMES\n",
    "preds_data = {}\n",
    "for outcome_column in OUTCOME_COLUMNS:\n",
    "    preds_data[f'{outcome_column}_predicted'] = [original_actions_preds[outcome_column].iloc[0],\n",
    "                                                 prescribed_actions_preds[outcome_column].iloc[0],\n",
    "                                                 custom_actions_preds[outcome_column].iloc[0]]\n",
    "preds_df = pd.DataFrame(preds_data)\n",
    "comp_df[list(preds_data.keys())] = preds_df[list(preds_data.keys())].values\n",
    "# Diff\n",
    "for outcome_column in OUTCOME_COLUMNS:\n",
    "    if is_numeric_dtype(comp_df[outcome_column]):\n",
    "        comp_df[f'{outcome_column}_diff'] = comp_df[f'{outcome_column}_predicted'] - comp_df[outcome_column]\n",
    "comp_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic model training code\n",
    "- basis for all other tests (make sure to modify this whenever any bug has been discovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import wandb\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(project='gradient_debug', save_code=True, id=\"initial-run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv (filtered data)\n",
    "df = pd.read_csv(\"data/filtered_ds.csv\")\n",
    "# cast start time min (our basis for timeline)\n",
    "df[\"start_time_min\"] = df[\"start_time_min\"].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe by patient id then by time\n",
    "df = df.sort_values(by=[\"patient_id\", \"start_time_min\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a session (a row in the dataframe), take domain ids and domain scores, which are strings in the format of n, n, ...\n",
    "#  and return the numbers in a tuple of lists for easier processing\n",
    "def process_row(row):\n",
    "    values_a = [float(x.strip()) for x in str(row['domain_ids']).split(',')]\n",
    "    values_b = [float(x.strip()) for x in str(row['domain_scores']).split(',')]\n",
    "    return values_a, values_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a dataframe of a single patient's sessions and return a similar dataframe but with domain scores filled in\n",
    "'''\n",
    "def create_training_data(data : pd.DataFrame):\n",
    "    score = np.zeros((1,14)) # temp array for score for that session\n",
    "    scores = np.zeros((len(data), 14)) # scores matrix for the entire patient data\n",
    "    i = 0 # counter\n",
    "    \n",
    "    data = data.sort_values(by=[\"start_time_min\"]) # make sure that data is sorted by start time min\n",
    "    # iterate through data\n",
    "    for idx, row in data.iterrows():\n",
    "        # parse the curretn session's domain and corresponding scores\n",
    "        domains, domain_scores = process_row(row)\n",
    "        # iterate through current session's domains\n",
    "        for j in range(len(domains)):\n",
    "            # in temp score array, set the jth column (jth domain) to the correct score\n",
    "            score[0, int(domains[j] - 1)] = domain_scores[j]\n",
    "        # set the corresponding session in the scores matrix to the temp score array\n",
    "        # we don't have to reset the temp score array because we want the most updated scores\n",
    "        scores[i] = score\n",
    "        # increment counter\n",
    "        i += 1\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores, columns=[\"domain %d score\" % i for i in range(1, 15)])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    scores_df.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([data, scores_df], axis=1)\n",
    "    return data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a dataframe of a single patient's sessions and return a similar dataframe but with domain scores filled in\n",
    "# changes for consolidation, now instead of returning a dataframe that is counted by session, we modify it so that it is counted by days\n",
    "# meaning all sessions in a day count as one instead compared to the old function above\n",
    "def create_training_data(data : pd.DataFrame):\n",
    "    scores = np.zeros((data[\"start_time_min\"].map(lambda t: t.date()).nunique(), 15))\n",
    "    scores[:, 0] = data[\"patient_id\"].iloc[0] # set first column to be patient id\n",
    "    i = 0\n",
    "\n",
    "    data = data.sort_values(by=[\"start_time_min\"])\n",
    "    prevday = None\n",
    "    curday_score = np.zeros((1,14))\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        curday = row[\"start_time_min\"].date()\n",
    "        domains, domain_scores = process_row(row)\n",
    "\n",
    "        # if curday is a different day than the previous day, set scores[i] to the curday scores, increment the counter\n",
    "        if curday != prevday and prevday != None:\n",
    "            scores[i, 1:] = curday_score\n",
    "            i += 1\n",
    "        # loop through the domains, update corresponding score in curdayscore\n",
    "        for j in range(len(domains)):\n",
    "            # print(j)\n",
    "            curday_score[0, int(domains[j] - 1)] = domain_scores[j]\n",
    "        # we finished the session, set prevday to curday\n",
    "        prevday = curday\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores, columns=[\"patient_id\"] + [\"domain %d score\" % i for i in range(1, 15)])\n",
    "    scores_df.reset_index(drop=True, inplace=True)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(\"patient_id\")[df.columns].apply(create_training_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick a random patient for case study\n",
    "pid = random.choice(pd.unique(model_data[\"patient_id\"]))\n",
    "# pid = 346256\n",
    "## save the patient's data\n",
    "case_study_data = copy.deepcopy(model_data[model_data['patient_id'] == pid])\n",
    "## remove the patient's data from the dataset\n",
    "model_data = model_data.drop(model_data[model_data['patient_id'] == pid].index)\n",
    "\n",
    "## reset index for the case study\n",
    "case_study_data = case_study_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nonzero_rows(df, max_zeros):\n",
    "    # Count number of zeros in each row\n",
    "    zeros_count = (df == 0).sum(axis=1)\n",
    "    \n",
    "    # Filter rows with at most max_zeros number of zeros\n",
    "    non_zero_rows = df[zeros_count <= max_zeros]\n",
    "    return non_zero_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = filter_nonzero_rows(model_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data[[\"domain %d score\" % i for i in range(1, 15)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_array(rows, cols, N):\n",
    "    if N > cols:\n",
    "        raise ValueError(\"N cannot be greater than the number of columns.\")\n",
    "    \n",
    "    # Create an array of ones with the desired shape\n",
    "    array = np.ones((rows, cols), dtype=int)\n",
    "    \n",
    "    # Randomly assign N zeros to each row\n",
    "    for i in range(rows):\n",
    "        indices = np.random.choice(cols, N, replace=False)\n",
    "        array[i, indices] = 0\n",
    "    \n",
    "    return array, array.shape[0] * N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't modify to return the mask yet\n",
    "def generate_mask(data, N=10):\n",
    "    rows, cols = data.shape\n",
    "    assert(cols == 14)\n",
    "    mask, n_zeros = generate_random_array(rows, cols, N)\n",
    "    return data * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = copy.deepcopy(model_data)\n",
    "model_data = generate_mask(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_indicator(data):\n",
    "    (l, w) = data.shape\n",
    "    temp = np.zeros((l, w*2))\n",
    "    for i in range(l):\n",
    "        for d in range(w):\n",
    "            p = data[i, d]\n",
    "            # update output array\n",
    "            if p == 0:\n",
    "                # print(\"shouldn't be here\")\n",
    "                missing_ind = np.random.choice(2, 1)[0]\n",
    "                temp[i, d*2] = missing_ind\n",
    "                temp[i, d*2+1] = missing_ind\n",
    "            else:\n",
    "                temp[i, d*2] = p # score\n",
    "                temp[i, d*2+1] = 1-p # 1-score\n",
    "    return copy.deepcopy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = create_missing_indicator(model_data)\n",
    "ground_truth = create_missing_indicator(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(model_data == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_domains = 14\n",
    "        hidden1 = 100\n",
    "        hidden2 = 25\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_domains * 2, hidden1),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(hidden1, hidden2),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden2, hidden1),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(hidden1, n_domains * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(arr):\n",
    "    assert(len(arr) == 2)\n",
    "    # if two values are the same (the only three possible values pairs are (0, 0), (1, 1), and (0.5, 0.5))\n",
    "    return arr[0] == arr[1] and (arr[0] == 0 or arr[0] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, x_val,y_train, y_val, epochs, model, optimizer, loss_function):\n",
    "    outputs = []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    w = 14 ## hardcoded\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        val_epoch_loss = []\n",
    "        ## training\n",
    "        model.train()\n",
    "        for i in range(len(x_train)):\n",
    "            session = x_train[i]\n",
    "            # Output of Autoencoder\n",
    "            session_rs = session.reshape(-1, w * 2)\n",
    "            session_t = torch.from_numpy(session_rs).float()\n",
    "            reconstructed = model(session_t)\n",
    "            \n",
    "            # # handle missing value\n",
    "            missing_exists = False\n",
    "            missing_domains = []\n",
    "            for domain in range(w):\n",
    "                if missing(session[domain*2:domain*2+2]):\n",
    "                    missing_exists = True\n",
    "                    missing_domains.append(domain)\n",
    "                \n",
    "            # assert missing_exists == False, \"shouldn't have missing values\"\n",
    "            target = copy.deepcopy(y_train[i].reshape(-1, w * 2))\n",
    "            if missing_exists:\n",
    "                recon = reconstructed.detach().numpy()\n",
    "                for d in missing_domains:\n",
    "                    target[0, d*2:d*2+2] = recon[0, d*2:d*2+2]\n",
    "            target = torch.from_numpy(target).float()\n",
    "\n",
    "\n",
    "            # Calculating the loss function\n",
    "            loss = loss_function(reconstructed, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Storing the losses in a list for plotting\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        losses.append(statistics.mean(epoch_loss))\n",
    "        outputs.append((epochs, session_t, reconstructed))\n",
    "\n",
    "        ## validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for k in range(len(x_val)):\n",
    "                val = x_val[k]\n",
    "                val_rs = val.reshape(-1, w * 2)\n",
    "                val_t = torch.from_numpy(val_rs)\n",
    "                val_t = val_t.clone().detach().type(torch.float32)\n",
    "                answer = model(val_t)\n",
    "                val_loss = loss_function(answer, torch.from_numpy(y_val[k].reshape(-1, w * 2)).type(torch.float32))\n",
    "                val_epoch_loss.append(val_loss.item())\n",
    "        val_losses.append(statistics.mean(val_epoch_loss))\n",
    "    return losses, val_losses, outputs, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_2d = []\n",
    "val_losses_2d = []\n",
    "for i in range(num_runs):\n",
    "    model = AE()\n",
    "    # log wandb\n",
    "    wandb.watch(model, log='all')\n",
    "    epochs = 5\n",
    "    lr = 0.005\n",
    "\n",
    "    # Validation using MSE Loss function\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "    # Using an Adam Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(model_data, ground_truth, test_size=0.50)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.from_numpy(x_train).float())\n",
    "        zero_loss = loss_function(predictions, torch.from_numpy(y_train).float())\n",
    "\n",
    "        predictions = model(torch.from_numpy(x_val).float())\n",
    "        zero_loss_val = loss_function(predictions, torch.from_numpy(y_val).float())\n",
    "    \n",
    "    losses, val_losses, outputs, model = train_model(x_train, x_val, y_train, y_val, epochs, model, optimizer, loss_function)\n",
    "    losses = [zero_loss.item()] + losses\n",
    "    val_losses = [zero_loss_val.item()] + val_losses\n",
    "    \n",
    "    losses_2d.append(losses)\n",
    "    val_losses_2d.append(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_and_std(data, color_choice=\"blue\", setting=\"\"):\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation across multiple lists of data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (list of lists): A list where each element is a list of numbers.\n",
    "    \n",
    "    The function will compute the mean and standard deviation at each point\n",
    "    across the lists and plot these as a line for the mean and shading for the\n",
    "    standard deviation.\n",
    "    \"\"\"\n",
    "    # Convert data to a NumPy array for easier manipulation\n",
    "    data_array = np.array(data)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    means = np.mean(data_array, axis=0)\n",
    "    stds = np.std(data_array, axis=0)\n",
    "    # print(means, stds)\n",
    "    \n",
    "    # Create the x-axis values\n",
    "    x_values = np.arange(len(means))\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(x_values, means, label='Mean', color=color_choice)  # Mean line\n",
    "    plt.fill_between(x_values, means - stds, means + stds, color=color_choice, alpha=0.2, label='Standard Deviation')\n",
    "    \n",
    "    plt.title('Mean and Standard Deviation Plot of %s' %setting)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "with torch.no_grad():\n",
    "    plot_mean_and_std(losses_2d, \"blue\", \"Training\")\n",
    "    plot_mean_and_std(val_losses_2d, \"orange\", \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_box_plot(data_array):\n",
    "    \"\"\"\n",
    "    Creates a box plot for each column in the given NumPy array and overlays mean and standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_array (numpy.ndarray): A 2D NumPy array where each column represents a series of data points.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # Create the boxplot\n",
    "    bp = ax.boxplot(data_array, patch_artist=True, notch=True, meanline=True)\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    means = np.mean(data_array, axis=0)\n",
    "    stds = np.std(data_array, axis=0)\n",
    "    n_cols = data_array.shape[1]\n",
    "\n",
    "    # Add mean and standard deviation lines\n",
    "    for i in range(n_cols):\n",
    "        # Mean line\n",
    "        plt.plot([i+1], [means[i]], color='red', marker='o', markersize=5)\n",
    "        # Standard deviation lines\n",
    "        plt.plot([i+1, i+1], [means[i] - stds[i], means[i] + stds[i]], color='purple', marker='_', markersize=10, linestyle='None')\n",
    "\n",
    "    # Customizing the plot\n",
    "    plt.title('Box Plot with Mean and Standard Deviation')\n",
    "    plt.xlabel('Column Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# enhanced_box_plot(model_data[:, 0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_test = np.zeros((5000, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((5000, 14*2))\n",
    "for i in range(5000):\n",
    "    for d in range(14):\n",
    "        p = zero_test[i, d]\n",
    "        # update output array\n",
    "        if p == 0:\n",
    "            # print(\"shouldn't be here\")\n",
    "            missing_ind = np.random.choice(2, 1)[0]\n",
    "            temp[i, d*2] = missing_ind\n",
    "            temp[i, d*2+1] = missing_ind\n",
    "        else:\n",
    "            temp[i, d*2] = p # score\n",
    "            temp[i, d*2+1] = 1-p # 1-score\n",
    "\n",
    "zero_test = copy.deepcopy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total = np.zeros((1, 28))\n",
    "with torch.no_grad():\n",
    "    for row in zero_test:\n",
    "        temp = row.reshape(-1, 14 * 2)\n",
    "        temp = torch.from_numpy(temp)\n",
    "        temp = temp.clone().detach().type(torch.float32)\n",
    "        output = model(temp).detach().numpy()[0]\n",
    "        total = output + total\n",
    "\n",
    "total = total/5000\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how this changes the score trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"domain %d score\" % i for i in range(1, 15)]\n",
    "column_rename_dict = {}\n",
    "for i in range(len(column_names)):\n",
    "    column_rename_dict[column_names[i]] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(data : pd.DataFrame):\n",
    "    updated_domain_pct = dict() # keeps updated domain pct\n",
    "    score = 0 # score for each session, an average of all available domains\n",
    "    scores = []\n",
    "\n",
    "    data = data.sort_values(by=\"start_time_min\").reset_index(drop=True)\n",
    "    column_names = [\"domain %d score\" % i for i in range(1, 15)]\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        domain_ids, _ = process_row(row)\n",
    "        for j in range(len(domain_ids)):\n",
    "            col = column_names[int(domain_ids[j] - 1)]\n",
    "            updated_domain_pct[col] = row[col] # update domain pct to the latest one\n",
    "        # find sum of all domain pct\n",
    "        for k, v in updated_domain_pct.items():\n",
    "            score += float(v)\n",
    "        # take average of domain pct, add to list, reset score to 0\n",
    "        score /= len(updated_domain_pct)\n",
    "        scores.append(score)\n",
    "        score = 0\n",
    "    # return scores list\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data_scores = case_study_data[column_names].to_numpy()\n",
    "patient_data_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, w = patient_data_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data_scores = create_missing_indicator(patient_data_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.from_numpy(patient_data_scores).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions[:, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Removes columns from the second DataFrame that correspond to all-zero columns in the first DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (pd.DataFrame): A DataFrame where some columns might have all elements as zero.\n",
    "    - df2 (pd.DataFrame): A DataFrame from which the corresponding all-zero columns in df1 will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A modified DataFrame with the all-zero columns removed from df2.\n",
    "    \"\"\"\n",
    "    # Check that the number of columns in df1 and df2 match\n",
    "    if df1.shape[1] != df2.shape[1]:\n",
    "        raise ValueError(\"Both DataFrames must have the same number of columns.\")\n",
    "\n",
    "    # Find columns in the first DataFrame that have all zeros\n",
    "    zero_columns = df1.columns[(df1 == 0).all()]\n",
    "\n",
    "    # Remove the corresponding columns from the second DataFrame\n",
    "    filtered_df = df2.drop(columns=zero_columns)\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference\n",
    "difference = predictions - case_study_data.iloc[:, -14:].to_numpy()\n",
    "difference = pd.DataFrame(data=difference, columns=column_names)\n",
    "difference = remove_zero_columns(case_study_data.iloc[:, -14:], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(data=predictions, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for subplots\n",
    "fig = plt.figure(figsize=(12, 18))  # Increase figure size if necessary\n",
    "\n",
    "# set vmin and vmax\n",
    "vmin = 0\n",
    "vmax = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plot_mean_and_std(losses_2d, \"blue\", \"Training\")\n",
    "    plot_mean_and_std(val_losses_2d, \"orange\", \"Validation\")\n",
    "\n",
    "# Plotting the first matrix heatmap\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.heatmap(predictions.rename(columns=column_rename_dict), cmap=\"viridis\", cbar=True, vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Predictions\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Session\")\n",
    "\n",
    "# Plotting the second matrix heatmap\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.heatmap(case_study_data.iloc[:, -14:].rename(columns=column_rename_dict), cmap=\"viridis\", cbar=True, vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Session\")\n",
    "\n",
    "# Plotting the third matrix heatmap\n",
    "ax = plt.subplot(3, 2, 5)\n",
    "mean_data = np.mean(x_train, axis=0)[::2].reshape(1, -1)\n",
    "sns.heatmap(mean_data, cmap=\"viridis\", cbar=True, annot=True, annot_kws={\"size\": 8}, vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Dataset Mean\")\n",
    "plt.xlabel(\"Domain\")\n",
    "ax.set_xticklabels(range(1, 15))\n",
    "ax.set_yticklabels(\"\")\n",
    "\n",
    "# Heatmap of difference between two matrices\n",
    "# Plotting the heatmap\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.heatmap(difference.rename(columns=column_rename_dict), cmap=\"coolwarm\", cbar=True, center=0, vmin=-1, vmax=1)\n",
    "plt.title(\"Differences Between Prediction and Ground Truth\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Session\")\n",
    "\n",
    "# Adjust the spacing between the plots and margins of the figure\n",
    "fig.suptitle(\"patient %d\" % pid, size=14)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

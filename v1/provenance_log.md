# Provenance Log for Version 1

This file documents the origin, purpose, and status of experimental files and datasets from the legacy (v1) version of the project.

---

## archive/

### `autoencoder_pytorch.ipynb`
- **Purpose**: implement an autoencoder model that predicts all domains in Pytorch

### `autoencoder_weird_drop.ipynb`
- **Purpose**: document a weird drop in autoencoder model training and validation loss

### `autoencoder.ipynb`
- **Purpose**: implement an autoencoder model that predicts all domains in Tensorflow

### `baseline.ipynb`
- **Purpose**: autoencoder model with masking that predicts missing values domains so we can evaluate overall performance and compare with imputation baseline, implemented using Pytorch

### `context_action.ipynb`
- **Purpose**: visualize context and action interaction

### `context.ipynb`
- **Purpose**: visualize context information

### `dropout.ipynb`
- **Purpose**: investigate the effects of dropout layers in the autoencoder model

### `gpr.ipynb`
- **Purpose**: implement Gaussian Process that predicts probability that 

### `outcome.ipynb`
- **Purpose**: looks at score trends

### `prediction.ipynb`
- **Purpose**: preliminary prediction task attempt, looks mostly abandoned

### `test_dataset_generator.py`
- **Purpose**: create artificial datasets that are random so that it is easy to test experimental code

### `wandb_sample.py`
- **Purpose**: testing Weights & Bias connection, starter code from their website

---

## data/

### `consolidate_data_w_ds.csv`
- **Source**: 

---

## misc/
- **Purpose**: pem file and credentials to connect to SQL server, used in conjunction with `connection.py`

---

## model/
- previous models, what they are exactly is 不可考

---

## output/
- next step prediction model experiments, each folder stores the hyperparameters and records, generated by running `next_step.py`
- `experiment4/` was used in RPE and dissertation proposal

---

## queries/
- pulls from Constant Therapy AWS database

### `context_query.sql`
- **Goal**: pulls context information such as age group, condition since, deficit and disorder ids
- **Usage**: used for understanding user demographic in `context.ipynb`

### `deficit_action.sql`
- **Goal**: pulls sessions with deficit information in it
- **Usage**: limit sessins used to only patients with deficit information to gain a more complete picture, used in conjunction with disorder version

### `disorder_action.sql`
- **Goal**: pulls sessions with disorder information in it
- **Usage**: limit sessions used to only patients with disorder information to gain a more complete picture, used in conjunction with deficit version

### `domain_score.sql`
- **Goal**: pulls sessions with domain_score information
- **Usage**: initial query used when building autoencoder (predict missing domains) and next step prediction model (predict next step)

### `model_query.sql`
- **Goal**: the final query that pulls the data we used to train next step prediction model, modeled after Claire Cordella's queries for her paper about dosage frequency
- **Usage**: pulls sessions that fit the requirements, but before processing time related information

### `progression.sql`
- **Goal**: query that pulls progression_order information
- **Usage**: used to obtain progression order and task mapping so that we can further manipulate dataframes in Python

### `test.sql`
- **Goal**: simple test to make sure that the connection is working
- **Usage**: test connection for sanity check

---

## wandb/
- Weights & Biases Metadata

---

## main

### `baseline_quant.ipynb`
- **Purpose**: similar to archive/baseline.ipynb, but the results are quantified. shows plots of mae/mse vs number of missing (masked) domains using prediction and imputation methods
- **Outcome**: prediction performs better than the two imputation methods

### `connection.py`
- **Purpose**: utilize sshtunnel and pymysql to set up connection to Constant Therapy database to pull data from
- **Outcome**: NA

### `debug.ipynb`
- **Purpose**: debug various functions used the variance.ipynb, baseline_quant.ipynb, masking.ipynb, and sanity_checks.ipynb
- **Outcome**: NA

### `gradient_skipping.ipynb`
- **Purpose**: trying to implement gradient skipping for units that represent missing values
- **Outcome**: not working but preliminary work

### `helper.py`
- **Purpose**: helper functions
- **Outcome**: NA

### `imputation.ipynb`
- **Purpose**: compare imputation methods 
- **Outcome**: using dataset average imputation is better than known domain average

### `masking.ipynb`
- **Purpose**: baseline code for masking method: take only the sessions with 14 known domains, mask some domains, train model to predict all of them
- **Outcome**: the error is a little large

### `next_step_prediction.ipynb`
- **Purpose**: first version of next step prediction model, predict performance at next step given next domain to practice on
- **Outcome**: compared prediction performance with imputation baseline, model performed better

### `next_step.py`
- **Purpose**: a `.py` version of `next_step_prediction.ipynb`, more streamlined, used to generate results in `output/`
- **Outcome**: NA, see `output/`

### `one_domain_next_step.ipynb`
- **Purpose**: predict performance at next step given next domain to practice on, only train one domain at a time, an easier version of next_step_prediction so that we can build upon and debug slowly
- **Outcome**: model training well, moved onto predicting different domains using one model

### `pipeline copy.ipynb`
- **Purpose**: Unsure
- **Outcome**: Unsure, but it looks like code didn't run

### `pipeline_debug copy.ipynb`
- **Purpose**: same thing as `pipeline_debug copy.ipynb`, but to separate repeat and non repeat code
- **Outcome**: results for non-repeat section of RPE and dissertation proposal

### `pipeline_debug.ipynb`
- **Purpose**: originally a version of `pipeline.ipynb` for debugging, evolved to use a better visualization.
- **Outcome**: results for repeat section of RPE and dissertation proposal

### `pipeline.ipynb`
- **Purpose**: compares between different decision-making policies and ground truth scores (aggregate quantify number for whole dataset) using next step prediction as a foundation
- **Outcome**: nothing usable to my current knowledge

### `playground.py`
- **Purpose**: a small test script that test how pytorch's layers work, a foundation to gradient skipping
- **Outcome**: NA

### `sanity_checks.ipynb`
- **Purpose**: for masking model, check if consolidating sessions within the same day performs better than not consolidating and if model prediction reflects trends rather than predicting average
- **Outcome**: nothing usable to my current knowledge, unsure

### `trajectory.ipynb`
- **Purpose**: simulate different decision-making strategies using the next step prediction model
- **Outcome**: "best" strategy consistently outperformed both "random" and "worst

### `variance.ipynb`
- **Purpose**: using the masking model, predict, check distribution against ground truth to see if model can be used to model outcomes 
- **Outcome**: it didn't work well, the distribution was fairly flat and didn't reflect ground truth trends, this led to masking varying different number of domains

---

## Notes
- Output figures were useful for building the dissertation proposal, the base for v2
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "- pipeline that finds the best suggestion, prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random, sys, copy, os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Ensure deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/filtered_model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"start_time\"] = df[\"start_time\"].astype('datetime64[ns]')\n",
    "# df = df.sort_values(by=[\"patient_id\", \"start_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a row of sessions, take domain_ids and domain_scores, which are in string format separated by \",\", and replace with a list of the values\n",
    "def process_row(row):\n",
    "    values_a = [int(x.strip()) for x in str(row['domain_ids']).split(',')]\n",
    "    values_b = [float(x.strip()) for x in str(row['domain_scores']).split(',')]\n",
    "    return values_a, values_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in a dataframe of a patient's session, extract information useful for training\n",
    "def create_training_data(data: pd.DataFrame):\n",
    "    # Initialize variables\n",
    "    session_row = [] # contents of a row (patient id, encoding, cur score, prev score, repeat)\n",
    "    overall = [] # aggregate of everything (n sessions x 44)\n",
    "\n",
    "    cur_score = np.empty((14)) # score for each session\n",
    "    cur_score.fill(np.nan)\n",
    "    prev_score = None\n",
    "\n",
    "    seen = {} # dictionary for seen\n",
    "    patient_id = data[\"patient_id\"].iloc[0] # save patient_id\n",
    "\n",
    "    # Sort data by session start time\n",
    "    data = data.sort_values(by=[\"start_time\"])\n",
    "\n",
    "    # Process each row\n",
    "    for idx, row in data.iterrows():\n",
    "        domains, domain_scores = process_row(row)  # returns a list of domains : int and of domain_scores : float\n",
    "\n",
    "        # Track repeat status and update scores\n",
    "        repeat = False\n",
    "\n",
    "        for j, domain in enumerate(domains):\n",
    "            if domain not in seen:\n",
    "                seen[domain] = True\n",
    "            else:\n",
    "                repeat = True\n",
    "            \n",
    "            cur_score[domain - 1] = domain_scores[j] # update score in the loop\n",
    "\n",
    "        # Encode domains for this session\n",
    "        domain_encoding = np.zeros(14)\n",
    "        for domain in domains:\n",
    "            domain_encoding[domain - 1] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "        # if the session does not contain the target domain or is the first (no prev score), continue in the loop without doing anything, do this before appending\n",
    "        if prev_score is None:\n",
    "            session_row = []\n",
    "            prev_score = cur_score.copy()\n",
    "            continue\n",
    "        # assert np.sum(domain_encoding) != 1, \"continue not working\"\n",
    "\n",
    "        # append everything in the row list\n",
    "        session_row.append(patient_id)\n",
    "        session_row.extend(domain_encoding.copy().tolist())\n",
    "        session_row.extend(prev_score.copy().tolist())\n",
    "        session_row.extend(cur_score.copy().tolist())\n",
    "        session_row.append(repeat)\n",
    "        assert len(session_row) == 44, \"session row length weird\"\n",
    "\n",
    "        # append row to overall, reset\n",
    "        overall.append(session_row)\n",
    "        session_row = []\n",
    "        prev_score = cur_score.copy()\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    if overall:\n",
    "        overall = np.array(overall)\n",
    "        assert len(overall.shape) == 2, \"dimensions of overall wrong\"\n",
    "    else:\n",
    "        # Handle case where scores is empty\n",
    "        return pd.DataFrame(columns=[\"patient_id\"] + [\"domain %d encoding\" % i for i in range(1, 15)] +\n",
    "                                   [\"domain %d score\" % i for i in range(1, 15)] +\n",
    "                                   [\"domain %d target\" % i for i in range(1, 15)] +\n",
    "                                   [\"repeat\"])\n",
    "    \n",
    "        # Create column names\n",
    "    column_names = (\n",
    "        [\"patient_id\"]\n",
    "        + [f\"domain {i} encoding\" for i in range(1, 15)]\n",
    "        + [f\"domain {i} score\" for i in range(1, 15)]\n",
    "        + [f\"domain {i} target\" for i in range(1, 15)]\n",
    "        + [\"repeat\"]\n",
    "    )\n",
    "\n",
    "    # Create dataframe\n",
    "    scores_df = pd.DataFrame(overall, columns=column_names)\n",
    "    scores_df.reset_index(drop=True, inplace=True)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = df.groupby(\"patient_id\")[df.columns].apply(create_training_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"data/next_step_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/next_step_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "n_samples = 100000\n",
    "\n",
    "## one sample for train, only to see if it learns that one example\n",
    "train_data = train_data[:n_samples].copy()\n",
    "test_data = test_data[:n_samples].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = [\"domain %d score\" %i for i in range(1, 15)]\n",
    "encoding_columns = [\"domain %d encoding\" %i for i in range(1, 15)]\n",
    "target_columns = [\"domain %d target\" %i for i in range(1, 15)]\n",
    "repeat_columns = [\"repeat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create missing indicator when given the score data\n",
    "def create_missing_indicator(data):\n",
    "    (l, w) = data.shape\n",
    "    temp = np.zeros((l, w*2))\n",
    "    for i in range(l):\n",
    "        for d in range(w):\n",
    "            p = data[i, d]\n",
    "            # update output array\n",
    "            if np.isnan(p):\n",
    "                missing_ind = np.random.choice(2, 1)[0]\n",
    "                temp[i, d*2] = missing_ind\n",
    "                temp[i, d*2+1] = missing_ind\n",
    "            else:\n",
    "                temp[i, d*2] = p # score\n",
    "                temp[i, d*2+1] = 1-p # 1-score\n",
    "    return copy.deepcopy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a processed dataframe, return data and target tensors that can be put in the model\n",
    "def create_model_data(data : pd.DataFrame):\n",
    "    target = data[target_columns].copy().to_numpy() * data[encoding_columns].copy().to_numpy()\n",
    "    data_scores = create_missing_indicator(data[score_columns].copy().to_numpy())\n",
    "    final_data = np.hstack((data[encoding_columns].copy().to_numpy(), data_scores))\n",
    "    return torch.from_numpy(final_data).float(), torch.from_numpy(target).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = create_model_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input : 14 domain encodings + 14 domains (28 total features with missing indicator)\n",
    "## output: 28 score (prediction for the scores after next domain)\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_domains = 14\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_domains * 3, 100),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(100, n_domains)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# used for batch training\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index, :], self.target[index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m NN()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/experiment4/model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:265\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ct/lib/python3.9/site-packages/torch/serialization.py:249\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    246\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "model = NN()\n",
    "model = torch.load(\"output/experiment4/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study_data_mp = create_missing_indicator(case_study_data[score_columns].copy().to_numpy()[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model.eval()\n",
    "with torch.no_grad():\n",
    "    mp_prediction = mp_model(torch.from_numpy(case_study_data_mp).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict next step values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step = np.zeros(14)\n",
    "\n",
    "# Select a random index to place the '1'\n",
    "random_index = np.random.choice(14)\n",
    "next_step[random_index] = 1\n",
    "next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.cat((torch.from_numpy(next_step).float(), mp_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_ns_score = ns_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ns_score_ = np.sum(predicted_ns_score.numpy() * next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ns_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict missing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_truth = case_study_data[score_columns].copy(deep=True).to_numpy()[-1, :]\n",
    "new_truth[random_index] = predicted_ns_score_\n",
    "new_truth = create_missing_indicator(new_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model.eval()\n",
    "with torch.no_grad():\n",
    "    mp_prediction_new = mp_model(torch.from_numpy(new_truth).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_prediction[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_prediction_new[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mp_prediction_new[::2].numpy())/14 - np.sum(mp_prediction[::2].numpy())/ 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(arr):\n",
    "    assert(len(arr) == 2)\n",
    "    # if two values are the same (the only three possible values pairs are (0, 0), (1, 1), and (0.5, 0.5))\n",
    "    return arr[0] == arr[1] and (arr[0] == 0 or arr[0] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative (overall baseline vs prediction view)\n",
    "- compare our prediction to baseline in test set in a quantitative manner (rather than looking at an individual heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take in dataframe, return known domain average for prediction, modified from baseline quant 11/04/2024\n",
    "def known_domain_average(data : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Imputes missing values (NaN) in a DataFrame with the average of the non-missing values in the same row.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The DataFrame to impute.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    assert(data.shape[1] == 14 * 3) ## make sure that the dataframe is the right shape with encoding, current, and target\n",
    "\n",
    "    # separate encoding, scores, and target (which we dont need so ignored in here)\n",
    "    encoding = data.iloc[:, :14].copy()\n",
    "    data_ = data.iloc[:, 14:28].copy()\n",
    "\n",
    "    # Replace '0' with NaN to handle them as missing values\n",
    "    data_ = data.replace(0, np.nan)\n",
    "    # initialize the list we are going to use to store all known domain average\n",
    "    known_domain_average_lst = []\n",
    "\n",
    "    # Iterate over each row\n",
    "    for index, row in data_.iterrows():\n",
    "        # Calculate the mean of the non-NaN values in the row\n",
    "        mean_value = row.mean()\n",
    "        # if there are no known domains, we say known domain average is 0\n",
    "        if np.isnan(mean_value):\n",
    "            mean_value = 0\n",
    "        # append the average onto the list\n",
    "        known_domain_average_lst.append(mean_value)\n",
    "        # in the encoding df, replace na with \n",
    "        encoding.loc[index] = encoding.loc[index].replace(1, mean_value)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, y_true, n, fn : str):\n",
    "    diff = y_pred - y_true\n",
    "    if fn == \"mse\":\n",
    "        return np.sum(np.power(diff, 2)) / n\n",
    "    elif fn == \"mae\":\n",
    "        return np.sum(np.abs(diff)) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the model\n",
    "def predict(data, model):\n",
    "    model.eval()\n",
    "    data_t = torch.tensor(data, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        return model(data_t).clone().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "accuracy_list_prediction_sq = []\n",
    "accuracy_list_knownimputation_sq = []\n",
    "accuracy_list_prediction_abs = []\n",
    "accuracy_list_knownimputation_abs = []\n",
    "\n",
    "for masked_number in range(14):\n",
    "    ## create masked testing data so we can see how the model predicts and how imputation compares\n",
    "    masked_test_data = test_data.copy() # make a copy of test data for masked\n",
    "    masked_test_data[score_columns], n_missing, mask = generate_mask(test_data[score_columns].copy().to_numpy(), N=masked_number)\n",
    "    assert n_missing == masked_number * test_data.shape[0], \"n missing weird\"\n",
    "    assert n_missing == np.size(mask) - np.sum(mask), \"n missing weird\"\n",
    "\n",
    "    # known domain average prediction for the next domains\n",
    "    baseline_prediction = known_domain_average(masked_test_data).to_numpy()\n",
    "\n",
    "    # predict\n",
    "    masked_test_data_ = copy.deepcopy(masked_test_data[encoding_columns + score_columns])\n",
    "    masked_test_data_ = masked_test_data_.to_numpy()\n",
    "    test_data_scores = create_missing_indicator(masked_test_data_[:, -14:])\n",
    "    masked_test_data_ = np.hstack((masked_test_data_[:, :14], test_data_scores))\n",
    "    model_prediction = predict(masked_test_data_, model)\n",
    "    model_prediction = np.multiply(model_prediction, test_data[encoding_columns].to_numpy()) # element wise multiply the two so we have the same form as baseline prediction\n",
    "    # we only have the values we are predicting in the matrix, everything else is 0\n",
    "\n",
    "    # get accuracy (abs)\n",
    "    original = test_data[target_columns].copy().to_numpy() # ground truth\n",
    "    original = np.multiply(original, test_data[encoding_columns].copy().to_numpy())# element wise multiply the two so we only have the scores we care about predicting\n",
    "    assert original.shape == mask.shape and original.shape == model_prediction.shape and original.shape == baseline_prediction.shape\n",
    "    n_predicting = np.sum(test_data[encoding_columns].copy().to_numpy())\n",
    "    \n",
    "    accuracy_list_prediction_abs.append(get_accuracy(model_prediction, original, n_predicting, \"mae\"))\n",
    "    accuracy_list_knownimputation_abs.append(get_accuracy(baseline_prediction, original, n_predicting, \"mae\"))\n",
    "\n",
    "    ## get accuracy (sq)\n",
    "    accuracy_list_prediction_sq.append(get_accuracy(model_prediction, original, n_predicting, \"mse\"))\n",
    "    accuracy_list_knownimputation_sq.append(get_accuracy(baseline_prediction, original, n_predicting, \"mse\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "# plt.errorbar(range(0, 14), accuracy_list_prediction_sq, yerr=error_prediction_sq, label=\"model\", linewidth=3)\n",
    "# plt.errorbar(range(0, 14), accuracy_list_imputation_sq, yerr=error_imputation_sq, label=\"baseline\")\n",
    "\n",
    "x_values = range(0, 14)\n",
    "\n",
    "plt.plot(x_values, accuracy_list_prediction_sq, label=\"model\", marker=\"o\")\n",
    "plt.plot(x_values, accuracy_list_knownimputation_sq, label=\"baseline\", marker=\"o\")\n",
    "\n",
    "for x, y1, y2 in zip(x_values, accuracy_list_prediction_sq, accuracy_list_knownimputation_sq):\n",
    "    offset = 10\n",
    "\n",
    "    label = \"{:.4f}\".format(y1)\n",
    "    plt.annotate(label, # this is the text\n",
    "                (x,y1), # these are the coordinates to position the label\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext=(0,10), # distance from text to points (x,y)\n",
    "                ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "    label = \"{:.4f}\".format(y2)\n",
    "    plt.annotate(label, # this is the text\n",
    "                (x,y2), # these are the coordinates to position the label\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext=(0,-10), # distance from text to points (x,y)\n",
    "                ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"number of masked domains\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "plt.title(\"Mean squared error vs number of masked domains\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "# plt.errorbar(range(0, 14), accuracy_list_prediction_abs, yerr=error_prediction_abs, label=\"prediction\", linewidth=3)\n",
    "# plt.errorbar(range(0, 14), accuracy_list_imputation_abs, yerr=error_imputation_abs, label=\"imputation\")\n",
    "x_values = range(0, 14)\n",
    "\n",
    "plt.plot(x_values, accuracy_list_prediction_abs, label=\"model\", marker=\"o\")\n",
    "plt.plot(x_values, accuracy_list_knownimputation_abs, label=\"baseline\", marker=\"o\")\n",
    "\n",
    "for x, y1, y2 in zip(x_values, accuracy_list_prediction_abs, accuracy_list_knownimputation_abs):\n",
    "\n",
    "    label = \"{:.4f}\".format(y1)\n",
    "    plt.annotate(label, # this is the text\n",
    "                (x,y1), # these are the coordinates to position the label\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext=(0,10), # distance from text to points (x,y)\n",
    "                ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "    label = \"{:.4f}\".format(y2)\n",
    "    plt.annotate(label, # this is the text\n",
    "                (x,y2), # these are the coordinates to position the label\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext=(0,-10), # distance from text to points (x,y)\n",
    "                ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "\n",
    "plt.xlabel(\"number of masked domains\")\n",
    "plt.ylabel(\"mean absolute error\")\n",
    "plt.title(\"Mean absolute erorr vs number of masked domains\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "- visualize individual patient's trajectory (ground truth vs prediction vs imputation?)\n",
    "- look at overall quantitative picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"domain %d score\" % i for i in range(1, 15)]\n",
    "column_rename_dict = {}\n",
    "for i in range(len(column_names)):\n",
    "    column_rename_dict[column_names[i]] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a matrix, returns a list mean along rows\n",
    "def get_score(data):\n",
    "    return np.mean(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground truth scores (overall)\n",
    "GT_scores = get_score(case_study_data[column_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction scores\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = case_study_data[encoding_columns + score_columns].to_numpy()\n",
    "    test_data_scores = create_missing_indicator(test_data[:, -14:])\n",
    "    test_data = np.hstack((test_data[:, :14], test_data_scores))\n",
    "    predictions = model(torch.from_numpy(test_data).type(torch.float32)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = np.sum(case_study_data[encoding_columns].to_numpy())\n",
    "target_ = np.multiply(case_study_data[target_columns].to_numpy(), case_study_data[encoding_columns].to_numpy()) # only the targets we want to predict\n",
    "prediction_ = np.multiply(predictions, case_study_data[encoding_columns].to_numpy()) # only the scores we want to predict\n",
    "mse_ = np.divide(np.sum(np.abs(target_ - prediction_)), n_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(range(case_study_data.shape[0]), np.divide(np.sum(np.abs(target_ - prediction_), axis=1), np.sum(case_study_data[encoding_columns].to_numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(case_study_data[encoding_columns].to_numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study_data[encoding_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.patient_id == pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df[df.patient_id == pid][\"domain_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data(df[df.patient_id == pid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model as \n",
    "torch.save(model, \"model/next_step_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import copy\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Ensure deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "regular model as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hyperparams=dict):\n",
    "        super().__init__()\n",
    "        self.dimensions = hyperparams[\"input_dim\"] + hyperparams[\"hidden_dim\"] + hyperparams[\"output_dim\"] # list of dimensions\n",
    "        self.create_model()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            # Apply Sigmoid activation for all layers except the last one\n",
    "            if i < len(self.model) - 1:  # Skip activation for the output layer\n",
    "                # x = torch.relu(x)\n",
    "                pass\n",
    "        return x\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = nn.ModuleList() # initialize module list\n",
    "\n",
    "        # Create only linear layers, activations handled in forward()\n",
    "        for i in range(len(self.dimensions) - 1):\n",
    "            self.model.append(nn.Linear(self.dimensions[i], self.dimensions[i + 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Gradient Function for missing value prediction\n",
    "class CustomGradient(Function):\n",
    "    @staticmethod\n",
    "    def missing(values):\n",
    "        assert values.shape[0] == 2, \"shape incorrect in missing function\"\n",
    "        return values[0] == values[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors  # Retrieve input saved during forward pass\n",
    "        n_domains = input.shape[1] // 2  # Find the number of domains\n",
    "        grad_input = grad_output.clone()  # Copy to modify\n",
    "\n",
    "        # Modify gradients without breaking computation graph\n",
    "        for i in range(input.shape[0]):\n",
    "            for j in range(0, n_domains, 2):  # Loop through the entire input matrix\n",
    "                pair = input[i, j:j+2]  # Check pairs using tensor operations\n",
    "                if CustomGradient.missing(pair):  # Modify gradient to be 0\n",
    "                    grad_input[i, j] = 0\n",
    "                    grad_input[i, j + 1] = 0\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "custom_grad = CustomGradient.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a custom network based on the baseline model\n",
    "class CustomNet(Baseline):\n",
    "    def __init__(self, hyperparams=dict):\n",
    "        super().__init__(hyperparams)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = custom_grad(x)\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            # Apply Sigmoid activation for all layers except the last one\n",
    "            if i < len(self.model) - 1:  # Skip activation for the output layer\n",
    "                # x = torch.relu(x)\n",
    "                pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing\n",
    "I wanna see how the two models will change and be different\n",
    "- if there's no missing, it should be the same\n",
    "- if there are missing, confirm how the gradient changes by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set universal hyperparameters\n",
    "size = 14\n",
    "lr = 0.01\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
       "          0.1332, 0.9346, 0.5936, 0.8694, 0.5677]]),\n",
       " tensor([[0.0000, 0.0000, 0.3829, 0.9593, 0.0000, 0.0000, 0.2566, 0.7936, 0.9408,\n",
       "          0.1332, 0.0000, 0.0000, 0.8694, 0.5677]], requires_grad=True),\n",
       " tensor([[0.0000, 0.0000, 0.3829, 0.9593, 0.0000, 0.0000, 0.2566, 0.7936, 0.9408,\n",
       "          0.1332, 0.0000, 0.0000, 0.8694, 0.5677]], requires_grad=True))"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create example input\n",
    "x = torch.rand(1, size)\n",
    "target_x = x.clone().detach()\n",
    "\n",
    "x[0, 0] = 0\n",
    "x[0, 1] = 0\n",
    "x[0, 4] = 0\n",
    "x[0, 5] = 0\n",
    "x[0, 10] = 0\n",
    "x[0, 11] = 0\n",
    "\n",
    "gradient_x = x.clone().detach().requires_grad_(True)\n",
    "baseline_x = x.clone().detach().requires_grad_(True)\n",
    "target_x, baseline_x, gradient_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define hyperparameters\n",
    "hyperparameters = dict()\n",
    "hyperparameters[\"input_dim\"] = [size]\n",
    "hyperparameters[\"output_dim\"] = [size]\n",
    "hyperparameters[\"hidden_dim\"] = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define baseline model and associated thingies\n",
    "torch.manual_seed(42)\n",
    "baseline_model = Baseline(hyperparameters)\n",
    "\n",
    "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define custom model and associated thingies\n",
    "torch.manual_seed(42)\n",
    "custom_model = CustomNet(hyperparameters)\n",
    "## loss function and optimizer definition\n",
    "\n",
    "custom_optimizer = torch.optim.Adam(custom_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hook function to amplify gradients\n",
    "def hook_fn(grad):\n",
    "    # print(\"Hooked Gradient:\", grad)  # Debugging output\n",
    "    return grad * 10  # Amplify gradients\n",
    "\n",
    "# Attach hooks to weights and biases\n",
    "for i, layer in enumerate(custom_model.model):\n",
    "    if isinstance(layer, nn.Linear):  # Apply to Linear layers only\n",
    "        layer.weight.register_hook(hook_fn)  # Hook on weights\n",
    "        layer.bias.register_hook(hook_fn)    # Hook on biases\n",
    "\n",
    "# # Debugging gradient flow\n",
    "# def debug_hook(module, grad_input, grad_output):\n",
    "#     print(f\"Layer: {module}, Grad Input: {grad_input}, Grad Output: {grad_output}\")\n",
    "\n",
    "# # Attach debugging hooks to each layer\n",
    "# for layer in custom_model.model:\n",
    "#     if isinstance(layer, nn.Linear):\n",
    "#         layer.register_backward_hook(debug_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one step training\n",
    "def train_one_step(model, optimizer, loss_fn, input, target):\n",
    "    model_output = model(input)\n",
    "    loss = loss_fn(target, model_output)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_steps, model, optimizer, loss_fn, input, target):\n",
    "    output = []\n",
    "    loss = []\n",
    "    for i in range(n_steps):\n",
    "        model_output_, loss_ = train_one_step(model, optimizer, loss_fn, input, target)\n",
    "        output.append(model_output_)\n",
    "        loss.append(loss_)\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output, baseline_loss = train(steps, baseline_model, baseline_optimizer, loss_function, baseline_x, target_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Baseline Input Leaf? True\n",
      "Baseline Input Gradient: tensor([[-0.0568, -0.0400, -0.0411,  0.0521,  0.2190,  0.0415, -0.4059, -0.0967,\n",
      "         -0.3956, -0.5788, -0.4435, -0.2656, -0.0733, -0.1300]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Is Baseline Input Leaf?\", baseline_x.is_leaf)\n",
    "print(\"Baseline Input Gradient:\", baseline_x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_output, custom_loss = train(steps, custom_model, custom_optimizer, loss_function, gradient_x, target_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Custom Input Leaf? True\n",
      "Custom Input Gradient: tensor([[ 0.0000,  0.0000, -0.0411,  0.0521,  0.0000,  0.0000, -0.4059, -0.0967,\n",
      "         -0.3956, -0.5788, -0.4435, -0.2656, -0.0733, -0.1300]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Is Custom Input Leaf?\", gradient_x.is_leaf)\n",
    "print(\"Custom Input Gradient:\", gradient_x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks - how do these models behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Output Difference: 0.0\n",
      "Step 2: Output Difference: 1.1175870895385742e-07\n",
      "Step 3: Output Difference: 1.7881393432617188e-07\n",
      "Step 4: Output Difference: 1.341104507446289e-07\n",
      "Step 5: Output Difference: 1.1920928955078125e-07\n",
      "Step 6: Output Difference: 2.384185791015625e-07\n",
      "Step 7: Output Difference: 3.2782554626464844e-07\n",
      "Step 8: Output Difference: 4.0978193283081055e-07\n",
      "Step 9: Output Difference: 2.980232238769531e-07\n",
      "Step 10: Output Difference: 3.5762786865234375e-07\n"
     ]
    }
   ],
   "source": [
    "for step in range(steps):\n",
    "    print(f\"Step {step+1}: Output Difference: {(baseline_output[step] - custom_output[step]).abs().max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6675, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.5744, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.4944, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.4233, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.3577, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.2957, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.2368, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1821, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1332, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.0920, grad_fn=<MseLossBackward0>)]"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6675, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.5744, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.4944, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.4233, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.3577, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.2957, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.2368, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1821, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1332, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.0920, grad_fn=<MseLossBackward0>)]"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs Comparison:\n",
      "Baseline Output: tensor([[0.8508, 0.6859, 0.3533, 0.2704, 0.4322, 0.4066, 0.2286, 0.4641, 0.3173,\n",
      "         0.0766, 0.6813, 0.8757, 0.6688, 0.3809]], grad_fn=<AddmmBackward0>)\n",
      "Gradient Output: tensor([[0.8508, 0.6859, 0.3533, 0.2704, 0.4322, 0.4066, 0.2286, 0.4641, 0.3173,\n",
      "         0.0766, 0.6813, 0.8757, 0.6688, 0.3809]], grad_fn=<AddmmBackward0>)\n",
      "Difference: tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Outputs Comparison:\")\n",
    "print(\"Baseline Output:\", baseline_output[-1])\n",
    "print(\"Gradient Output:\", custom_output[-1])\n",
    "print(\"Difference:\", (baseline_output[-1] - custom_output[-1]).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients Comparison:\n",
      "Baseline Input Gradient: tensor([[-0.0568, -0.0400, -0.0411,  0.0521,  0.2190,  0.0415, -0.4059, -0.0967,\n",
      "         -0.3956, -0.5788, -0.4435, -0.2656, -0.0733, -0.1300]])\n",
      "Gradient Input Gradient: tensor([[ 0.0000,  0.0000, -0.0411,  0.0521,  0.0000,  0.0000, -0.4059, -0.0967,\n",
      "         -0.3956, -0.5788, -0.4435, -0.2656, -0.0733, -0.1300]])\n",
      "Gradient Difference: tensor(0.2190)\n"
     ]
    }
   ],
   "source": [
    "# Compare input gradients\n",
    "print(\"Gradients Comparison:\")\n",
    "print(\"Baseline Input Gradient:\", baseline_x.grad)\n",
    "print(\"Gradient Input Gradient:\", gradient_x.grad)\n",
    "print(\"Gradient Difference:\", (baseline_x.grad - gradient_x.grad).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Output Difference: 4.172325134277344e-07\n",
      "Layer 1 Output Difference: 3.8743019104003906e-07\n",
      "Layer 0 Weight Gradient Difference: 0.9944490194320679\n",
      "Layer 0 Bias Gradient Difference: 1.0366342067718506\n",
      "Layer 1 Weight Gradient Difference: 1.7068126201629639\n",
      "Layer 1 Bias Gradient Difference: 1.6445785760879517\n"
     ]
    }
   ],
   "source": [
    "# Enable gradients for inputs\n",
    "baseline_input = baseline_x.clone().detach().requires_grad_(True)\n",
    "gradient_input = gradient_x.clone().detach().requires_grad_(True)\n",
    "\n",
    "for i, (b_layer, g_layer) in enumerate(zip(baseline_model.model, custom_model.model)):\n",
    "    # Forward pass\n",
    "    baseline_input = b_layer(baseline_input)\n",
    "    gradient_input = g_layer(gradient_input)\n",
    "\n",
    "    # Retain gradients for intermediate outputs\n",
    "    baseline_input.retain_grad()\n",
    "    gradient_input.retain_grad()\n",
    "\n",
    "    # Print output differences layer-by-layer\n",
    "    print(f\"Layer {i} Output Difference: {(baseline_input - gradient_input).abs().max().item()}\")\n",
    "\n",
    "# Compute MSE Loss\n",
    "baseline_loss = F.mse_loss(baseline_input, target_x)\n",
    "gradient_loss = F.mse_loss(gradient_input, target_x)\n",
    "\n",
    "# Backward pass for gradients\n",
    "baseline_loss.backward(retain_graph=True)\n",
    "gradient_loss.backward(retain_graph=True)\n",
    "\n",
    "# Compare gradients for weights and biases layer-by-layer\n",
    "for j, (b_layer, g_layer) in enumerate(zip(baseline_model.model, custom_model.model)):\n",
    "    if isinstance(b_layer, nn.Linear):  # Only compare Linear layers\n",
    "        # Weight gradient difference\n",
    "        weight_diff = (b_layer.weight.grad - g_layer.weight.grad).abs().max().item()\n",
    "        # Bias gradient difference\n",
    "        bias_diff = (b_layer.bias.grad - g_layer.bias.grad).abs().max().item()\n",
    "\n",
    "        print(f\"Layer {j} Weight Gradient Difference: {weight_diff}\")\n",
    "        print(f\"Layer {j} Bias Gradient Difference: {bias_diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Difference: 2.5331974029541016e-07\n",
      "Bias Difference: 8.940696716308594e-08\n"
     ]
    }
   ],
   "source": [
    "baseline_weight = baseline_model.model[0].weight\n",
    "gradient_weight = custom_model.model[0].weight\n",
    "\n",
    "baseline_bias = baseline_model.model[0].bias\n",
    "gradient_bias = custom_model.model[0].bias\n",
    "\n",
    "print(\"Weight Difference:\", (baseline_weight - gradient_weight).abs().max().item())\n",
    "print(\"Bias Difference:\", (baseline_bias - gradient_bias).abs().max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
